{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Populate our RDF database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "from IPython.display import clear_output\n",
    "\n",
    "#!pip install rdflib\n",
    "from rdflib import Graph, Literal, RDF, URIRef, Namespace, term\n",
    "from rdflib.namespace import FOAF, XSD\n",
    "\n",
    "#season parameters\n",
    "y = 2019\n",
    "shortSeas = \"1920\"\n",
    "fullSeas = str(y)+\"-\"+str(y+1)\n",
    "\n",
    "# parameters and URLs\n",
    "path = str(Path(os.path.abspath(os.getcwd())).parent.absolute())\n",
    "statsUrl = path + \"/inDepthSoccerStats/\"+fullSeas+\".csv\"\n",
    "statsFBrefUrl = path + \"/inDepthSoccerStats/transfermarkt_fbref_20\"+shortSeas+\".csv\"\n",
    "playersUrl = path + '/inDepthSoccerStats/players.csv'\n",
    "teamsUrl = path + '/inDepthSoccerStats/clubs.csv'\n",
    "appUrl = path + '/inDepthSoccerStats/appearances.csv'\n",
    "gamesUrl = path + '/inDepthSoccerStats/games.csv'\n",
    "\n",
    "# country codes\n",
    "countriesURL = path + '/inDepthSoccerStats/wikipedia-iso-country-codes.csv'\n",
    "\n",
    "# saving folder\n",
    "savePath =  path + '/rdf/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "stats = pd.read_csv(statsUrl, sep=',', index_col='indCol')\n",
    "#these dataframes store data from Transfermarkt\n",
    "players = pd.read_csv(playersUrl, sep=',', index_col='player_id')\n",
    "teams = pd.read_csv(teamsUrl, sep=',', index_col='club_id')\n",
    "app = pd.read_csv(appUrl, sep=',', index_col='appearance_id')\n",
    "games = pd.read_csv(gamesUrl, sep=',', index_col='game_id')\n",
    "#FBref file used for completing some missing data. Available only for 3 seasons.\n",
    "if(y >= 2017):\n",
    "    statsFBref = pd.read_csv(statsFBrefUrl, sep=';', index_col='Column1', dtype={\"Attendance\": \"string\"})\n",
    "\n",
    "#load the country codes\n",
    "# we need to convert NaN values to something else otherwise NA strings are converted to NaN -> problem with Namibia\n",
    "countries = pd.read_csv(countriesURL, sep=',', index_col='English short name lower case', keep_default_na=False, na_values=['_'])\n",
    "\n",
    "#players.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Parsing and matching utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "#!pip install unidecode\n",
    "#!pip install googlesearch-python\n",
    "from unidecode import unidecode\n",
    "from itertools import permutations\n",
    "from difflib import SequenceMatcher\n",
    "from googlesearch import search\n",
    "\n",
    "#string parsing\n",
    "\n",
    "def hyphenize(s):\n",
    "    return unidecode(s.lower().replace(\" \",\"-\").replace(\"&#039;\",\"'\"))\n",
    "\n",
    "def cleanChars(item):\n",
    "    return item.str.replace(\"ć\", \"c\").str.replace(\"ğ\",\"g\").str.replace(\"İ\",\"i\").str.replace(\"-scaron-\",\"s\")\n",
    "\n",
    "def cleanString(s):\n",
    "    return unidecode(s.replace(\"ć\", \"c\").replace(\"ğ\",\"g\").replace(\"İ\",\"i\"))\n",
    "    \n",
    "\n",
    "#permutations strategy\n",
    "\n",
    "def genSeqByLength(perm, length):\n",
    "    newName = \"\"\n",
    "    for j in range(0, length - 1):\n",
    "        newName = newName + perm[j] + \"-\"\n",
    "    newName = newName + perm[length - 1]\n",
    "    return newName\n",
    "\n",
    "def match_seq(splitS1, lis):\n",
    "    resultList = []\n",
    "    #iterate on all possible permutations\n",
    "    length = -1\n",
    "    if(len(lis) == 1):\n",
    "        length = len(lis[0].split(\"-\"))\n",
    "        uniqueItem = lis[0]\n",
    "    for perm in permutations(splitS1):\n",
    "        if(length != -1):\n",
    "            newName = genSeqByLength(perm, length)\n",
    "            if(uniqueItem == newName):\n",
    "                return [uniqueItem]\n",
    "        #consider all lengths from 2 to n\n",
    "        else:\n",
    "            for i in range(2, len(splitS1) + 1):\n",
    "                newName = genSeqByLength(perm, i)\n",
    "                for item in lis:\n",
    "                    if(item == newName):\n",
    "                        resultList.append(newName)\n",
    "    return resultList\n",
    "\n",
    "#multiple matches resolution\n",
    "\n",
    "def getAppsByID(ID, apps):\n",
    "    return np.size(apps[apps['player_id'] == ID], 0);\n",
    "\n",
    "#I: number of games in the season, candidate players, list describing single appearances\n",
    "#O: player from players file, or empty Series\n",
    "def solve_with_apps_approx(statsGames, somePlayers, appsCol, tol=5):\n",
    "    minDiff = 50\n",
    "    minInd = 0\n",
    "    for ind in somePlayers.index:\n",
    "        if(abs(statsGames - getAppsByID(ind, appsCol)) < minDiff):\n",
    "            minDiff = abs(statsGames - getAppsByID(ind, appsCol))\n",
    "            minInd = ind\n",
    "    if(minDiff <= tol):\n",
    "        player = somePlayers[somePlayers.index == minInd].iloc[0] \n",
    "        return player, minInd\n",
    "    else:\n",
    "        return pd.Series([]), -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Matching teams from different sources\n",
    "This section is intended to create a dictionary to match team names in stats files to their Transfermarkt ID. This operation can be performed just once, as the dictionary can be dumped to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1 out of 143 GOOGLE: Athletic Club --> Athletic Bilbao\n",
      "  2 out of 143 GOOGLE: Fulham --> Fulham FC\n",
      "  3 out of 143 GOOGLE: Lille --> LOSC Lille\n",
      "  4 out of 143 GOOGLE: Torino --> Torino FC\n",
      "  5 out of 143 GOOGLE: Mallorca --> RCD Mallorca\n",
      "  6 out of 143 GOOGLE: Roma --> AS Roma\n",
      "  8 out of 143 GOOGLE: Levante --> Levante UD\n",
      "  9 out of 143 GOOGLE: Atletico Madrid --> Atlético de Madrid\n",
      " 10 out of 143 GOOGLE: Brescia --> Brescia Calcio\n",
      " 11 out of 143 GOOGLE: Celta Vigo --> Celta de Vigo\n",
      " 12 out of 143 GOOGLE: Barcelona --> FC Barcelona\n",
      " 13 out of 143 GOOGLE: Monaco --> AS Monaco\n",
      " 15 out of 143 GOOGLE: Bayer Leverkusen --> Bayer 04 Leverkusen\n",
      " 16 out of 143 GOOGLE: SPAL 2013 --> SPAL\n",
      " 17 out of 143 GOOGLE: Hoffenheim --> TSG 1899 Hoffenheim\n",
      " 19 out of 143 GOOGLE: Hull --> Hull City\n",
      " 20 out of 143 GOOGLE: Deportivo La Coruna --> Deportivo de La Coruña\n",
      " 21 out of 143 GOOGLE: Middlesbrough --> Middlesbrough FC\n",
      " 23 out of 143 GOOGLE: Verona --> Hellas Verona\n",
      " 24 out of 143 GOOGLE: Rennes --> Stade Rennais FC\n",
      " 26 out of 143 GOOGLE: Burnley --> Burnley FC\n",
      " 27 out of 143 GOOGLE: Watford --> Watford FC\n",
      " 29 out of 143 GOOGLE: Fiorentina --> ACF Fiorentina\n",
      " 30 out of 143 GOOGLE: Lens --> RC Lens\n",
      " 31 out of 143 GOOGLE: Fortuna Duesseldorf --> Fortuna Düsseldorf\n",
      " 32 out of 143 GOOGLE: Nuernberg --> 1.FC Nuremberg\n",
      " 33 out of 143 GOOGLE: Swansea --> Swansea City\n",
      " 34 out of 143 GOOGLE: Udinese --> Udinese Calcio\n",
      " 35 out of 143 GOOGLE: Alaves --> Deportivo Alavés\n",
      " 36 out of 143 GOOGLE: Las Palmas --> UD Las Palmas\n",
      " 37 out of 143 GOOGLE: Southampton --> Southampton FC\n",
      " 38 out of 143 GOOGLE: Cagliari --> Cagliari Calcio\n",
      " 39 out of 143 GOOGLE: Norwich --> Norwich City\n",
      " 40 out of 143 GOOGLE: Chievo --> Chievo Verona\n",
      " 41 out of 143 GOOGLE: Crotone --> FC Crotone\n",
      " 42 out of 143 GOOGLE: Bournemouth --> AFC Bournemouth\n",
      " 43 out of 143 GOOGLE: Leicester --> Leicester City\n",
      " 44 out of 143 GOOGLE: Sampdoria --> UC Sampdoria\n",
      " 45 out of 143 GOOGLE: Sevilla --> Sevilla FC\n",
      " 46 out of 143 GOOGLE: Leganes --> CD Leganés\n",
      " 47 out of 143 GOOGLE: Everton --> Everton FC\n",
      " 49 out of 143 GOOGLE: Ingolstadt --> FC Ingolstadt 04\n",
      " 50 out of 143 GOOGLE: Espanyol --> RCD Espanyol Barcelona\n",
      " 51 out of 143 GOOGLE: Reims --> Stade Reims\n",
      " 53 out of 143 GOOGLE: Bordeaux --> FC Girondins Bordeaux\n",
      " 54 out of 143 GOOGLE: Napoli --> SSC Napoli\n",
      " 55 out of 143 GOOGLE: Amiens --> Amiens SC\n",
      " 56 out of 143 GOOGLE: Lorient --> FC Lorient\n",
      " 57 out of 143 GOOGLE: Cardiff --> Cardiff City\n",
      " 58 out of 143 GOOGLE: Werder Bremen --> SV Werder Bremen\n",
      " 59 out of 143 GOOGLE: Strasbourg --> RC Strasbourg Alsace\n",
      " 60 out of 143 GOOGLE: Union Berlin --> 1.FC Union Berlin\n",
      " 61 out of 143 GOOGLE: Atalanta --> Atalanta BC\n",
      " 63 out of 143 GOOGLE: Mainz 05 --> 1.FSV Mainz 05\n",
      " 64 out of 143 GOOGLE: Nimes --> Nîmes Olympique\n",
      " 66 out of 143 GOOGLE: Schalke 04 --> FC Schalke 04\n",
      " 67 out of 143 GOOGLE: Stoke --> Stoke City\n",
      "-->\n",
      " 68 out of 143 GOOGLE: FC Cologne --> 1.FC Köln\n",
      " 69 out of 143 GOOGLE: Benevento --> Benevento Calcio\n",
      " 70 out of 143 GOOGLE: Cordoba --> Córdoba CF\n",
      " 72 out of 143 GOOGLE: Borussia M.Gladbach --> Borussia Mönchengladbach\n",
      " 73 out of 143 GOOGLE: Parma --> Parma Calcio 1913\n",
      " 74 out of 143 GOOGLE: Real Betis --> Real Betis Balompié\n",
      " 75 out of 143 GOOGLE: Chelsea --> Chelsea FC\n",
      " 76 out of 143 GOOGLE: Paderborn --> SC Paderborn 07\n",
      " 77 out of 143 GOOGLE: Brighton --> Brighton & Hove Albion\n",
      " 78 out of 143 GOOGLE: Caen --> SM Caen\n",
      " 79 out of 143 GOOGLE: Girona --> Girona FC\n",
      " 80 out of 143 GOOGLE: Malaga --> Málaga CF\n",
      " 81 out of 143 GOOGLE: Metz --> FC Metz\n",
      " 83 out of 143 GOOGLE: Huddersfield --> Huddersfield Town\n",
      " 84 out of 143 GOOGLE: Troyes --> ESTAC Troyes\n",
      " 85 out of 143 GOOGLE: Eibar --> SD Eibar\n",
      " 86 out of 143 GOOGLE: Frosinone --> Frosinone Calcio\n",
      " 87 out of 143 GOOGLE: Hertha Berlin --> Hertha BSC\n",
      " 88 out of 143 GOOGLE: Carpi --> AC Carpi\n",
      " 89 out of 143 GOOGLE: Almeria --> UD Almería\n",
      " 90 out of 143 GOOGLE: Lyon --> Olympique Lyon\n",
      " 91 out of 143 GOOGLE: Palermo --> Palermo FC\n",
      " 92 out of 143 GOOGLE: Liverpool --> Liverpool FC\n",
      " 94 out of 143 GOOGLE: Sassuolo --> US Sassuolo\n",
      " 95 out of 143 GOOGLE: Osasuna --> CA Osasuna\n",
      " 96 out of 143 GOOGLE: Getafe --> Getafe CF\n",
      " 97 out of 143 GOOGLE: Villarreal --> Villarreal CF\n",
      " 98 out of 143 GOOGLE: West Ham --> West Ham United\n",
      " 99 out of 143 GOOGLE: Wolfsburg --> VfL Wolfsburg\n",
      "100 out of 143 GOOGLE: Sporting Gijon --> Sporting Gijón\n",
      "101 out of 143 GOOGLE: Bologna --> Bologna FC 1909\n",
      "103 out of 143 GOOGLE: Sunderland --> Sunderland AFC\n",
      "105 out of 143 GOOGLE: Real Valladolid --> Real Valladolid CF\n",
      "106 out of 143 GOOGLE: Elche --> Elche CF\n",
      "107 out of 143 GOOGLE: Toulouse --> FC Toulouse\n",
      "108 out of 143 GOOGLE: Evian Thonon Gaillard --> Thonon Évian Grand Genève FC\n",
      "109 out of 143 GOOGLE: Pescara --> Delfino Pescara 1936\n",
      "110 out of 143 GOOGLE: Arsenal --> Arsenal FC\n",
      "112 out of 143 GOOGLE: Guingamp --> EA Guingamp\n",
      "113 out of 143 GOOGLE: Paris Saint Germain --> Paris Saint-Germain\n",
      "114 out of 143 GOOGLE: Angers --> Angers SCO\n",
      "115 out of 143 GOOGLE: Brest --> Stade Brestois 29\n",
      "116 out of 143 GOOGLE: Lazio --> SS Lazio\n",
      "117 out of 143 GOOGLE: Tottenham --> Tottenham Hotspur\n",
      "118 out of 143 GOOGLE: Granada --> Granada CF\n",
      "119 out of 143 GOOGLE: Cesena --> Cesena FC\n",
      "120 out of 143 GOOGLE: Marseille --> Olympique Marseille\n",
      "121 out of 143 GOOGLE: Saint-Etienne --> AS Saint-Étienne\n",
      "124 out of 143 GOOGLE: Genoa --> Genoa CFC\n",
      "125 out of 143 GOOGLE: Lecce --> US Lecce\n",
      "126 out of 143 GOOGLE: Dijon --> Dijon FCO\n",
      "127 out of 143 GOOGLE: Darmstadt --> SV Darmstadt 98\n",
      "129 out of 143 GOOGLE: Nantes --> FC Nantes\n",
      "131 out of 143 GOOGLE: Juventus --> Juventus FC\n",
      "132 out of 143 GOOGLE: Augsburg --> FC Augsburg\n",
      "133 out of 143 GOOGLE: Freiburg --> SC Freiburg\n",
      "134 out of 143 GOOGLE: Nice --> OGC Nice\n",
      "137 out of 143 GOOGLE: RasenBallsport Leipzig --> RB Leipzig\n",
      "139 out of 143 GOOGLE: Montpellier --> Montpellier HSC\n",
      "140 out of 143 GOOGLE: Empoli --> FC Empoli\n",
      "141 out of 143 GOOGLE: Valencia --> Valencia CF\n",
      "142 out of 143 GOOGLE: Inter --> Inter Milan\n",
      "143 out of 143 GOOGLE: Nancy --> AS Nancy-Lorraine\n",
      "CPU times: total: 55.6 s\n",
      "Wall time: 4min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "statsTeamsSet = set()\n",
    "for n in range(2014, 2020):\n",
    "    tmpstatsUrl = path + \"/inDepthSoccerStats/\"+str(n)+\"-\"+str(n+1)+\".csv\"\n",
    "    tmpstats = pd.read_csv(tmpstatsUrl, sep=',', index_col='indCol')\n",
    "    for ind, row in tmpstats.iterrows():\n",
    "        currTeams = row['teams_played_for'].split(\",\")\n",
    "        statsTeamsSet.update(currTeams)\n",
    "\n",
    "statsTeams = list(statsTeamsSet)\n",
    "teamIDDict = dict()\n",
    "i = 0\n",
    "for statsTeam in statsTeams:\n",
    "    i += 1\n",
    "    maxS = 0\n",
    "    maxId = 0\n",
    "    for tind, trow in teams.iterrows():\n",
    "        sm = SequenceMatcher(None, statsTeam, trow['name'])\n",
    "        sim = sm.ratio()\n",
    "        if(sim > maxS):\n",
    "            maxS = sim\n",
    "            maxId = tind\n",
    "\n",
    "    if(maxS < 0.95):\n",
    "        splitURL = next(search(statsTeam+\" transfermarkt startseite verein\", num_results=1)).split(\"/\")\n",
    "        #some teams contain numbers in their name, so we need to take only the suffix of the URL\n",
    "        trID = splitURL[len(splitURL) - 1]\n",
    "        if(trID != \"\"):\n",
    "            if(len(teams[teams.index == int(trID)]) == 1):\n",
    "                maxId = int(trID)\n",
    "                print(\"{:3d}\".format(i)+\" out of \"+\"{:3d}\".format(len(statsTeams))+\" GOOGLE: \"+statsTeam+\" --> \"+teams.at[maxId, 'name'])\n",
    "            else:\n",
    "                print(\"Invalid ID extracted from \"+URL)\n",
    "        else:\n",
    "            print(\"No ID in URL \"+URL)\n",
    "\n",
    "    teamIDDict[statsTeam] = maxId"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We dump the dictionary to a file in the working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('teamdict.pkl', 'wb') as f:\n",
    "    pickle.dump(teamIDDict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching players from different sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the dictionary with team IDs which is saved in the working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('teamdict.pkl', 'rb') as f:\n",
    "    teamIDDict = pickle.load(f)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add 3 new columns to the dataframe to insert also Transfermarkt IDs of the player and the teams he has played for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats['trID'] = [0] * len(stats)\n",
    "stats['team0ID'] = [0] * len(stats)\n",
    "stats['team1ID'] = [0] * len(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  18 out of 2661 PERM1 : son-heung-min --> heung-min-son\n",
      "  28 out of 2661 GOOGLE: willian --> willian\n",
      "  30 out of 2661 GOOGLE: callum-wilson --> callum-wilson\n",
      "  43 out of 2661 GOOGLE: oliver-mcburnie --> oli-mcburnie\n",
      "  50 out of 2661 GOOGLE: trezeguet --> mahmoud-trezeguet\n",
      "  51 out of 2661 GOOGLE: david-silva --> david-silva\n",
      "  64 out of 2661 GOOGLE: wesley --> wesley-moraes\n",
      "  73 out of 2661 GOOGLE: adama-traore --> adama-traore\n",
      "  75 out of 2661 GOOGLE: jorginho --> jorginho\n",
      "  95 out of 2661 GOOGLE: rodri --> rodri\n",
      " 119 out of 2661 GOOGLE: sokratis --> sokratis-papastathopoulos\n",
      " 150 out of 2661 GOOGLE: jonny --> jonny-otto\n",
      " 152 out of 2661 GOOGLE: fabinho --> fabinho\n",
      " 158 out of 2661 PERM1 : tanguy-ndombele-alvaro --> tanguy-ndombele\n",
      " 172 out of 2661 GOOGLE: matthew-longstaff --> matty-longstaff\n",
      " 193 out of 2661 PRES  : pedro --> pedro\n",
      " 207 out of 2661 GOOGLE: tom-davies --> tom-davies\n",
      " 225 out of 2661 GOOGLE: joao-moutinho --> joao-moutinho\n",
      " 256 out of 2661 PERM1 : ezri-konsa-ngoyo --> ezri-konsa\n",
      " 261 out of 2661 GOOGLE: mbwana-samatta --> ally-samatta\n",
      " 296 out of 2661 GOOGLE: jose-holebas --> jose-cholevas\n",
      " 309 out of 2661 GOOGLE: fernandinho --> fernandinho\n",
      " 317 out of 2661 GOOGLE: ben-davies --> ben-davies\n",
      " 325 out of 2661 PERM1 : ki-sung-yueng --> sung-yueng-ki\n",
      " 331 out of 2661 GOOGLE: daniel-drinkwater --> danny-drinkwater\n",
      " 336 out of 2661 GOOGLE: adam-smith --> adam-smith\n",
      " 356 out of 2661 GOOGLE: joseph-gomez --> joe-gomez\n",
      " 364 out of 2661 GOOGLE: jose-reina --> pepe-reina\n",
      " 387 out of 2661 GOOGLE: jota --> jota-peleteiro\n",
      " 391 out of 2661 GOOGLE: andre-gomes --> andre-gomes\n",
      " 392 out of 2661 GOOGLE: mat-ryan --> mathew-ryan\n",
      " 403 out of 2661 GOOGLE: emiliano-martinez --> emiliano-martinez\n",
      " 404 out of 2661 GOOGLE: matthew-james --> matty-james\n",
      " 408 out of 2661 GOOGLE: kepa --> kepa-arrizabalaga\n",
      " 410 out of 2661 GOOGLE: roberto-jimenez --> roberto\n",
      " 413 out of 2661 PRES  : bernardo --> bernardo\n",
      " 429 out of 2661 GOOGLE: ederson --> ederson\n",
      " 457 out of 2661 GOOGLE: kelland-watts --> kell-watts\n",
      " 482 out of 2661 PERM1 : arnaut-danjuma-groeneveld --> arnaut-danjuma\n",
      " 483 out of 2661 GOOGLE: reece-james --> reece-james\n",
      " 493 out of 2661 GOOGLE: william-smallbone --> will-smallbone\n",
      " 499 out of 2661 PRES  : joao-pedro --> joao-pedro\n",
      " 504 out of 2661 GOOGLE: faustino-anjorin --> tino-anjorin\n",
      " 519 out of 2661 GOOGLE: luis-suarez --> luis-suarez\n",
      " 520 out of 2661 GOOGLE: raul-garcia --> raul-garcia\n",
      " 526 out of 2661 GOOGLE: santiago-cazorla --> santi-cazorla\n",
      " 531 out of 2661 GOOGLE: roger --> roger-marti\n",
      " 534 out of 2661 GOOGLE: angel --> angel-rodriguez\n",
      " 540 out of 2661 GOOGLE: ezequiel-avila --> chimy-avila\n",
      " 543 out of 2661 PRES  : ruben-garcia --> ruben-garcia\n",
      " 546 out of 2661 GOOGLE: daniel-parejo --> dani-parejo\n",
      " 557 out of 2661 GOOGLE: anssumane-fati --> ansu-fati\n",
      " 559 out of 2661 GOOGLE: charles --> charles\n",
      " 575 out of 2661 GOOGLE: muniain --> iker-muniain\n",
      " 578 out of 2661 PERM1 : juan-camilo-hernandez --> juan-hernandez\n",
      " 579 out of 2661 GOOGLE: dani-rodriguez --> dani-rodriguez\n",
      " 583 out of 2661 GOOGLE: rafinha --> rafinha\n",
      " 584 out of 2661 GOOGLE: morales --> jose-luis-morales\n",
      " 595 out of 2661 PERM1 : wu-lei --> lei-wu\n",
      " 599 out of 2661 GOOGLE: bernardo --> bernardo-espinosa\n",
      " 609 out of 2661 GOOGLE: vitolo --> vitolo\n",
      " 615 out of 2661 GOOGLE: arthur --> arthur-melo\n",
      " 625 out of 2661 GOOGLE: nyom --> allan-nyom\n",
      " 626 out of 2661 GOOGLE: fernando --> fernando\n",
      " 635 out of 2661 GOOGLE: yuri --> yuri-berchiche\n",
      " 641 out of 2661 GOOGLE: montoro --> angel-montoro\n",
      " 653 out of 2661 GOOGLE: oier --> oier-sanjurjo\n",
      " 661 out of 2661 GOOGLE: franck-zambo --> frank-anguissa\n",
      " 662 out of 2661 PERM2 : jose-arnaiz --> jose-manuel-arnaiz\n",
      " 667 out of 2661 PRES  : joaquin-fernandez --> joaquin-fernandez\n",
      " 668 out of 2661 PERM1 : lee-kang-in --> kang-in-lee\n",
      " 673 out of 2661 GOOGLE: hernani --> hernani\n",
      " 693 out of 2661 MAXSIM: robert-ibanez --> rober-ibanez\n",
      " 697 out of 2661 GOOGLE: papakouly-diop --> pape-diop\n",
      " 700 out of 2661 GOOGLE: diego-llorente --> diego-llorente\n",
      " 703 out of 2661 GOOGLE: guerrero --> miguel-angel-guerrero\n",
      " 708 out of 2661 PRES  : marcelo --> marcelo\n",
      " 718 out of 2661 GOOGLE: aduriz --> aritz-aduriz\n",
      " 725 out of 2661 GOOGLE: raillo --> antonio-raillo\n",
      " 743 out of 2661 GOOGLE: estupinan --> pervis-estupinan\n",
      " 758 out of 2661 GOOGLE: oghenekaro-etebo --> peter-etebo\n",
      " 770 out of 2661 PERM1 : bryan-gil-salvatierra --> bryan-gil\n",
      " 776 out of 2661 GOOGLE: lumor-agbenyenu --> lumor\n",
      " 781 out of 2661 PRES  : felipe --> felipe\n",
      " 785 out of 2661 GOOGLE: fernando-nino --> fer-nino\n",
      " 787 out of 2661 PRES  : victor-garcia --> victor-garcia\n",
      " 792 out of 2661 GOOGLE: alberto-moreno --> alberto-moreno\n",
      " 798 out of 2661 PERM1 : ki-sung-yueng --> sung-yueng-ki\n",
      " 800 out of 2661 PRES  : neto --> neto\n",
      " 803 out of 2661 GOOGLE: diego-lopez --> diego-lopez\n",
      " 811 out of 2661 PRES  : javi-lopez --> javi-lopez\n",
      " 820 out of 2661 GOOGLE: yoel --> yoel-rodriguez\n",
      " 824 out of 2661 GOOGLE: tono --> tono-garcia\n",
      " 825 out of 2661 PRES  : ruben-martinez --> ruben-martinez\n",
      " 827 out of 2661 GOOGLE: sergio-alvarez --> sergio-alvarez\n",
      " 828 out of 2661 GOOGLE: zaldua --> joseba-zaldua\n",
      " 833 out of 2661 GOOGLE: oier --> oier-olazabal\n",
      " 835 out of 2661 GOOGLE: stefan-savic --> stefan-savic\n",
      " 836 out of 2661 GOOGLE: moya --> miguel-angel-moya\n",
      " 837 out of 2661 GOOGLE: gimenez --> jose-maria-gimenez\n",
      " 838 out of 2661 PRES  : sergio-alvarez --> sergio-alvarez\n",
      " 844 out of 2661 GOOGLE: sene --> josep-sene\n",
      " 846 out of 2661 GOOGLE: bruno --> bruno-soriano\n",
      " 849 out of 2661 GOOGLE: rober --> rober-pier\n",
      " 852 out of 2661 GOOGLE: aaron --> aaron-escandell\n",
      " 856 out of 2661 GOOGLE: etxeita --> xabi-etxeita\n",
      " 857 out of 2661 PERM2 : san-jose --> mikel-san-jose\n",
      " 858 out of 2661 GOOGLE: balenziaga --> mikel-balenziaga\n",
      " 859 out of 2661 GOOGLE: benat --> benat-etxebarria\n",
      " 860 out of 2661 GOOGLE: iturraspe --> ander-iturraspe\n",
      " 863 out of 2661 GOOGLE: eraso --> javi-eraso\n",
      " 870 out of 2661 GOOGLE: bruno --> bruno-gonzalez\n",
      " 871 out of 2661 PERM2 : de-marcos --> oscar-de-marcos\n",
      " 873 out of 2661 GOOGLE: lillo --> lillo-castellano\n",
      " 874 out of 2661 GOOGLE: jaume --> jaume-domenech\n",
      " 877 out of 2661 GOOGLE: sabin --> sabin-merino\n",
      " 879 out of 2661 GOOGLE: tejero --> alvaro-tejero\n",
      " 880 out of 2661 GOOGLE: amath-diedhiou --> amath-ndiaye\n",
      " 882 out of 2661 GOOGLE: remiro --> alex-remiro\n",
      " 898 out of 2661 PRES  : ximo-navarro --> ximo-navarro\n",
      " 899 out of 2661 PRES  : naldo --> naldo\n",
      " 906 out of 2661 GOOGLE: vitorino-antunes --> antunes\n",
      " 908 out of 2661 GOOGLE: aitor-fernandez --> aitor-fernandez\n",
      " 909 out of 2661 GOOGLE: kevin --> kevin-vazquez\n",
      " 915 out of 2661 PRES  : fernando-pacheco --> fernando-pacheco\n",
      " 916 out of 2661 GOOGLE: manu-garcia --> manu-garcia\n",
      " 920 out of 2661 MAXSIM: cristian-rivera --> christian-rivera\n",
      " 923 out of 2661 GOOGLE: roberto-jimenez --> roberto\n",
      " 924 out of 2661 GOOGLE: david-garcia --> david-garcia\n",
      " 926 out of 2661 GOOGLE: rui-silva --> rui-silva\n",
      " 927 out of 2661 GOOGLE: raul-garcia --> raul-carnero\n",
      " 935 out of 2661 GOOGLE: caro --> jose-antonio-caro\n",
      " 936 out of 2661 GOOGLE: antonio-moya --> toni-moya\n",
      " 943 out of 2661 GOOGLE: brandon --> brandon-thomas\n",
      " 952 out of 2661 GOOGLE: djene-dakonam --> djene\n",
      " 963 out of 2661 GOOGLE: manuel-morlanes --> manu-morlanes\n",
      " 968 out of 2661 GOOGLE: anuar-mohamed --> anuar\n",
      " 981 out of 2661 GOOGLE: rodrigo-tarin --> rodri-tarin\n",
      " 989 out of 2661 GOOGLE: xavier-quintilla --> xavi-quintilla\n",
      " 998 out of 2661 GOOGLE: manuel-sanchez --> manu-sanchez\n",
      "1010 out of 2661 GOOGLE: aleksander-sedlar --> aleksandar-sedlar\n",
      "1029 out of 2661 PRES  : raul-garcia --> raul-garcia\n",
      "1040 out of 2661 PRES  : pablo-martinez --> pablo-martinez\n",
      "1041 out of 2661 GOOGLE: eliseo --> eliseo-falcon\n",
      "1044 out of 2661 PERM1 : miguel-angel-atienza --> miguel-atienza\n",
      "1047 out of 2661 PRES  : javi-martinez --> javi-martinez\n",
      "1049 out of 2661 GOOGLE: sofian-chakla --> soufiane-chakla\n",
      "1055 out of 2661 PERM1 : jose-luis-rodriguez --> jose-rodriguez\n",
      "1056 out of 2661 GOOGLE: javier-lopez --> javi-lopez\n",
      "1057 out of 2661 PERM1 : mahmoud-abdallahi --> abdallahi-mahmoud\n",
      "1061 out of 2661 PERM1 : jon-pacheco-dozagarat --> jon-pacheco\n",
      "1065 out of 2661 GOOGLE: nicolas-melamed --> nico-melamed\n",
      "1067 out of 2661 GOOGLE: manuel-garrido --> manu-garrido\n",
      "1070 out of 2661 MAXSIM: adria-guerrero --> adrian-guerrero\n",
      "1071 out of 2661 MAXSIM: rafael-obrador --> rafel-obrador\n",
      "1084 out of 2661 PRES  : andre-silva --> andre-silva\n",
      "1106 out of 2661 PERM1 : gian-luca-waldschmidt --> luca-waldschmidt\n",
      "1125 out of 2661 PRES  : jonas-hofmann --> jonas-hofmann\n",
      "1167 out of 2661 GOOGLE: joshua-sargent --> josh-sargent\n",
      "1176 out of 2661 GOOGLE: thiago-alcantara --> thiago\n",
      "1189 out of 2661 PRES  : paulinho --> paulinho\n",
      "1221 out of 2661 PERM1 : kwon-chang-hoon --> chang-hoon-kwon\n",
      "1228 out of 2661 GOOGLE: joao-victor --> victor-sa\n",
      "1231 out of 2661 GOOGLE: cauly-oliveira-souza --> cauly\n",
      "1232 out of 2661 GOOGLE: jeremiah-st.-juste --> jerry-st-juste\n",
      "1290 out of 2661 PRES  : william --> william\n",
      "1293 out of 2661 GOOGLE: bote-baku --> ridle-baku\n",
      "1294 out of 2661 GOOGLE: kasim-nuhu --> kasim-adams\n",
      "1319 out of 2661 PERM2 : john-brooks --> john-anthony-brooks\n",
      "1321 out of 2661 PERM2 : per-skjelbred --> per-ciljan-skjelbred\n",
      "1323 out of 2661 GOOGLE: yunus-malli --> yunus-mallı\n",
      "1348 out of 2661 PRES  : wendell --> wendell\n",
      "1358 out of 2661 PRES  : javi-martinez --> javi-martinez\n",
      "1414 out of 2661 PRES  : aaron-martin --> aaron-martin\n",
      "1422 out of 2661 GOOGLE: dayotchanculle-upamecano --> dayot-upamecano\n",
      "1434 out of 2661 PERM1 : gian-luca-itter --> luca-itter\n",
      "1457 out of 2661 PERM1 : kwasi-okyere-wriedt --> kwasi-wriedt\n",
      "1463 out of 2661 GOOGLE: louis-beyer --> jordan-beyer\n",
      "1475 out of 2661 MAXSIM: philipp-mwene --> phillipp-mwene\n",
      "1476 out of 2661 PERM1 : leandro-barreiro-martins --> leandro-barreiro\n",
      "1502 out of 2661 PRES  : mads-pedersen --> mads-pedersen\n",
      "1510 out of 2661 GOOGLE: munir-mercan --> levent-mercan\n",
      "1540 out of 2661 PERM1 : samuel-kari-fridjonsson --> samuel-fridjonsson\n",
      "1556 out of 2661 PRES  : jonas-hofmann --> jonas-hofmann\n",
      "1565 out of 2661 GOOGLE: joao-pedro --> joao-pedro\n",
      "1600 out of 2661 GOOGLE: ruslan-malinovskiy --> ruslan-malinovskyi\n",
      "1616 out of 2661 GOOGLE: luis-alberto --> luis-alberto\n",
      "1633 out of 2661 GOOGLE: marco-faraoni --> davide-faraoni\n",
      "1639 out of 2661 GOOGLE: kostas-manolas --> konstantinos-manolas\n",
      "1659 out of 2661 GOOGLE: aaron-ramsey --> aaron-ramsey\n",
      "1677 out of 2661 GOOGLE: fabian --> fabian-ruiz\n",
      "1689 out of 2661 GOOGLE: danilo --> danilo\n",
      "1695 out of 2661 GOOGLE: allan --> allan\n",
      "1713 out of 2661 GOOGLE: berat-gjimshiti --> berat-djimsiti\n",
      "1716 out of 2661 PRES  : danilo --> danilo\n",
      "1745 out of 2661 GOOGLE: samir --> samir-caetano\n",
      "1774 out of 2661 PRES  : romulo --> romulo\n",
      "1793 out of 2661 GOOGLE: luiz-felipe --> luiz-felipe\n",
      "1797 out of 2661 GOOGLE: rogerio --> rogerio\n",
      "1814 out of 2661 PERM1 : wilfried-stephane-singo --> wilfried-singo\n",
      "1822 out of 2661 GOOGLE: julian-chabot --> jeff-chabot\n",
      "1831 out of 2661 MAXSIM: evgen-shakhov --> yevgen-shakhov\n",
      "1834 out of 2661 PERM1 : amad-diallo-traore --> amad-diallo\n",
      "1859 out of 2661 GOOGLE: gabriel --> gabriel\n",
      "1860 out of 2661 PRES  : felipe --> felipe\n",
      "1897 out of 2661 MAXSIM: gianmarco-ferrari --> gian-marco-ferrari\n",
      "1900 out of 2661 GOOGLE: jose-reina --> pepe-reina\n",
      "1907 out of 2661 GOOGLE: bruno-alves --> bruno-alves\n",
      "1910 out of 2661 PRES  : rafael --> rafael\n",
      "1962 out of 2661 MAXSIM: antonio-la-gumina --> antonino-la-gumina\n",
      "1992 out of 2661 GOOGLE: marlon-santos --> marlon\n",
      "1994 out of 2661 GOOGLE: hernani --> hernani\n",
      "2002 out of 2661 PRES  : andre-silva --> andre-silva\n",
      "2023 out of 2661 GOOGLE: ndary-adopo --> michel-adopo\n",
      "2048 out of 2661 GOOGLE: ibanez --> roger-ibanez\n",
      "2065 out of 2661 MAXSIM: igor-julio --> igor-juliao\n",
      "2080 out of 2661 PRES  : yildirim-mert-cetin --> mert-cetin\n",
      "2084 out of 2661 GOOGLE: georgios-kyriakopoulos --> georgios-kyriakopoulos\n",
      "2090 out of 2661 PRES  : pedro --> pedro\n",
      "2093 out of 2661 GOOGLE: sinan-gumus --> sinan-gumuş\n",
      "2132 out of 2661 PERM1 : kylian-mbappe-lottin --> kylian-mbappe\n",
      "2146 out of 2661 GOOGLE: sehrou-guirassy --> serhou-guirassy\n",
      "2163 out of 2661 PERM1 : hwang-ui-jo --> ui-jo-hwang\n",
      "2171 out of 2661 GOOGLE: john-mendoza --> stiven-mendoza\n",
      "2176 out of 2661 GOOGLE: keita --> keita-balde\n",
      "2179 out of 2661 PRES  : pablo --> pablo\n",
      "2195 out of 2661 PRES  : marquinhos --> marquinhos\n",
      "2199 out of 2661 PRES  : abdoulaye-toure --> abdoulaye-toure\n",
      "2224 out of 2661 PRES  : pablo-martinez --> pablo-martinez\n",
      "2245 out of 2661 MAXSIM: dmitri-lienard --> dimitri-lienard\n",
      "2248 out of 2661 PRES  : otavio --> otavio\n",
      "2254 out of 2661 MAXSIM: yassin-benrahou --> yassine-benrahou\n",
      "2257 out of 2661 GOOGLE: nianzou-kouassi --> tanguy-nianzou\n",
      "2259 out of 2661 PRES  : moussa-kone --> moussa-kone\n",
      "2273 out of 2661 PRES  : pedro-mendes --> pedro-mendes\n",
      "2290 out of 2661 PRES  : adama-traore --> adama-traore\n",
      "2307 out of 2661 GOOGLE: digbo-maiga --> habib-maiga\n",
      "2314 out of 2661 PERM1 : joia-nuno-da-costa --> nuno-da-costa\n",
      "2324 out of 2661 PERM1 : suk-hyun-jun --> hyun-jun-suk\n",
      "2334 out of 2661 PERM1 : juan-ferney-otero --> juan-otero\n",
      "2336 out of 2661 GOOGLE: naif-aguerd --> nayef-aguerd\n",
      "2349 out of 2661 PERM2 : pereira-lage --> mathias-pereira-lage\n",
      "2352 out of 2661 GOOGLE: yusuf-yazici --> yusuf-yazıcı\n",
      "2359 out of 2661 PRES  : marcelo --> marcelo\n",
      "2361 out of 2661 PRES  : luiz-gustavo --> luiz-gustavo\n",
      "2372 out of 2661 PRES  : matheus-pereira --> matheus-pereira\n",
      "2378 out of 2661 PRES  : fabio --> fabio\n",
      "2380 out of 2661 PRES  : gabriel-silva --> gabriel-silva\n",
      "2390 out of 2661 PRES  : danilo --> danilo\n",
      "2392 out of 2661 PRES  : alvaro-gonzalez --> alvaro-gonzalez\n",
      "2402 out of 2661 PRES  : rafael --> rafael\n",
      "2404 out of 2661 PRES  : thiago-silva --> thiago-silva\n",
      "2416 out of 2661 PERM1 : jacques-alaixys-romao --> alaixys-romao\n",
      "2450 out of 2661 GOOGLE: jordan-siebatcheu --> jordan\n",
      "2460 out of 2661 GOOGLE: kelvin-adou --> kelvin-amian\n",
      "2471 out of 2661 GOOGLE: nicolas-gaitan --> nico-gaitan\n",
      "2485 out of 2661 GOOGLE: fernando-marcal --> marcal\n",
      "2494 out of 2661 MAXSIM: sami-benamar --> sami-ben-amar\n",
      "2496 out of 2661 PERM1 : james-edward-lea-siliki --> james-lea-siliki\n",
      "2497 out of 2661 GOOGLE: zaydou-youssef --> zaydou-youssouf\n",
      "2498 out of 2661 PERM2 : fode-toure --> fode-ballo-toure\n",
      "2501 out of 2661 GOOGLE: prince --> prince-gouano\n",
      "2534 out of 2661 PRES  : ibrahima-diallo --> ibrahima-diallo\n",
      "2535 out of 2661 GOOGLE: percy-ruiz --> percy-prado\n",
      "2538 out of 2661 PERM1 : mehmet-zeki-celik --> zeki-celik\n",
      "2561 out of 2661 PERM1 : benoit-badiashile-mukinayi --> benoit-badiashile\n",
      "2563 out of 2661 MAXSIM: kephren-thuram --> khephren-thuram\n",
      "2564 out of 2661 PERM1 : thody-elie-youan --> elie-youan\n",
      "2566 out of 2661 PERM1 : charles-nathan-abi --> charles-abi\n",
      "2574 out of 2661 GOOGLE: reinildo --> reinildo-mandava\n",
      "2575 out of 2661 GOOGLE: kouadio-kone --> manu-kone\n",
      "2578 out of 2661 PRES  : ibrahim-cisse --> ibrahim-cisse\n",
      "2579 out of 2661 PERM1 : albert-nicolas-lottin --> albert-lottin\n",
      "2581 out of 2661 MAXSIM: timothee-nkada --> timothe-nkada\n",
      "2587 out of 2661 PERM1 : nathan-ngoumou-minpol --> nathan-ngoumou\n",
      "2593 out of 2661 PRES  : sambou-sissoko --> sambou-sissoko\n",
      "2615 out of 2661 PERM1 : lucas-lionel-dias --> lucas-dias\n",
      "2616 out of 2661 PERM1 : cristian-benavente-bristol --> cristian-benavente\n",
      "2621 out of 2661 PRES  : ibrahim-cisse --> ibrahim-cisse\n",
      "2633 out of 2661 GOOGLE: hichem-boudaoui --> hicham-boudaoui\n",
      "2635 out of 2661 GOOGLE: mathis-cherki --> rayan-cherki\n",
      "2639 out of 2661 PERM1 : ahmad-toure-ngouyamsa-nounchili --> ahmad-ngouyamsa\n",
      "2640 out of 2661 PERM1 : edmilson-indjai-correia --> edmilson-correia\n",
      "2644 out of 2661 MAXSIM: lucas-valls --> luca-valls\n",
      "2652 out of 2661 PERM1 : yun-il-lok --> il-lok-yun\n",
      "CPU times: total: 3min 6s\n",
      "Wall time: 9min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#select only months and years from appearance dates\n",
    "appMonth = app['date'].str.split(\"/\").str[1]\n",
    "appYear = app['date'].str.split(\"/\").str[2]\n",
    "#select appearances from this season\n",
    "isThisSeas = ((appYear == str(y)) & (appMonth >= \"08\")) | ((appYear == str(y+1)) & (appMonth <= \"06\"))\n",
    "appThisSeas = app[isThisSeas]\n",
    "playerCodes = cleanChars(players['player_code'])\n",
    "\n",
    "#iterate on stats file\n",
    "statsRows = np.size(stats, 0);\n",
    "toComplRows = len(stats[stats['trID'] == 0])\n",
    "exact_matches = no_matches = resolved_google = resolved_permS = resolved_permP = resolved_max_sim = resolved_pres = i = 0\n",
    "for index, row in stats.iterrows():\n",
    "    if(row['trID'] == 0):\n",
    "        i += 1\n",
    "        mode = \"NONE\"\n",
    "        player = pd.Series([])\n",
    "        statsName = hyphenize(row['player_name']).replace(\"'\",\"\")\n",
    "        \n",
    "        matchedPlayers = players[playerCodes == statsName]\n",
    "    \n",
    "        #multiple rows with same name in stats mapped to a single player are ok (he has changed team during the season)\n",
    "        if(np.size(matchedPlayers, 0) > 0):\n",
    "            mode = \"MATCH\"\n",
    "           \n",
    "        if(mode == \"NONE\"):\n",
    "            #split name in stats and use permutations strategy\n",
    "            splitStatsName = statsName.split(\"-\")\n",
    "            if(len(splitStatsName) >= 2):\n",
    "                matchedCodes = match_seq(splitStatsName, playerCodes)\n",
    "                if(len(matchedCodes) > 0):\n",
    "                    matchedPlayers = players[playerCodes.isin(matchedCodes)]\n",
    "                    mode = \"PERM1\"\n",
    "    \n",
    "        if(mode == \"NONE\"):\n",
    "            maxSim = 0\n",
    "            maxC = \"\"\n",
    "            for c in playerCodes:\n",
    "                sm = SequenceMatcher(None, statsName, c)\n",
    "                #do not proceed if the upper bound is too small\n",
    "                if(sm.real_quick_ratio() >= 0.5):\n",
    "                    #remember: similarity is not commutative\n",
    "                    simm = sm.ratio()                  \n",
    "                    #if sim is big enough, try permutation strategy with name from players file\n",
    "                    if(simm >= 0.6):\n",
    "                        splitC = c.split(\"-\")\n",
    "                        if(len(splitC) >= len(splitStatsName) and len(splitStatsName) >= 2):\n",
    "                            matchedCodes = match_seq(splitC, [statsName])\n",
    "                            if(len(matchedCodes) > 0):\n",
    "                                newMatchedPlayers = players[players['player_code'] == c]\n",
    "                                matchedPlayers = pd.concat([matchedPlayers, newMatchedPlayers])\n",
    "                    if(simm > maxSim):\n",
    "                        maxSim = simm\n",
    "                        maxC = c\n",
    "    \n",
    "            if(maxSim >= 0.95):\n",
    "                matchedPlayers = players[playerCodes == maxC]\n",
    "                mode = \"MAXSIM\"\n",
    "            elif(np.size(matchedPlayers, 0) > 0):\n",
    "                mode = \"PERM2\"\n",
    "    \n",
    "        #managing results of any method\n",
    "        matches = np.size(matchedPlayers, 0)\n",
    "        if(matches == 1):\n",
    "            player = matchedPlayers.iloc[0]\n",
    "            trID = matchedPlayers.index[0]\n",
    "        if(matches > 1):\n",
    "            player, trID = solve_with_apps_approx(row['games'], matchedPlayers, appThisSeas, 3)\n",
    "            if(trID == -1):\n",
    "                mode = \"NONE\"\n",
    "            else:\n",
    "                mode = \"PRES\"\n",
    "        if(mode == \"NONE\"):\n",
    "            splitURL = next(search(row['player_name']+\" \"+row['teams_played_for']+\" transfermarkt profil spieler\", num_results=1)).split(\"/\")\n",
    "            trID = splitURL[len(splitURL) - 1]\n",
    "            if(trID != \"\"):\n",
    "                urlPlayers = players[players.index == int(trID)]\n",
    "                if(len(urlPlayers) != 0):\n",
    "                    mode = \"GOOGLE\"\n",
    "                    player = urlPlayers.iloc[0]\n",
    "                else:\n",
    "                    print(\"!!! Invalid ID \"+trID+\" extracted from \"+str(splitURL))\n",
    "                    mode = \"NONE\"\n",
    "            else:\n",
    "                print(\"!!! No ID in URL \"+str(splitURL))\n",
    "                mode = \"NONE\"\n",
    "        \n",
    "        \n",
    "        if(mode == \"NONE\"):\n",
    "            no_matches += 1\n",
    "        elif(mode == \"PERM1\"):\n",
    "            resolved_permS += 1\n",
    "        elif(mode == \"PERM2\"):\n",
    "            resolved_permP += 1\n",
    "        elif(mode == \"GOOGLE\"):\n",
    "            resolved_google += 1\n",
    "        elif(mode == \"MATCH\"):\n",
    "            exact_matches += 1\n",
    "        elif(mode == \"MAXSIM\"):\n",
    "            resolved_max_sim += 1\n",
    "        elif(mode == \"PRES\"):\n",
    "            resolved_pres += 1\n",
    "    \n",
    "        if(mode == \"NONE\"):\n",
    "            print(\"{:4d}\".format(i)+\" out of \"+str(toComplRows)+\" NONE  : \"+statsName+\", \"+row['teams_played_for']+\", matches: \"+str(matches))\n",
    "        elif(mode != \"MATCH\"):\n",
    "            print(\"{:4d}\".format(i)+\" out of \"+str(toComplRows)+\" \"+mode.ljust(6)+\": \"+statsName+\" --> \"+player['player_code'])\n",
    "    \n",
    "        if(mode != \"NONE\"):\n",
    "            stats.at[index, 'trID'] = int(trID)\n",
    "            teamList = row['teams_played_for'].split(\",\")\n",
    "            stats.at[index, 'team0ID'] = int(teamIDDict[teamList[0]])\n",
    "            if(len(teamList) == 2):\n",
    "                stats.at[index, 'team1ID'] = int(teamIDDict[teamList[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   --- STATISTICS ---\n",
      "Total matches:                             2661 -- percentage: 100.00%\n",
      "  ---> exact matches:                      2376 -- percentage: 89.29%\n",
      "  ---> resolved permutating statsName:       40 -- percentage: 1.50%\n",
      "  ---> resolved permutating player code:      7 -- percentage: 0.26%\n",
      "  ---> resolved with max sim.:               15 -- percentage: 0.56%\n",
      "  ---> resolved with apps:                   55 -- percentage: 2.07%\n",
      "  ---> resolved with google:                168 -- percentage: 6.31%\n",
      "No matches:                                   0 -- percentage: 0.00%\n",
      "  ---> zero matches found:                    0 -- percentage: 0.00%\n"
     ]
    }
   ],
   "source": [
    "#print statistics\n",
    "print(\"   --- STATISTICS ---\")\n",
    "tot_matches = exact_matches + + resolved_google + resolved_permS + resolved_permP + resolved_max_sim + resolved_pres\n",
    "print(\"Total matches:                            \"+\"{:5d}\".format(tot_matches)+\" -- percentage: \" + \"{:.2f}%\".format(tot_matches*100/statsRows))\n",
    "print(\"  ---> exact matches:                     \"+\"{:5d}\".format(exact_matches)+\" -- percentage: \" + \"{:.2f}%\".format(exact_matches*100/statsRows)) \n",
    "print(\"  ---> resolved permutating statsName:    \"+\"{:5d}\".format(resolved_permS)+\" -- percentage: \" + \"{:.2f}%\".format(resolved_permS*100/statsRows))\n",
    "print(\"  ---> resolved permutating player code:  \"+\"{:5d}\".format(resolved_permP)+\" -- percentage: \" + \"{:.2f}%\".format(resolved_permP*100/statsRows))\n",
    "print(\"  ---> resolved with max sim.:            \"+\"{:5d}\".format(resolved_max_sim)+\" -- percentage: \" + \"{:.2f}%\".format(resolved_max_sim*100/statsRows))\n",
    "print(\"  ---> resolved with apps:                \"+\"{:5d}\".format(resolved_pres)+\" -- percentage: \" + \"{:.2f}%\".format(resolved_pres*100/statsRows))\n",
    "print(\"  ---> resolved with google:              \"+\"{:5d}\".format(resolved_google)+\" -- percentage: \" + \"{:.2f}%\".format(resolved_google*100/statsRows))\n",
    "print(\"No matches:                               \"+\"{:5d}\".format(no_matches)+\" -- percentage: \" + \"{:.2f}%\".format(no_matches*100/statsRows))\n",
    "print(\"  ---> zero matches found:                \"+\"{:5d}\".format(no_matches)+\" -- percentage: \" + \"{:.2f}%\".format((no_matches)*100/statsRows))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the dataframe contains TM IDs, which will be used like foreign keys for matching purposes. Therefore, we save the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the dataframe\n",
    "stats.to_csv(\"stats\"+shortSeas+\"_IDs.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Completing and correcting statistics in corner cases\n",
    "We need to manage the fact some rows contain total information about a player switching team in the same league during the season. <br>\n",
    "We can use information in the FBref files to complete our data; we have observed that, in this situation, it contains correct information only for the row of the two which has lower index: some statistics in the second row can be therefore corrected by subtracting the ones in the first row from the total ones. <br>\n",
    "\n",
    "Observe that we load stats to make this cell idempotent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cédric Soares\n",
      "José Arnáiz\n",
      "Carles Aleñá\n",
      "Alejandro Pozo\n",
      "Kévin Rodrigues\n",
      "Bryan Gil Salvatierra\n",
      "Bruno\n",
      "Raúl García\n",
      "Eduard Löwen\n",
      "Antonin Barak\n",
      "Rômulo\n",
      "Nehuén Paz\n",
      "Igor Julio\n",
      "Jeff Reine-Adelaide\n",
      "Adama Traoré\n",
      "Could match: 56 could not: 15\n"
     ]
    }
   ],
   "source": [
    "#we will use copies of row of the df, so we silence the warnings that would arise\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "statsFBref = pd.read_csv(statsFBrefUrl, sep=';', index_col='Column1', dtype={\"Attendance\": \"string\"})\n",
    "statsFBref['player'] = [unidecode(name) for name in statsFBref['player']]\n",
    "\n",
    "path = str(Path(os.path.abspath(os.getcwd())))\n",
    "statsUrl = path + '/stats'+shortSeas+'_IDs.csv'\n",
    "stats = pd.read_csv(statsUrl, sep=',', index_col='indCol')\n",
    "\n",
    "m = nm = 0\n",
    "newRows = []\n",
    "for ind, row in stats.iterrows():\n",
    "    if(row['team1ID'] != 0):\n",
    "        player = players.loc[row['trID']]\n",
    "        fbmatch = statsFBref[statsFBref['player'] == unidecode(player['name'])]\n",
    "        if(len(fbmatch) == 2):\n",
    "            m += 1\n",
    "            team0ID, team1ID = row['team0ID'], row['team1ID']\n",
    "            team0 = teams.loc[team0ID]['name']\n",
    "\n",
    "            #select the index pointing to wrong row\n",
    "            if(fbmatch.index[0] < fbmatch.index[1]):\n",
    "                corrInd, wrongInd = fbmatch.index\n",
    "            else:\n",
    "                wrongInd, corrInd = fbmatch.index\n",
    "\n",
    "            sm0 = SequenceMatcher(None, team0, fbmatch.loc[corrInd]['squad'])\n",
    "            sm1 = SequenceMatcher(None, team0, fbmatch.loc[wrongInd]['squad'])\n",
    "\n",
    "            #if the team corresponding to first ID matches with fbref row with index wrongInd, swap team IDs\n",
    "            if(sm0.ratio() < sm1.ratio()):\n",
    "                team0ID, team1ID = row['team1ID'], row['team0ID']\n",
    "\n",
    "            corrRow = stats.loc[ind]\n",
    "            wrongRow = stats.loc[ind]\n",
    "            \n",
    "            corrRow.at['teams_played_for'] = teams.loc[team0ID]['name']\n",
    "            corrRow.at['team0ID'] = team0ID\n",
    "            corrRow.at['team1ID'] = 0\n",
    "            corrRow.at['games'] = statsFBref.at[corrInd, 'games']\n",
    "            corrRow.at['goals'] = statsFBref.at[corrInd, 'goals']\n",
    "            corrRow.at['minutes_played'] = statsFBref.at[corrInd, 'minutes']\n",
    "            corrRow.at['npg'] = statsFBref.at[corrInd, 'goals'] - statsFBref.at[corrInd, 'pens_made']\n",
    "            corrRow.at['assists'] = statsFBref.at[corrInd, 'assists']\n",
    "            corrRow.at['xG'] = statsFBref.at[corrInd, 'xg']\n",
    "            corrRow.at['xG90'] = corrRow['xG'] / corrRow['minutes_played'] * 90\n",
    "            corrRow.at['npxG'] = statsFBref.at[corrInd, 'npxg']\n",
    "            corrRow.at['npxG90'] = corrRow['npxG'] / corrRow['minutes_played'] * 90\n",
    "            corrRow.at['xA'] = statsFBref.at[corrInd, 'xa']\n",
    "            corrRow.at['xA90'] = corrRow['xA'] / corrRow['minutes_played'] * 90\n",
    "            corrRow.at['yellow_cards'] = statsFBref.at[corrInd, 'cards_yellow']\n",
    "            corrRow.at['red_cards'] = statsFBref.at[corrInd, 'cards_red']\n",
    "            corrRow.at['shots'] = statsFBref.at[corrInd, 'shots_total']\n",
    "\n",
    "            wrongRow.at['teams_played_for'] = teams.loc[team1ID]['name']\n",
    "            wrongRow.at['team0ID'] = team1ID\n",
    "            wrongRow.at['team1ID'] = 0\n",
    "            wrongRow.at['games'] = row['games'] - corrRow['games']\n",
    "            wrongRow.at['goals'] = row['goals'] - corrRow['goals']\n",
    "            wrongRow.at['minutes_played'] = row['minutes_played'] - corrRow['minutes_played']\n",
    "            wrongRow.at['npg'] = row['npg'] - corrRow['npg']\n",
    "            wrongRow.at['assists'] = row['assists'] - corrRow['assists']\n",
    "            #xG could result < 0 for different roundings or slightly different calculations\n",
    "            #therefore, in such cases, setting it to 0 could be reasonable\n",
    "            wrongRow.at['xG'] = max(row['xG'] - corrRow['xG'], 0)\n",
    "            wrongRow.at['xG90'] = wrongRow['xG'] / wrongRow['minutes_played'] * 90\n",
    "            wrongRow.at['npxG'] = max(row['npxG'] - corrRow['npxG'], 0)\n",
    "            wrongRow.at['npxG90'] = wrongRow['npxG'] / wrongRow['minutes_played'] * 90\n",
    "            wrongRow.at['xA'] = max(row['xA'] - corrRow['xA'], 0)\n",
    "            wrongRow.at['xA90'] = wrongRow['xA'] / wrongRow['minutes_played'] * 90\n",
    "            wrongRow.at['yellow_cards'] = row['yellow_cards'] - corrRow['yellow_cards']\n",
    "            #shots could result negative for the same reason as above\n",
    "            wrongRow.at['shots'] = max(row['shots'] - corrRow['shots'], 0)\n",
    "\n",
    "            #we lack of data for these columns, so we mark this lack for later\n",
    "            corrRow.at['xGBuildup'] = corrRow['xGChain'] = wrongRow.at['xGBuildup'] = wrongRow['xGChain'] = -1\n",
    "            corrRow.at['key_passes'] = wrongRow.at['key_passes'] = -1\n",
    "            \n",
    "            newRows.append(corrRow.values)\n",
    "            newRows.append(wrongRow.values)\n",
    "\n",
    "        else:\n",
    "            nm += 1\n",
    "            print(row['player_name'])\n",
    "            \n",
    "print(\"Could match: \"+str(m)+\" could not: \"+str(nm))\n",
    "#create the new dataframe to be appended to the old one\n",
    "startInd = stats.index[len(stats) - 1] + 1\n",
    "newStats = pd.DataFrame(newRows, columns=stats.columns, index=range(startInd,startInd+len(newRows)))\n",
    "stats = pd.concat([stats, newStats])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the dataframe\n",
    "stats.to_csv(\"stats\"+shortSeas+\"_IDs.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Namespaces and prefixes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This snippet is repeated below to save serializations in different files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the soccer stats ontology namespace\n",
    "#CNS = Namespace(\"http://eulersharp.sourceforge.net/2003/03swap/countries#\")\n",
    "DCSSO = Namespace(\"http://www.dei.unipd.it/db2/dcsso#\")\n",
    "\n",
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "#g.bind(\"countries\", CNS)\n",
    "g.bind(\"dcsso\", DCSSO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for ind, row in countries.iterrows():\n",
    "    country = URIRef(DCSSO[row['Alpha-2 code']])\n",
    "    g.add((country, RDF.type, DCSSO.Country))\n",
    "    g.add((country, FOAF.name, Literal(cleanString(ind), datatype=XSD.string)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Leagues\n",
    "Leagues are added manually because we need to store only five using very limited information from the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "SerieA = URIRef(DCSSO[\"IT1\"])\n",
    "g.add((SerieA, RDF.type, DCSSO.League))\n",
    "g.add((SerieA, FOAF['name'], Literal(\"Serie A\", datatype=XSD.string)))\n",
    "g.add((SerieA, DCSSO['hasCountry'], URIRef(DCSSO[\"IT\"])))\n",
    "\n",
    "Ligue1 = URIRef(DCSSO[\"FR1\"])\n",
    "g.add((Ligue1, RDF.type, DCSSO.League))\n",
    "g.add((Ligue1, FOAF['name'], Literal(\"Ligue 1\", datatype=XSD.string)))\n",
    "g.add((Ligue1, DCSSO['hasCountry'], URIRef(DCSSO[\"FR\"])))\n",
    "\n",
    "LaLiga = URIRef(DCSSO[\"ES1\"])\n",
    "g.add((LaLiga, RDF.type, DCSSO.League))\n",
    "g.add((LaLiga, FOAF['name'], Literal(\"LaLiga\", datatype=XSD.string)))\n",
    "g.add((LaLiga, DCSSO['hasCountry'], URIRef(DCSSO[\"ES\"])))\n",
    "\n",
    "Premier = URIRef(DCSSO[\"GB1\"])\n",
    "g.add((Premier, RDF.type, DCSSO.League))\n",
    "g.add((Premier, FOAF['name'], Literal(\"Premier League\", datatype=XSD.string)))\n",
    "g.add((Premier, DCSSO['hasCountry'], URIRef(DCSSO[\"GB\"])))\n",
    "\n",
    "Bundesliga = URIRef(DCSSO[\"L1\"])\n",
    "g.add((Bundesliga, RDF.type, DCSSO.League))\n",
    "g.add((Bundesliga, FOAF['name'], Literal(\"Bundesliga\", datatype=XSD.string)))\n",
    "g.add((Bundesliga, DCSSO['hasCountry'], URIRef(DCSSO[\"DE\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We serialize the graph and save the output with turtle syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'countries_leagues.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now clean the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the soccer stats ontology namespace\n",
    "DCSSO = Namespace(\"http://www.dei.unipd.it/db2/dcsso#\")\n",
    "\n",
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"dcsso\", DCSSO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Teams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the dictionary with team IDs, to understand which teams need to be stored in the DB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 31.2 ms\n",
      "Wall time: 73.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "with open('teamdict.pkl', 'rb') as f:\n",
    "    teamIDDict = pickle.load(f)\n",
    "\n",
    "for tID in set(teamIDDict.values()):\n",
    "    team = teams.loc[tID]\n",
    "    Team = URIRef(DCSSO[\"team\"+str(tID)])\n",
    "    g.add((Team, RDF.type, DCSSO.Team))\n",
    "    g.add((Team, FOAF.name, Literal(cleanString(team['name']), datatype=XSD.string)))\n",
    "    g.add((Team, DCSSO['participatesIn'], URIRef(DCSSO[team['domestic_competition_id']])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "CPU times: total: 31.2 ms\n",
      "Wall time: 44.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'teams.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the soccer stats ontology namespace\n",
    "DCSSO = Namespace(\"http://www.dei.unipd.it/db2/dcsso#\")\n",
    "\n",
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"dcsso\", DCSSO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Team participations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for tID in set(teamIDDict.values()):\n",
    "    Team = URIRef(DCSSO[\"team\"+str(tID)])\n",
    "    for year in range(2014, 2020):\n",
    "        #check if there are any games for this team in season starting in a year\n",
    "        if((games[games['season'] == year]['home_club_id'] == tID).any() == True):\n",
    "            #if there are any, 'year' represents a season in which this team has played in its domestic top league\n",
    "            Participation = URIRef(DCSSO[\"part\"+str(tID)+\"s\"+str(year)])\n",
    "            g.add((Participation, RDF.type, DCSSO.SeasonalParticipation))\n",
    "            g.add((Team, DCSSO['hasParticipation'], Participation))\n",
    "            g.add((Participation, DCSSO['season'], Literal(year, datatype=XSD.int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "CPU times: total: 141 ms\n",
      "Wall time: 207 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'team_part.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the soccer stats ontology namespace\n",
    "DCSSO = Namespace(\"http://www.dei.unipd.it/db2/dcsso#\")\n",
    "\n",
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"dcsso\", DCSSO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% completed.\n",
      "CPU times: total: 3.64 s\n",
      "Wall time: 7.04 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "players['first_name'] = players['first_name'].fillna(\"\")\n",
    "players['last_name'] = players['last_name'].fillna(\"\")\n",
    "players['country_of_citizenship'] = players['country_of_citizenship'].fillna(\"\")\n",
    "players['height_in_cm'] = players['height_in_cm'].fillna(0)\n",
    "players['date_of_birth'] = players['date_of_birth'].fillna(\"\")\n",
    "\n",
    "#collect all player IDs in the files produced above\n",
    "plIDSet = set()\n",
    "for n in range(14, 20):\n",
    "    tmpstats = pd.read_csv('stats'+str(n)+str(n+1)+'_IDs.csv', sep=',')\n",
    "    for ind, row in tmpstats.iterrows():\n",
    "        if(row['trID'] != 0):\n",
    "            plIDSet.add(row['trID'])\n",
    "\n",
    "c = 0\n",
    "for id in plIDSet:\n",
    "    c+=1\n",
    "    if(int(100*(c-1)/len(plIDSet)) < int(100*c/len(plIDSet))):\n",
    "        clear_output(wait=True)\n",
    "        print(str(int(100*c/len(plIDSet)))+\"% completed.\"+\".\"*(c%3))\n",
    "    player = players.loc[id]\n",
    "    # the node has the namespace + the transfermarkt ID as URI\n",
    "    ref = \"player\"+str(id)\n",
    "    Footballer = URIRef(DCSSO[ref])\n",
    "    g.add((Footballer, RDF.type, DCSSO.Footballer))\n",
    "    if(player['first_name'] != \"\"):\n",
    "        g.add((Footballer, FOAF['firstName'], Literal(cleanString(player['first_name']), datatype=XSD.string)))\n",
    "    if(player['last_name'] != \"\"):\n",
    "        g.add((Footballer, FOAF['familyName'], Literal(cleanString(player['last_name']), datatype=XSD.string)))\n",
    "    if(player['date_of_birth'] != \"\"):\n",
    "        g.add((Footballer, DCSSO['birthdate'], Literal(player['date_of_birth'][:4], datatype=XSD.int)))\n",
    "    g.add((Footballer, FOAF['name'], Literal(cleanString(player['name']), datatype=XSD.string)))\n",
    "    if(player['country_of_citizenship'] != \"\"):\n",
    "        g.add((Footballer, DCSSO['hasCitizenship'], DCSSO[countries.loc[player['country_of_citizenship']]['Alpha-2 code']]))\n",
    "    if(player['height_in_cm'] != 0 and np.isnan(player['height_in_cm']) == False):\n",
    "        g.add((Footballer, DCSSO['height'], Literal(player['height_in_cm'], datatype=XSD.int)))\n",
    "\n",
    "    if(player['position'] != \"Missing\"):\n",
    "        subPosition = player['sub_position'].replace(\" \", \"\").replace(\"-\", \"\")\n",
    "        if(player['position'] == \"Goalkeeper\" or player['position'] == \"Defender\"):\n",
    "            g.add((Footballer, DCSSO['position'], DCSSO[player['position']]))\n",
    "        elif(player['position'] == \"Midfield\"):\n",
    "            g.add((Footballer, DCSSO['position'], DCSSO[\"Midfielder\"]))\n",
    "            subPosition += \"er\"\n",
    "        else:\n",
    "            g.add((Footballer, DCSSO['position'], DCSSO[\"Forward\"]))\n",
    "    \n",
    "        g.add((Footballer, DCSSO['subPosition'], DCSSO[subPosition]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "CPU times: total: 1.61 s\n",
      "Wall time: 2.92 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'players.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the soccer stats ontology namespace\n",
    "DCSSO = Namespace(\"http://www.dei.unipd.it/db2/dcsso#\")\n",
    "\n",
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"dcsso\", DCSSO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we finally store the statistics available for each player in a season."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% completed..\n",
      "CPU times: total: 1.64 s\n",
      "Wall time: 4.45 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "y=2019\n",
    "shortSeas = \"1920\"\n",
    "stats = pd.read_csv('stats'+str(shortSeas)+'_IDs.csv', sep=',')\n",
    "\n",
    "c=0\n",
    "for index, row in stats.iterrows():\n",
    "    c+=1\n",
    "    if(int(100*(c-1)/len(stats)) < int(100*c/len(stats))):\n",
    "        clear_output(wait=True)\n",
    "        print(str(int(100*c/len(stats)))+\"% completed.\"+\".\"*(c%3))\n",
    "    tmId = str(row['trID'])\n",
    "    if(tmId == \"0\"):\n",
    "        continue\n",
    "    # the node has the namespace + the transfermarkt ID as URI; we don't create the node for the footballer here\n",
    "    ref = \"player\"+tmId\n",
    "    Footballer = URIRef(DCSSO[ref])\n",
    "\n",
    "    teamIDs = str(row['team0ID'])\n",
    "    if(row['team1ID'] != 0):\n",
    "        teamIDs += \"_\"+str(row['team1ID'])\n",
    "        MembClassURI = DCSSO.SeasonalAggrMembership\n",
    "    else:\n",
    "        MembClassURI = DCSSO.SeasonalMembership\n",
    "    Memb = URIRef(DCSSO[\"memb\"+tmId+\"s\"+str(y)+\"t\"+teamIDs])\n",
    "    g.add((Memb, RDF.type, MembClassURI))\n",
    "    g.add((Footballer, DCSSO['hasMembership'], Memb))\n",
    "    g.add((Memb, DCSSO['season'], Literal(str(y), datatype=XSD.int)))\n",
    "    g.add((Memb, DCSSO['forTeam'], URIRef(DCSSO[\"team\"+str(row['team0ID'])])))\n",
    "    if(row['team1ID'] != 0):\n",
    "        g.add((Memb, DCSSO['forTeam'], URIRef(DCSSO[\"team\"+str(row['team1ID'])])))\n",
    "\n",
    "    #statistics\n",
    "    g.add((Memb, DCSSO['games'], Literal(row['games'], datatype=XSD.int)))\n",
    "    g.add((Memb, DCSSO['minutes'], Literal(row['minutes_played'], datatype=XSD.int)))\n",
    "    g.add((Memb, DCSSO['goals'], Literal(row['goals'], datatype=XSD.int)))\n",
    "    g.add((Memb, DCSSO['npg'], Literal(row['npg'], datatype=XSD.int)))\n",
    "    g.add((Memb, DCSSO['assists'], Literal(row['assists'], datatype=XSD.int)))\n",
    "    g.add((Memb, DCSSO['shots'], Literal(row['shots'], datatype=XSD.int)))\n",
    "    g.add((Memb, DCSSO['yellowCards'], Literal(row['yellow_cards'], datatype=XSD.int)))\n",
    "    g.add((Memb, DCSSO['redCards'], Literal(row['red_cards'], datatype=XSD.int)))\n",
    "    \n",
    "    g.add((Memb, DCSSO['xG'], Literal(row['xG'], datatype=XSD.double)))\n",
    "    g.add((Memb, DCSSO['xA'], Literal(row['xA'], datatype=XSD.double)))\n",
    "    g.add((Memb, DCSSO['npxG'], Literal(row['npxG'], datatype=XSD.double)))\n",
    "    g.add((Memb, DCSSO['xG90'], Literal(row['xG90'], datatype=XSD.double)))\n",
    "    g.add((Memb, DCSSO['xA90'], Literal(row['xA90'], datatype=XSD.double)))\n",
    "    g.add((Memb, DCSSO['npxG90'], Literal(row['npxG90'], datatype=XSD.double)))\n",
    "\n",
    "    #key pass. set to -1 implies that we miss the following data\n",
    "    if(row['key_passes'] != -1):\n",
    "        g.add((Memb, DCSSO['keyPasses'], Literal(row['key_passes'], datatype=XSD.int)))\n",
    "        g.add((Memb, DCSSO['xGBuildup'], Literal(row['xGBuildup'], datatype=XSD.double)))\n",
    "        g.add((Memb, DCSSO['xGChain'], Literal(row['xGChain'], datatype=XSD.double)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "CPU times: total: 1.81 s\n",
      "Wall time: 4.26 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'stats'+shortSeas+'.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sparqlwrapper\n",
    "# https://rdflib.github.io/sparqlwrapper/\n",
    "\n",
    "import sys\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "\n",
    "endpoint_url = \"https://query.wikidata.org/sparql\"\n",
    "\n",
    "query = \"\"\"SELECT ?oLabel\n",
    "WHERE\n",
    "{\n",
    "wd:Q192923 skos:altLabel ?o.\n",
    "FILTER(isLiteral(?o))\n",
    "SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\" }\n",
    "}\"\"\"\n",
    "\n",
    "\n",
    "def get_results(endpoint_url, query):\n",
    "    user_agent = \"WDQS-example Python/%s.%s\" % (sys.version_info[0], sys.version_info[1])\n",
    "    # TODO adjust user agent; see https://w.wiki/CX6\n",
    "    sparql = SPARQLWrapper(endpoint_url, agent=user_agent)\n",
    "    sparql.setQuery(query)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    return sparql.query().convert()\n",
    "\n",
    "\n",
    "results = get_results(endpoint_url, query)\n",
    "\n",
    "for result in results[\"results\"][\"bindings\"]:\n",
    "    print(result[\"oLabel\"][\"value\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
