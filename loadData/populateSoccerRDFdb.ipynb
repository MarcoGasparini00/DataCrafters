{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate an RDF database\n",
    "\n",
    "This notebook reports the main steps to download CSV files, process them and create an RDF dataset from them accordingly to an ontology. \n",
    "\n",
    "To measure execution time in Jupyter notebooks: <code>pip install ipython-autotime</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters and URLs\n",
    "path = str(Path(os.path.abspath(os.getcwd())).parent.absolute())\n",
    "stats1920Url = path + '/inDepthSoccerStats/2019-2020.csv'\n",
    "stats1819Url = path + '/inDepthSoccerStats/2018-2019.csv'\n",
    "playersUrl = path + '/inDepthSoccerStats/players.csv'\n",
    "appUrl = path + '/inDepthSoccerStats/appearances.csv'\n",
    "\n",
    "# country codes\n",
    "# countriesURL = path + '/data/countryCodes/wikipedia-iso-country-codes.csv'\n",
    "\n",
    "# saving folder\n",
    "savePath =  path + '/rdf/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soccer Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "stats1920 = pd.read_csv(stats1920Url, sep=',', index_col='player_name')\n",
    "stats1819 = pd.read_csv(stats1819Url, sep=',', index_col='player_name')\n",
    "players = pd.read_csv(playersUrl, sep=',', index_col='player_id')\n",
    "app = pd.read_csv(appUrl, sep=',', index_col='appearance_id')\n",
    "# cast year to int. If type(year) = str --> Literal= year-01-01\n",
    "#stats1920.astype({'year': 'int32'}).dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the country codes\n",
    "# we need to convert NaN values to something else otherwise NA strings are converted to NaN -> problem with Namibia\n",
    "#countries = pd.read_csv(countriesURL, sep=',', index_col='Name', keep_default_na=False, na_values=['_'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2661 entries, Jamie Vardy to Jean Onana\n",
      "Data columns (total 21 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   Unnamed: 0        2661 non-null   int64  \n",
      " 1   teams_played_for  2661 non-null   object \n",
      " 2   league            2661 non-null   object \n",
      " 3   games             2661 non-null   int64  \n",
      " 4   minutes_played    2661 non-null   int64  \n",
      " 5   goals             2661 non-null   int64  \n",
      " 6   npg               2661 non-null   int64  \n",
      " 7   assists           2661 non-null   int64  \n",
      " 8   xG                2661 non-null   float64\n",
      " 9   xA                2661 non-null   float64\n",
      " 10  npxG              2661 non-null   float64\n",
      " 11  xG90              2661 non-null   float64\n",
      " 12  xA90              2661 non-null   float64\n",
      " 13  npxG90            2661 non-null   float64\n",
      " 14  position          2661 non-null   object \n",
      " 15  shots             2661 non-null   int64  \n",
      " 16  key_passes        2661 non-null   int64  \n",
      " 17  yellow_cards      2661 non-null   int64  \n",
      " 18  red_cards         2661 non-null   int64  \n",
      " 19  xGBuildup         2661 non-null   float64\n",
      " 20  xGChain           2661 non-null   float64\n",
      "dtypes: float64(8), int64(10), object(3)\n",
      "memory usage: 457.4+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 30302 entries, 598 to 925584\n",
      "Data columns (total 22 columns):\n",
      " #   Column                                Non-Null Count  Dtype  \n",
      "---  ------                                --------------  -----  \n",
      " 0   first_name                            28337 non-null  object \n",
      " 1   last_name                             30302 non-null  object \n",
      " 2   name                                  30302 non-null  object \n",
      " 3   last_season                           30302 non-null  int64  \n",
      " 4   current_club_id                       30302 non-null  int64  \n",
      " 5   player_code                           30302 non-null  object \n",
      " 6   country_of_birth                      27613 non-null  object \n",
      " 7   city_of_birth                         28099 non-null  object \n",
      " 8   country_of_citizenship                29759 non-null  object \n",
      " 9   date_of_birth                         30255 non-null  object \n",
      " 10  sub_position                          30130 non-null  object \n",
      " 11  position                              30302 non-null  object \n",
      " 12  foot                                  27913 non-null  object \n",
      " 13  height_in_cm                          28204 non-null  float64\n",
      " 14  market_value_in_eur                   19383 non-null  float64\n",
      " 15  highest_market_value_in_eur           28981 non-null  float64\n",
      " 16  contract_expiration_date              18835 non-null  object \n",
      " 17  agent_name                            14941 non-null  object \n",
      " 18  image_url                             30302 non-null  object \n",
      " 19  url                                   30302 non-null  object \n",
      " 20  current_club_domestic_competition_id  30302 non-null  object \n",
      " 21  current_club_name                     30302 non-null  object \n",
      "dtypes: float64(3), int64(2), object(17)\n",
      "memory usage: 5.3+ MB\n"
     ]
    }
   ],
   "source": [
    "stats1920.info()\n",
    "players.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to install <code>RDFLib</code>\n",
    "\n",
    "<code>pip3 install rdflib </code> [Documentation](https://rdflib.readthedocs.io/en/stable/gettingstarted.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the required libraries\n",
    "#!pip install rdflib\n",
    "from rdflib import Graph, Literal, RDF, URIRef, Namespace, term\n",
    "# rdflib knows about some namespaces, like FOAF\n",
    "from rdflib.namespace import FOAF, XSD\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the country and the movie ontology namespaces not known by RDFlib\n",
    "#CNS = Namespace(\"http://eulersharp.sourceforge.net/2003/03swap/countries#\")\n",
    "DCSSO = Namespace(\"http://www.dei.unipd.it/db2/dcsso#\")\n",
    "\n",
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "#g.bind(\"countries\", CNS)\n",
    "g.bind(\"dcsso\", DCSSO)\n",
    "\n",
    "#term.bind(\n",
    "#    XSD.double,\n",
    "#    float,\n",
    "#   constructor=float,\n",
    "#    lexicalizer=lambda val: f\"{val:f}\",\n",
    "#    datatype_specific=True\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "#!pip install unidecode\n",
    "from unidecode import unidecode\n",
    "from itertools import permutations\n",
    "\n",
    "def nameToRef(name):\n",
    "    return unidecode(name.replace(\" \",\"\"))\n",
    "\n",
    "def hyphenize(s):\n",
    "    return unidecode(s.lower().replace(\" \",\"-\").replace(\"&#039;\",\"'\"))\n",
    "\n",
    "def cleanChars(item):\n",
    "    return item.str.replace(\"ć\", \"c\").str.replace(\"ğ\",\"g\").str.replace(\"İ\",\"i\").str.replace(\"-scaron-\",\"s\")\n",
    "\n",
    "\n",
    "\n",
    "def getAppsByID(ID):\n",
    "    return np.size(app1819[app1819['player_id'] == ID], 0);\n",
    "\n",
    "def resolveHomonyms(statsName):\n",
    "    matchingPlayersIDs = players[players['player_code'] == statsName].index\n",
    "    count = 0\n",
    "    candID = \"\"\n",
    "    for playerID in matchingPlayersIDs:\n",
    "            playerApps = getAppsByID(playerID)\n",
    "            if(playerApps == row['games']):\n",
    "                candID = playerID\n",
    "                count = count + 1\n",
    "    if(count != 1):\n",
    "        candID = \"\"\n",
    "    return candID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- STATISTICS ---\n",
      "Total matches: 2437 -- percentage: 93.91%\n",
      "---  details:  ---\n",
      "Exact matches: 2368 -- percentage: 91.25%\n",
      "No matches: 118 -- percentage: 4.55%\n",
      "Homonyms in players: 79 -- percentage: 3.04%\n",
      "    -----> resolved: 57 -- percentage: 2.20%\n",
      "Double ambiguity: 30 -- percentage: 1.16%\n",
      "    -----> resolved: 12 -- percentage: 0.46%\n",
      "CPU times: total: 8.03 s\n",
      "Wall time: 16 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#select only months and years from appearance dates\n",
    "appMonth = app['date'].str.split(\"/\").str[1]\n",
    "appYear = app['date'].str.split(\"/\").str[2]\n",
    "#select appearances from 18/19 season\n",
    "is1819 = ((appYear == \"2018\") & (appMonth >= \"08\")) | ((appYear == \"2019\") & (appMonth <= \"06\"))\n",
    "app1819 = app[is1819]\n",
    "playerCodes = cleanChars(players['player_code'])\n",
    "\n",
    "#iterate on stats file\n",
    "exact_matches = no_matches = mult_both = only_hom = resolved_hom = resolved_mult = 0\n",
    "playerID = \"\"\n",
    "for index, row in stats1819.iterrows():\n",
    "    statsName = hyphenize(index).replace(\"'\",\"\")\n",
    "    matchedPlayersIDs = players[playerCodes == statsName].index\n",
    "    matches = np.size(matchedPlayersIDs, 0)\n",
    "    \n",
    "    #rows with same name in stats mapped to a single player are ok (he has changed team during the season)\n",
    "    if(matches == 1):\n",
    "        exact_matches = exact_matches + 1\n",
    "        playerID = matchedPlayersIDs[0]\n",
    "    \n",
    "    #no matches at all on statsName\n",
    "    elif(matches == 0):\n",
    "        splitStatsName = statsName.split(\"-\")\n",
    "        n = np.size(splitStatsName, 0)\n",
    "        if(n >= 2):\n",
    "            #iterate on all possible permutations\n",
    "            for perm in permutations(splitStatsName):\n",
    "                if(matches == 1):\n",
    "                    break\n",
    "                #consider lengths from 2 to n\n",
    "                for i in range(2, n + 1):\n",
    "                    newName = \"\"\n",
    "                    #generate a \"new name\" of length i\n",
    "                    for j in range(0, i - 1):\n",
    "                        newName = newName + perm[j] + \"-\"\n",
    "                    newName = newName + perm[i - 1]\n",
    "                    \n",
    "                    #check if there is a player matching this new name\n",
    "                    matchedPlayersIDs = players[playerCodes == newName].index\n",
    "                    matches = np.size(matchedPlayersIDs, 0)\n",
    "                    if(matches == 1):\n",
    "                        exact_matches = exact_matches + 1\n",
    "                        #print(\"Success: \" + newName)\n",
    "                        playerID = matchedPlayersIDs[0]\n",
    "                        break\n",
    "        \n",
    "        if(matches == 1):\n",
    "            continue\n",
    "        #here iff all possible permutations have not generated any unique match\n",
    "        else:\n",
    "            no_matches = no_matches + 1\n",
    "            #print(\"No match: \"+statsName)\n",
    "    \n",
    "    #ambiguity: rows with same name in stats, homonyms in players\n",
    "    elif(np.size(stats1819[stats1819.index == index], 0) > 1):\n",
    "        mult_both = mult_both + 1\n",
    "        playerID = resolveHomonyms(statsName)\n",
    "        if(playerID != \"\"):\n",
    "            resolved_mult = resolved_mult + 1\n",
    "            #print(\"Resolved mult.: \" + statsName + \" -- games: \" + str(row['games']))\n",
    "        #else:\n",
    "            #print(\"Unresolved hom. (mult): \"+statsName+\" -- stats1819 games: \"+str(row['games']))\n",
    "    \n",
    "    #ambiguity: only homonyms\n",
    "    else:\n",
    "        only_hom = only_hom + 1\n",
    "        playerID = resolveHomonyms(statsName)\n",
    "        if(playerID != \"\"):\n",
    "            resolved_hom = resolved_hom + 1\n",
    "            #print(\"Resolved hom.: \" + statsName + \" -- games: \" + str(row['games']))\n",
    "        #else:\n",
    "            #print(\"Unresolved hom.: \"+statsName+\" -- stats1819 games: \"+str(row['games']))\n",
    "\n",
    "#print statistics\n",
    "statsRows = np.size(stats1819, 0);\n",
    "print(\"--- STATISTICS ---\")\n",
    "tot_matches = exact_matches + resolved_hom + resolved_mult\n",
    "print(\"Total matches: \"+str(tot_matches)+\" -- percentage: \" + \"{:.2f}%\".format(tot_matches*100/statsRows))\n",
    "print(\"---  details:  ---\")\n",
    "print(\"Exact matches: \"+str(exact_matches)+\" -- percentage: \" + \"{:.2f}%\".format(exact_matches*100/statsRows)) \n",
    "print(\"No matches: \"+str(no_matches)+\" -- percentage: \" + \"{:.2f}%\".format(no_matches*100/statsRows))\n",
    "print(\"Homonyms in players: \"+str(only_hom)+\" -- percentage: \" + \"{:.2f}%\".format(only_hom*100/statsRows))\n",
    "print(\"    -----> resolved: \"+str(resolved_hom)+\" -- percentage: \" + \"{:.2f}%\".format(resolved_hom*100/statsRows))\n",
    "print(\"Double ambiguity: \"+str(mult_both)+\" -- percentage: \" + \"{:.2f}%\".format(mult_both*100/statsRows))\n",
    "print(\"    -----> resolved: \"+str(resolved_mult)+\" -- percentage: \" + \"{:.2f}%\".format(resolved_mult*100/statsRows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "# measure execution time\n",
    "\n",
    "# just a variable to diversify between footballers with the same name\n",
    "a = 0\n",
    "# iterate over the movies dataframe\n",
    "for index, row in stats1920.iterrows():\n",
    "    # Create the node to add to the Graph\n",
    "    # the node has the namespace + the footballer name as URI\n",
    "    name = unidecode(index.replace(\"&#039;\",\"'\"))\n",
    "    ref = name.replace(\" \",\"\")\n",
    "    # add a unique code if there are players with the same name\n",
    "    if(np.size(stats1920[stats1920.index == name], 0) > 1):\n",
    "        #print(name + \" : \" + str(np.size(stats1920[stats1920.index == name], 0)))\n",
    "        a = a + 1\n",
    "        ref = ref + str(a)\n",
    "    \n",
    "    Footballer = URIRef(DCSSO[ref])\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Footballer, RDF.type, DCSSO.Footballer))\n",
    "    g.add((Footballer, DCSSO['name'], Literal(name, datatype=XSD.string)))\n",
    "    #g.add((Movie, MO['releaseYear'], Literal(row['year'], datatype=XSD.gYear)))\n",
    "    \n",
    "    leagueName = unidecode(row['league'])\n",
    "    League = URIRef(DCSSO[leagueName.replace(\" \",\"\")])\n",
    "    g.add((League, DCSSO['name'], Literal(row['league'], datatype=XSD.string)))\n",
    "    \n",
    "    teams = unidecode(row['teams_played_for'].replace(\"&#039;\",\"'\").strip())\n",
    "    \n",
    "    membRef = ref+\"_1920_\"+teams.replace(\",\",\"_\").replace(\" \",\"\")\n",
    "    Memb = URIRef(DCSSO[membRef])\n",
    "    g.add((Footballer, DCSSO['hasMembership'], Memb))\n",
    "    \n",
    "    for teamName in teams.split(','):\n",
    "        teamRef = teamName.replace(\" \",\"\")\n",
    "        Team = URIRef(DCSSO[teamRef])\n",
    "        g.add((Team, DCSSO['name'], Literal(teamName, datatype=XSD.string)))\n",
    "        g.add((Team, DCSSO['participatesIn'], League))\n",
    "        g.add((Memb, DCSSO['forTeam'], Team))\n",
    "    \n",
    "    #statistics\n",
    "    g.add((Memb, DCSSO['games'], Literal(row['games'], datatype=XSD.int)))\n",
    "    g.add((Memb, DCSSO['minutes'], Literal(row['minutes_played'], datatype=XSD.int)))\n",
    "    g.add((Memb, DCSSO['goals'], Literal(row['goals'], datatype=XSD.int)))\n",
    "    g.add((Memb, DCSSO['npg'], Literal(row['npg'], datatype=XSD.int)))\n",
    "    g.add((Memb, DCSSO['assists'], Literal(row['assists'], datatype=XSD.int)))\n",
    "    g.add((Memb, DCSSO['keyPasses'], Literal(row['key_passes'], datatype=XSD.int)))\n",
    "    g.add((Memb, DCSSO['shots'], Literal(row['shots'], datatype=XSD.int)))\n",
    "    g.add((Memb, DCSSO['yellowCards'], Literal(row['yellow_cards'], datatype=XSD.int)))\n",
    "    g.add((Memb, DCSSO['redCards'], Literal(row['red_cards'], datatype=XSD.int)))\n",
    "    \n",
    "    g.add((Memb, DCSSO['xG'], Literal(row['xG'], datatype=XSD.double)))\n",
    "    g.add((Memb, DCSSO['xA'], Literal(row['xA'], datatype=XSD.double)))\n",
    "    g.add((Memb, DCSSO['npxG'], Literal(row['npxG'], datatype=XSD.double)))\n",
    "    g.add((Memb, DCSSO['xG90'], Literal(row['xG90'], datatype=XSD.double)))\n",
    "    g.add((Memb, DCSSO['xA90'], Literal(row['xA90'], datatype=XSD.double)))\n",
    "    g.add((Memb, DCSSO['npxG90'], Literal(row['npxG90'], datatype=XSD.double)))\n",
    "    g.add((Memb, DCSSO['xGBuildup'], Literal(row['xGBuildup'], datatype=XSD.double)))\n",
    "    g.add((Memb, DCSSO['xGChain'], Literal(row['xGChain'], datatype=XSD.double)))\n",
    "    \n",
    "        \n",
    "    #there can be more than one role per footballer\n",
    "    for rN in row['position'].split(' '):\n",
    "        g.add((Footballer, DCSSO['role'], Literal(rN.strip(), datatype=XSD.string)))    \n",
    "\n",
    "#print(\"Duplicates: \"+str(a/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'stats1920.rdf', 'w') as file:\n",
    "    file.write(g.serialize(format='xml'))\n",
    "    #.decode(\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Referential integrity\n",
    "Note that in RDF we are in an open world situation. We cannot guarantee the referential integrity between the entities. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Person\n",
    "\n",
    "Let us generate the RDF data relative to the movie workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "people = pd.read_csv(namesUrl, sep=',', index_col='imdb_name_id', keep_default_na=False, na_values=['_'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "People are modeled with the FOAF ontology. \n",
    "Refer to [FOAF Documentation](http://xmlns.com/foaf/spec/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new graph\n",
    "g = Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "#measure execution time\n",
    "\n",
    "#iterate over the person dataframe\n",
    "for index, row in people.iterrows():\n",
    "    # Create the node to add to the Graph\n",
    "    # the node has the namespace + the person id as URI\n",
    "    Person = URIRef(MO[index])\n",
    "    g.add((Person, RDF.type, FOAF.Person))\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Person, FOAF['name'], Literal(row['name'], datatype=XSD.string)))\n",
    "    if row['date_of_birth'] != '':\n",
    "        try:\n",
    "            datetime.datetime.strptime(str(row['date_of_birth']), '%Y-%m-%d')\n",
    "            g.add((Person, MO['birthday'], Literal(row['date_of_birth'], datatype=XSD.date)))\n",
    "        except ValueError:\n",
    "            # probably it's the year alone\n",
    "            # check length\n",
    "            if (len(row['date_of_birth'])==4):\n",
    "                #it is a year\n",
    "                g.add((Person, MO['birthday'], Literal(row['date_of_birth']+\"-01-01\", datatype=XSD.date)))\n",
    "    \n",
    "    if row['place_of_birth'] != '':\n",
    "        g.add((Person, MO['birthplace'], Literal(row['place_of_birth'], datatype=XSD.string)))\n",
    "    \n",
    "    # check if the death day is not empty--i.e., the person is still alive\n",
    "    if row['date_of_death'] != '':\n",
    "        try:\n",
    "            datetime.datetime.strptime(str(row['date_of_death']), '%Y-%m-%d')\n",
    "            g.add((Person, MO['deathDay'], Literal(row['date_of_death'], datatype=XSD.date)))\n",
    "        except ValueError:\n",
    "            # probably it's the year alone\n",
    "            # check length\n",
    "            if (len(row['date_of_death'])==4):\n",
    "                #it is a year\n",
    "                g.add((Person, MO['deathDay'], Literal(row['date_of_death']+\"-01-01\", datatype=XSD.date)))\n",
    "        \n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"mo\", MO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'names.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle').decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Person-Movie Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "join = pd.read_csv(joinTableUrl, sep=',', index_col='imdb_title_id', keep_default_na=False, na_values=['_'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new graph\n",
    "g = Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regular expressions\n",
    "import re\n",
    "actor = re.compile('act*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "#measure execution time\n",
    "\n",
    "#iterate over the join table dataframe\n",
    "for index, row in join.iterrows():\n",
    "    # Create the node about the movie\n",
    "    # note that we do not add this resource to the database (created before)\n",
    "    Movie = URIRef(MO[index])\n",
    "    \n",
    "    # Create the node about the person\n",
    "    # note that we do not add this resource to the database (created before)\n",
    "    Person = URIRef(MO[row['imdb_name_id']])\n",
    "    # get the role of the person\n",
    "    role = row['category']\n",
    "    \n",
    "    # we have an actor or actress\n",
    "    if actor.match(role): \n",
    "        g.add((Person, MO['acted'], Movie))\n",
    "    elif (role=='director'):\n",
    "        g.add((Person, MO['directed'], Movie))\n",
    "    else:\n",
    "        # note that, with the defined ontology, we cannot caracterize the specific role of this person in the movie. \n",
    "        # why?\n",
    "        g.add((Person, MO['worked'], Movie))\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"mo\", MO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'name_movie_join.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle').decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Awards - Oscars data\n",
    "Note that if we do not check the referential integrity then we could produce ghost triple movie-nominee-oscar where the movie is not in the RDF graph.\n",
    "\n",
    "On the other hand, we can check if an actor or a movie exists by using the DataFrame in Python. Note that this is an external check and not a constraints met by the RDF DB.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "oscars = pd.read_csv(oscarsUrl, sep=',', keep_default_na=False, na_values=['_'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from num2words import num2words\n",
    "import string\n",
    "import re\n",
    "#create a new graph\n",
    "g = Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#iterate over the join table dataframe\n",
    "for index,row in oscars.iterrows():\n",
    "    #create the oscar with a custom id \n",
    "    cat = re.sub(r'[^\\w\\s]','',row['category'])\n",
    "    Oscar = URIRef(MO['oscar_'+cat.replace(\" \", \"\").lower()+'_'+ str(num2words(row['ceremony'], to='ordinal'))])\n",
    "    \n",
    "    # check if there already is at least a triple about this oscar\n",
    "    if not (Oscar, None, None) in g:    \n",
    "        # check if the oscar is already in the graph\n",
    "        g.add((Oscar, RDF.type, MO.Oscar))\n",
    "        g.add((Oscar, MO['category'], Literal(row['category'].lower(), datatype=XSD.string)))\n",
    "        g.add((Oscar, MO['year'], Literal(row['year_ceremony'], datatype=XSD.gYear)))\n",
    "    \n",
    "    # check if there is a name matching the people, meaning that the oscar can be associated to a person\n",
    "    if (people[\"name\"] == row[\"name\"]).any() == True :\n",
    "        #there is a person with this name\n",
    "        # Create the node about the person\n",
    "        # note that we do not add this resource to the database (created before)\n",
    "        Person = URIRef(MO[people[people[\"name\"]==row[\"name\"]].index[0]])\n",
    "        if row['winner']:\n",
    "            g.add((Person, MO['winner'], Oscar))\n",
    "        else:\n",
    "            g.add((Person, MO['nominated'], Oscar))\n",
    "    \n",
    "    # an oscar for a person is also to be considered an oscar for the movie\n",
    "    # check if the movie is in our DB\n",
    "    if (movies[\"original_title\"] == row[\"film\"]).any():\n",
    "        # there is a movie with this title\n",
    "        Movie = URIRef(MO[movies[movies[\"original_title\"]==row[\"film\"]].index[0]])\n",
    "        if row['winner']:\n",
    "            g.add((Movie, MO['winner'], Oscar))\n",
    "        else:\n",
    "            g.add((Movie, MO['nominated'], Oscar))\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"mo\", MO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'oscars.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle').decode(\"utf-8\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
