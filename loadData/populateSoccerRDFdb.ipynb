{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate an RDF database\n",
    "\n",
    "This notebook reports the main steps to download CSV files, process them and create an RDF dataset from them accordingly to an ontology. \n",
    "\n",
    "To measure execution time in Jupyter notebooks: <code>pip install ipython-autotime</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# parameters and URLs\n",
    "path = str(Path(os.path.abspath(os.getcwd())).parent.absolute())\n",
    "stats1920Url = path + '/inDepthSoccerStats/2019-2020.csv'\n",
    "stats1819Url = path + '/inDepthSoccerStats/2018-2019.csv'\n",
    "playersUrl = path + '/inDepthSoccerStats/players.csv'\n",
    "appUrl = path + '/inDepthSoccerStats/appearances.csv'\n",
    "\n",
    "# country codes\n",
    "# countriesURL = path + '/data/countryCodes/wikipedia-iso-country-codes.csv'\n",
    "\n",
    "# saving folder\n",
    "savePath =  path + '/rdf/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soccer Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "stats1920 = pd.read_csv(stats1920Url, sep=',', index_col='player_name')\n",
    "stats1819 = pd.read_csv(stats1819Url, sep=',', index_col='player_name')\n",
    "players = pd.read_csv(playersUrl, sep=',', index_col='player_id')\n",
    "app = pd.read_csv(appUrl, sep=',', index_col='appearance_id')\n",
    "# cast year to int. If type(year) = str --> Literal= year-01-01\n",
    "#stats1920.astype({'year': 'int32'}).dtypes\n",
    "#load the country codes\n",
    "# we need to convert NaN values to something else otherwise NA strings are converted to NaN -> problem with Namibia\n",
    "#countries = pd.read_csv(countriesURL, sep=',', index_col='Name', keep_default_na=False, na_values=['_'])\n",
    "\n",
    "#stats1920.info()\n",
    "#players.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to install <code>RDFLib</code>\n",
    "\n",
    "<code>pip3 install rdflib </code> [Documentation](https://rdflib.readthedocs.io/en/stable/gettingstarted.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the required libraries\n",
    "#!pip install rdflib\n",
    "from rdflib import Graph, Literal, RDF, URIRef, Namespace, term\n",
    "# rdflib knows about some namespaces, like FOAF\n",
    "from rdflib.namespace import FOAF, XSD\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the country and the movie ontology namespaces not known by RDFlib\n",
    "#CNS = Namespace(\"http://eulersharp.sourceforge.net/2003/03swap/countries#\")\n",
    "DCSSO = Namespace(\"http://www.dei.unipd.it/db2/dcsso#\")\n",
    "\n",
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "#g.bind(\"countries\", CNS)\n",
    "g.bind(\"dcsso\", DCSSO)\n",
    "\n",
    "#term.bind(\n",
    "#    XSD.double,\n",
    "#    float,\n",
    "#   constructor=float,\n",
    "#    lexicalizer=lambda val: f\"{val:f}\",\n",
    "#    datatype_specific=True\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "#!pip install unidecode\n",
    "#!pip install googlesearch-python\n",
    "from unidecode import unidecode\n",
    "from itertools import permutations\n",
    "from difflib import SequenceMatcher\n",
    "from googlesearch import search\n",
    "\n",
    "#string parsing\n",
    "\n",
    "def nameToRef(name):\n",
    "    return unidecode(name.replace(\" \",\"\"))\n",
    "\n",
    "def hyphenize(s):\n",
    "    return unidecode(s.lower().replace(\" \",\"-\").replace(\"&#039;\",\"'\"))\n",
    "\n",
    "def cleanChars(item):\n",
    "    return item.str.replace(\"ć\", \"c\").str.replace(\"ğ\",\"g\").str.replace(\"İ\",\"i\").str.replace(\"-scaron-\",\"s\")\n",
    "\n",
    "#google lookup for wikidata item\n",
    "def checkEqualsWD(c1, c2):\n",
    "    URL1 = next(search(c1.replace(\"-\", \" \") + \" football wikidata\", num_results=1))\n",
    "    URL2 = next(search(c2.replace(\"-\", \" \") + \" football wikidata\", num_results=1))\n",
    "    if(URL1 == URL2):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "#permutations strategy\n",
    "\n",
    "def genSeqByLength(perm, length):\n",
    "    newName = \"\"\n",
    "    for j in range(0, length - 1):\n",
    "        newName = newName + perm[j] + \"-\"\n",
    "    newName = newName + perm[length - 1]\n",
    "    return newName\n",
    "\n",
    "def match_seq(splitS1, list):\n",
    "    resultList = []\n",
    "    #iterate on all possible permutations\n",
    "    length = -1\n",
    "    if(len(list) == 1):\n",
    "        length = len(list[0].split(\"-\"))\n",
    "        uniqueItem = list[0]\n",
    "    for perm in permutations(splitS1):\n",
    "        if(length != -1):\n",
    "            newName = genSeqByLength(perm, length)\n",
    "            if(uniqueItem == newName):\n",
    "                return [uniqueItem]\n",
    "        #consider all lengths from 2 to n\n",
    "        else:\n",
    "            for i in range(2, len(splitS1) + 1):\n",
    "                newName = genSeqByLength(perm, i)\n",
    "                for item in list:\n",
    "                    if(item == newName):\n",
    "                        resultList.append(newName)\n",
    "    return resultList\n",
    "\n",
    "#multiple matches resolution\n",
    "\n",
    "def getAppsByID(ID, apps):\n",
    "    return np.size(apps[apps['player_id'] == ID], 0);\n",
    "\n",
    "#I: number of games in the season, candidate players, list describing single appearances\n",
    "#O: player from players file, or empty Series\n",
    "def solve_with_apps(statsGames, somePlayers, appsCol):\n",
    "    for ind in somePlayers.index:\n",
    "        if(statsGames == getAppsByID(ind, appsCol)):\n",
    "            player = somePlayers[somePlayers.index == ind].iloc[0]\n",
    "            return player\n",
    "    return pd.Series([])\n",
    "\n",
    "def solve_with_apps_approx(statsGames, somePlayers, appsCol):\n",
    "    minDiff = 50\n",
    "    minInd = 0\n",
    "    for ind in somePlayers.index:\n",
    "        if(abs(statsGames - getAppsByID(ind, appsCol)) < minDiff):\n",
    "            minDiff = abs(statsGames - getAppsByID(ind, appsCol))\n",
    "            minInd = ind\n",
    "    if(minDiff <= 5):\n",
    "        player = somePlayers[somePlayers.index == minInd].iloc[0] \n",
    "        return player\n",
    "    return pd.Series([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERM1: son-heung-min --> heung-min-son\n",
      "PERM2: bobby-reid --> bobby-de-cordova-reid\n",
      "GOOGLE: jose-holebas --> jose-cholevas\n",
      "NONE: sokratis, matches: 0\n",
      "NONE: jonny, matches: 0\n",
      "PERM1: ki-sung-yueng --> sung-yueng-ki\n",
      "GOOGLE: joseph-gomez --> joe-gomez\n",
      "GOOGLE: mat-ryan --> mathew-ryan\n",
      "NONE: kepa, matches: 0\n",
      "NONE: bruno, matches: 2\n",
      "NONE: franck-zambo, matches: 0\n",
      "NONE: roger, matches: 3\n",
      "GOOGLE: morales --> lucas-morales\n",
      "NONE: ezequiel-avila, matches: 0\n",
      "GOOGLE: daniel-parejo --> dani-parejo\n",
      "NONE: angel, matches: 0\n",
      "GOOGLE: muniain --> iker-muniain\n",
      "GOOGLE: santiago-cazorla --> santi-cazorla\n",
      "GOOGLE: etxeita --> xabi-etxeita\n",
      "PERM1: juan-camilo-hernandez --> juan-hernandez\n",
      "NONE: alfonso, matches: 0\n",
      "PERM1: wu-lei --> lei-wu\n",
      "NONE: keko, matches: 0\n",
      "NONE: yuri, matches: 0\n",
      "NONE: aduriz, matches: 0\n",
      "MAXSIM: cristian-rivera --> christian-rivera\n",
      "NONE: nyom, matches: 0\n",
      "NONE: bernardo, matches: 3\n",
      "NONE: papakouly-diop, matches: 0\n",
      "NONE: tono, matches: 2\n",
      "NONE: zaldua, matches: 0\n",
      "NONE: mario, matches: 0\n",
      "NONE: rober, matches: 0\n",
      "NONE: benat, matches: 0\n",
      "PERM2: de-marcos --> oscar-de-marcos\n",
      "NONE: amath-diedhiou, matches: 0\n",
      "NONE: borja, matches: 0\n",
      "NONE: vitorino-antunes, matches: 0\n",
      "NONE: anuar-mohamed, matches: 0\n",
      "GOOGLE: santiago-comesana --> santi-comesana\n",
      "PERM1: bryan-gil-salvatierra --> bryan-gil\n",
      "NONE: gabriel, matches: 2\n",
      "MAXSIM: robert-ibanez --> rober-ibanez\n",
      "NONE: yoel, matches: 0\n",
      "GOOGLE: jozabed --> jozabed-sanchez\n",
      "NONE: vigaray, matches: 0\n",
      "NONE: oier, matches: 0\n",
      "NONE: moya, matches: 0\n",
      "NONE: gimenez, matches: 0\n",
      "NONE: planas, matches: 0\n",
      "PERM2: san-jose --> mikel-san-jose\n",
      "GOOGLE: balenziaga --> mikel-balenziaga\n",
      "GOOGLE: iturraspe --> ander-iturraspe\n",
      "NONE: elustondo, matches: 0\n",
      "NONE: eraso, matches: 0\n",
      "NONE: bruno, matches: 2\n",
      "NONE: jaume, matches: 0\n",
      "NONE: sabin, matches: 0\n",
      "NONE: ivi, matches: 0\n",
      "NONE: miguelon, matches: 0\n",
      "NONE: borja, matches: 0\n",
      "NONE: kevin, matches: 0\n",
      "NONE: daniel-torres, matches: 0\n",
      "NONE: raul-garcia, matches: 3\n",
      "NONE: antonio-moya, matches: 0\n",
      "NONE: raba, matches: 0\n",
      "NONE: djene-dakonam, matches: 0\n",
      "NONE: hacen, matches: 0\n",
      "PERM2: jose-arnaiz --> jose-manuel-arnaiz\n",
      "NONE: anthony-lozano, matches: 0\n",
      "GOOGLE: manuel-morlanes --> manu-morlanes\n",
      "PERM2: dennis-eckert --> dennis-eckert-ayensa\n",
      "NONE: arthur, matches: 2\n",
      "NONE: daniel-ojeda, matches: 0\n",
      "PERM1: paik-seung-ho --> seung-ho-paik\n",
      "NONE: francisco-montero, matches: 0\n",
      "GOOGLE: rodrigo-tarin --> rodri-tarin\n",
      "GOOGLE: moises-delgado --> moi-delgado\n",
      "NONE: camacho, matches: 0\n",
      "PERM1: lee-kang-in --> kang-in-lee\n",
      "NONE: cristo-gonzalez, matches: 0\n",
      "GOOGLE: xavier-quintilla --> xavi-quintilla\n",
      "NONE: javier-diaz, matches: 0\n",
      "PERM1: gian-luca-waldschmidt --> luca-waldschmidt\n",
      "PERM2: john-brooks --> john-anthony-brooks\n",
      "NONE: felipe, matches: 2\n",
      "NONE: thiago-alcantara, matches: 0\n",
      "GOOGLE: joshua-sargent --> josh-sargent\n",
      "GOOGLE: yunus-malli --> yunus-mallı\n",
      "GOOGLE: yevhen-konoplyanka --> yevgen-konoplyanka\n",
      "PERM2: per-skjelbred --> per-ciljan-skjelbred\n",
      "NONE: naldo, matches: 2\n",
      "PERM2: isaac-thelin --> isaac-kiese-thelin\n",
      "GOOGLE: dayotchanculle-upamecano --> dayot-upamecano\n",
      "PERM1: gian-luca-itter --> luca-itter\n",
      "NONE: pal-dardai, matches: 0\n",
      "PERM1: chima-sean-okoroji --> chima-okoroji\n",
      "NONE: louis-beyer, matches: 0\n",
      "NONE: bote-baku, matches: 0\n",
      "NONE: kasim-nuhu, matches: 0\n",
      "MAXSIM: philipp-mwene --> phillipp-mwene\n",
      "PERM1: leandro-barreiro-martins --> leandro-barreiro\n",
      "PERM1: hans-nunoo-sarpei --> nunoo-sarpei\n",
      "NONE: keita, matches: 0\n",
      "NONE: fabian, matches: 0\n",
      "MAXSIM: gianmarco-ferrari --> gian-marco-ferrari\n",
      "NONE: samir, matches: 0\n",
      "MAXSIM: antonio-la-gumina --> antonino-la-gumina\n",
      "GOOGLE: kostas-manolas --> konstantinos-manolas\n",
      "GOOGLE: berat-gjimshiti --> berat-djimsiti\n",
      "NONE: marlon-santos, matches: 0\n",
      "NONE: jose-reina, matches: 0\n",
      "NONE: pepin, matches: 0\n",
      "PERM1: juan-manuel-valencia --> juan-valencia\n",
      "MAXSIM: nicolas-schiappacase --> nicolas-schiappacasse\n",
      "MAXSIM: dimitris-nikolaou --> dimitrios-nikolaou\n",
      "NONE: ibanez, matches: 0\n",
      "PERM1: paolo-gozzi-iweru --> paolo-gozzi\n",
      "PERM1: nicky-medja-beloko --> nicky-beloko\n",
      "PERM1: kylian-mbappe-lottin --> kylian-mbappe\n",
      "NONE: falcao, matches: 0\n",
      "PERM1: joia-nuno-da-costa --> nuno-da-costa\n",
      "NONE: jordan-siebatcheu, matches: 0\n",
      "GOOGLE: sehrou-guirassy --> serhou-guirassy\n",
      "PERM1: suk-hyun-jun --> hyun-jun-suk\n",
      "GOOGLE: kostas-mitroglou --> konstantinos-mitroglou\n",
      "GOOGLE: naif-aguerd --> nayef-aguerd\n",
      "PERM1: kwon-chang-hoon --> chang-hoon-kwon\n",
      "NONE: john-mendoza, matches: 0\n",
      "PERM1: juan-ferney-otero --> juan-otero\n",
      "NONE: cristian, matches: 2\n",
      "NONE: gabriel, matches: 2\n",
      "PERM1: firmin-ndombe-mubele --> firmin-mubele\n",
      "NONE: prince, matches: 0\n",
      "PERM1: tanguy-ndombele-alvaro --> tanguy-ndombele\n",
      "PERM1: mehmet-zeki-celik --> zeki-celik\n",
      "PERM1: benoit-badiashile-mukinayi --> benoit-badiashile\n",
      "NONE: naldo, matches: 2\n",
      "NONE: danilo, matches: 7\n",
      "PERM1: jacques-alaixys-romao --> alaixys-romao\n",
      "GOOGLE: lass-diarra --> lassana-diarra\n",
      "MAXSIM: moustapha-diallo --> mustapha-diallo\n",
      "PERM2: denis-poha --> denis-will-poha\n",
      "NONE: kelvin-adou, matches: 0\n",
      "NONE: fernando-marcal, matches: 0\n",
      "PERM1: james-edward-lea-siliki --> james-lea-siliki\n",
      "GOOGLE: zaydou-youssef --> zaydou-youssouf\n",
      "PERM2: fode-toure --> fode-ballo-toure\n",
      "MAXSIM: dmitri-lienard --> dimitri-lienard\n",
      "PERM2: kouadio-dabila --> kouadio-yves-dabila\n",
      "MAXSIM: ismail-aaneb --> ismail-aaneba\n",
      "MAXSIM: yannis-ammour --> yanis-ammour\n",
      "PERM1: mohamed-lamine-diaby --> mohamed-diaby\n",
      "GOOGLE: hakim-el-mokkedem --> hakim-el-mokeddem\n",
      "NONE: serigne-modou-kara-mbodji, matches: 0\n",
      "PERM1: thody-elie-youan --> elie-youan\n",
      "PERM1: charles-nathan-abi --> charles-abi\n",
      "NONE: reinildo, matches: 0\n",
      "PERM2: elhadji-diaw --> elhadji-pape-diaw\n",
      "NONE: kouadio-kone, matches: 0\n",
      "PERM1: albert-nicolas-lottin --> albert-lottin\n",
      "MAXSIM: yassin-benrahou --> yassine-benrahou\n",
      "PERM1: nathan-ngoumou-minpol --> nathan-ngoumou\n",
      "CPU times: total: 3min 11s\n",
      "Wall time: 9min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#select only months and years from appearance dates\n",
    "appMonth = app['date'].str.split(\"/\").str[1]\n",
    "appYear = app['date'].str.split(\"/\").str[2]\n",
    "#select appearances from 18/19 season\n",
    "is1819 = ((appYear == \"2018\") & (appMonth >= \"08\")) | ((appYear == \"2019\") & (appMonth <= \"06\"))\n",
    "app1819 = app[is1819]\n",
    "playerCodes = cleanChars(players['player_code'])\n",
    "\n",
    "#iterate on stats file\n",
    "exact_matches = no_matches = resolved_google = resolved_permS = resolved_permP = resolved_max_sim = unable_to_solve = 0\n",
    "for index, row in stats1819.iterrows():\n",
    "    mode = \"NONE\"\n",
    "    player = pd.Series([])\n",
    "    statsName = hyphenize(index).replace(\"'\",\"\")\n",
    "    \n",
    "    matchedPlayers = players[playerCodes == statsName]\n",
    "    \n",
    "    #multiple rows with same name in stats mapped to a single player are ok (he has changed team during the season)\n",
    "    if(np.size(matchedPlayers, 0) > 0):\n",
    "        mode = \"MATCH\"\n",
    "       \n",
    "    if(mode == \"NONE\"):\n",
    "        #split name in stats and use permutations strategy\n",
    "        splitStatsName = statsName.split(\"-\")\n",
    "        if(len(splitStatsName) >= 2):\n",
    "            matchedCodes = match_seq(splitStatsName, playerCodes)\n",
    "            if(len(matchedCodes) > 0):\n",
    "                matchedPlayers = players[players['player_code'].isin(matchedCodes)]\n",
    "                mode = \"PERM1\"\n",
    "\n",
    "    if(mode == \"NONE\"):\n",
    "        maxSim = 0\n",
    "        maxC = \"\"\n",
    "        for c in playerCodes:\n",
    "            sm = SequenceMatcher(None, statsName, c)\n",
    "            #do not proceed if the upper bound is too small\n",
    "            if(sm.real_quick_ratio() >= 0.5):\n",
    "                #remember: similarity is not commutative\n",
    "                simm = sm.ratio()\n",
    "                if(simm > maxSim):\n",
    "                    maxSim = simm\n",
    "                    maxC = c\n",
    "                    \n",
    "                #if sim is big enough, try permutation strategy with name from players file\n",
    "                if(simm >= 0.6):\n",
    "                    splitC = c.split(\"-\")\n",
    "                    if(len(splitC) >= len(splitStatsName) and len(splitStatsName) >= 2):\n",
    "                        matchedCodes = match_seq(splitC, [statsName])\n",
    "                        if(len(matchedCodes) > 0):\n",
    "                            newMatchedPlayers = players[players['player_code'] == c]\n",
    "                            matchedPlayers = pd.concat([matchedPlayers, newMatchedPlayers])\n",
    "\n",
    "        if(maxSim >= 0.95):\n",
    "            matchedPlayers = players[playerCodes == maxC]\n",
    "            mode = \"MAXSIM\"\n",
    "        elif(np.size(matchedPlayers, 0) > 0):\n",
    "            mode = \"PERM2\"\n",
    "        elif(checkEqualsWD(statsName, maxC) == True):\n",
    "            matchedPlayers = players[playerCodes == maxC]\n",
    "            mode = \"GOOGLE\"\n",
    "\n",
    "    #managing results of any method\n",
    "    matches = np.size(matchedPlayers, 0)\n",
    "    if(matches == 1):\n",
    "        player = matchedPlayers.iloc[0]\n",
    "    #more than 1 possible match\n",
    "    if(matches > 1):\n",
    "        player = solve_with_apps_approx(row['games'], matchedPlayers, app1819)\n",
    "        if(len(player) == 0):\n",
    "            mode = \"NONE\"\n",
    "            unable_to_solve += 1\n",
    "    \n",
    "    if(mode == \"NONE\"):\n",
    "        no_matches += 1\n",
    "    elif(mode == \"PERM1\"):\n",
    "        resolved_permS += 1\n",
    "    elif(mode == \"PERM2\"):\n",
    "        resolved_permP += 1\n",
    "    elif(mode == \"GOOGLE\"):\n",
    "        resolved_google += 1\n",
    "    elif(mode == \"MATCH\"):\n",
    "        exact_matches += 1\n",
    "    elif(mode == \"MAXSIM\"):\n",
    "        resolved_max_sim += 1\n",
    "\n",
    "    if(mode == \"NONE\"):\n",
    "        print(\"NONE: \"+statsName+\", matches: \"+str(matches))\n",
    "    elif(mode != \"MATCH\"):\n",
    "        print(mode+\": \"+statsName+\" --> \"+player['player_code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   --- STATISTICS ---\n",
      "Total matches:                             2515 -- percentage: 96.92%\n",
      "  ---> exact matches:                      2432 -- percentage: 93.72%\n",
      "  ---> resolved permutating statsName:       31 -- percentage: 1.19%\n",
      "  ---> resolved permutating player code:     12 -- percentage: 0.46%\n",
      "  ---> resolved with google:                 28 -- percentage: 1.08%\n",
      "  ---> resolved with max sim.:               12 -- percentage: 0.46%\n",
      "No matches:                                  80 -- percentage: 3.08%\n",
      "  ---> unable to resolve between matches:    14 -- percentage: 0.54%\n",
      "  ---> zero matches found:                   66 -- percentage: 2.54%\n"
     ]
    }
   ],
   "source": [
    "#print statistics\n",
    "statsRows = np.size(stats1819, 0);\n",
    "print(\"   --- STATISTICS ---\")\n",
    "tot_matches = exact_matches + + resolved_google + resolved_permS + resolved_permP + resolved_max_sim\n",
    "print(\"Total matches:                            \"+\"{:5d}\".format(tot_matches)+\" -- percentage: \" + \"{:.2f}%\".format(tot_matches*100/statsRows))\n",
    "print(\"  ---> exact matches:                     \"+\"{:5d}\".format(exact_matches)+\" -- percentage: \" + \"{:.2f}%\".format(exact_matches*100/statsRows)) \n",
    "print(\"  ---> resolved permutating statsName:    \"+\"{:5d}\".format(resolved_permS)+\" -- percentage: \" + \"{:.2f}%\".format(resolved_permS*100/statsRows))\n",
    "print(\"  ---> resolved permutating player code:  \"+\"{:5d}\".format(resolved_permP)+\" -- percentage: \" + \"{:.2f}%\".format(resolved_permP*100/statsRows))\n",
    "print(\"  ---> resolved with google:              \"+\"{:5d}\".format(resolved_google)+\" -- percentage: \" + \"{:.2f}%\".format(resolved_google*100/statsRows))\n",
    "print(\"  ---> resolved with max sim.:            \"+\"{:5d}\".format(resolved_max_sim)+\" -- percentage: \" + \"{:.2f}%\".format(resolved_max_sim*100/statsRows))\n",
    "print(\"No matches:                               \"+\"{:5d}\".format(no_matches)+\" -- percentage: \" + \"{:.2f}%\".format(no_matches*100/statsRows))\n",
    "print(\"  ---> unable to resolve between matches: \"+\"{:5d}\".format(unable_to_solve)+\" -- percentage: \" + \"{:.2f}%\".format(unable_to_solve*100/statsRows))\n",
    "print(\"  ---> zero matches found:                \"+\"{:5d}\".format(no_matches - unable_to_solve)+\" -- percentage: \" + \"{:.2f}%\".format((no_matches - unable_to_solve)*100/statsRows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "# measure execution time\n",
    "\n",
    "# just a variable to diversify between footballers with the same name\n",
    "a = 0\n",
    "# iterate over the movies dataframe\n",
    "for index, row in stats1920.iterrows():\n",
    "    # Create the node to add to the Graph\n",
    "    # the node has the namespace + the footballer name as URI\n",
    "    name = unidecode(index.replace(\"&#039;\",\"'\"))\n",
    "    ref = name.replace(\" \",\"\")\n",
    "    # add a unique code if there are players with the same name\n",
    "    if(np.size(stats1920[stats1920.index == name], 0) > 1):\n",
    "        #print(name + \" : \" + str(np.size(stats1920[stats1920.index == name], 0)))\n",
    "        a = a + 1\n",
    "        ref = ref + str(a)\n",
    "    \n",
    "    Footballer = URIRef(DCSSO[ref])\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Footballer, RDF.type, DCSSO.Footballer))\n",
    "    g.add((Footballer, DCSSO['name'], Literal(name, datatype=XSD.string)))\n",
    "    #g.add((Movie, MO['releaseYear'], Literal(row['year'], datatype=XSD.gYear)))\n",
    "    \n",
    "    leagueName = unidecode(row['league'])\n",
    "    League = URIRef(DCSSO[leagueName.replace(\" \",\"\")])\n",
    "    g.add((League, DCSSO['name'], Literal(row['league'], datatype=XSD.string)))\n",
    "    \n",
    "    teams = unidecode(row['teams_played_for'].replace(\"&#039;\",\"'\").strip())\n",
    "    \n",
    "    membRef = ref+\"_1920_\"+teams.replace(\",\",\"_\").replace(\" \",\"\")\n",
    "    Memb = URIRef(DCSSO[membRef])\n",
    "    g.add((Footballer, DCSSO['hasMembership'], Memb))\n",
    "    \n",
    "    for teamName in teams.split(','):\n",
    "        teamRef = teamName.replace(\" \",\"\")\n",
    "        Team = URIRef(DCSSO[teamRef])\n",
    "        g.add((Team, DCSSO['name'], Literal(teamName, datatype=XSD.string)))\n",
    "        g.add((Team, DCSSO['participatesIn'], League))\n",
    "        g.add((Memb, DCSSO['forTeam'], Team))\n",
    "    \n",
    "    #statistics\n",
    "    g.add((Memb, DCSSO['games'], Literal(row['games'], datatype=XSD.int)))\n",
    "    g.add((Memb, DCSSO['minutes'], Literal(row['minutes_played'], datatype=XSD.int)))\n",
    "    g.add((Memb, DCSSO['goals'], Literal(row['goals'], datatype=XSD.int)))\n",
    "    g.add((Memb, DCSSO['npg'], Literal(row['npg'], datatype=XSD.int)))\n",
    "    g.add((Memb, DCSSO['assists'], Literal(row['assists'], datatype=XSD.int)))\n",
    "    g.add((Memb, DCSSO['keyPasses'], Literal(row['key_passes'], datatype=XSD.int)))\n",
    "    g.add((Memb, DCSSO['shots'], Literal(row['shots'], datatype=XSD.int)))\n",
    "    g.add((Memb, DCSSO['yellowCards'], Literal(row['yellow_cards'], datatype=XSD.int)))\n",
    "    g.add((Memb, DCSSO['redCards'], Literal(row['red_cards'], datatype=XSD.int)))\n",
    "    \n",
    "    g.add((Memb, DCSSO['xG'], Literal(row['xG'], datatype=XSD.double)))\n",
    "    g.add((Memb, DCSSO['xA'], Literal(row['xA'], datatype=XSD.double)))\n",
    "    g.add((Memb, DCSSO['npxG'], Literal(row['npxG'], datatype=XSD.double)))\n",
    "    g.add((Memb, DCSSO['xG90'], Literal(row['xG90'], datatype=XSD.double)))\n",
    "    g.add((Memb, DCSSO['xA90'], Literal(row['xA90'], datatype=XSD.double)))\n",
    "    g.add((Memb, DCSSO['npxG90'], Literal(row['npxG90'], datatype=XSD.double)))\n",
    "    g.add((Memb, DCSSO['xGBuildup'], Literal(row['xGBuildup'], datatype=XSD.double)))\n",
    "    g.add((Memb, DCSSO['xGChain'], Literal(row['xGChain'], datatype=XSD.double)))\n",
    "    \n",
    "        \n",
    "    #there can be more than one role per footballer\n",
    "    for rN in row['position'].split(' '):\n",
    "        g.add((Footballer, DCSSO['role'], Literal(rN.strip(), datatype=XSD.string)))    \n",
    "\n",
    "#print(\"Duplicates: \"+str(a/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'stats1920.rdf', 'w') as file:\n",
    "    file.write(g.serialize(format='xml'))\n",
    "    #.decode(\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Referential integrity\n",
    "Note that in RDF we are in an open world situation. We cannot guarantee the referential integrity between the entities. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Person\n",
    "\n",
    "Let us generate the RDF data relative to the movie workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "people = pd.read_csv(namesUrl, sep=',', index_col='imdb_name_id', keep_default_na=False, na_values=['_'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "People are modeled with the FOAF ontology. \n",
    "Refer to [FOAF Documentation](http://xmlns.com/foaf/spec/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new graph\n",
    "g = Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "#measure execution time\n",
    "\n",
    "#iterate over the person dataframe\n",
    "for index, row in people.iterrows():\n",
    "    # Create the node to add to the Graph\n",
    "    # the node has the namespace + the person id as URI\n",
    "    Person = URIRef(MO[index])\n",
    "    g.add((Person, RDF.type, FOAF.Person))\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Person, FOAF['name'], Literal(row['name'], datatype=XSD.string)))\n",
    "    if row['date_of_birth'] != '':\n",
    "        try:\n",
    "            datetime.datetime.strptime(str(row['date_of_birth']), '%Y-%m-%d')\n",
    "            g.add((Person, MO['birthday'], Literal(row['date_of_birth'], datatype=XSD.date)))\n",
    "        except ValueError:\n",
    "            # probably it's the year alone\n",
    "            # check length\n",
    "            if (len(row['date_of_birth'])==4):\n",
    "                #it is a year\n",
    "                g.add((Person, MO['birthday'], Literal(row['date_of_birth']+\"-01-01\", datatype=XSD.date)))\n",
    "    \n",
    "    if row['place_of_birth'] != '':\n",
    "        g.add((Person, MO['birthplace'], Literal(row['place_of_birth'], datatype=XSD.string)))\n",
    "    \n",
    "    # check if the death day is not empty--i.e., the person is still alive\n",
    "    if row['date_of_death'] != '':\n",
    "        try:\n",
    "            datetime.datetime.strptime(str(row['date_of_death']), '%Y-%m-%d')\n",
    "            g.add((Person, MO['deathDay'], Literal(row['date_of_death'], datatype=XSD.date)))\n",
    "        except ValueError:\n",
    "            # probably it's the year alone\n",
    "            # check length\n",
    "            if (len(row['date_of_death'])==4):\n",
    "                #it is a year\n",
    "                g.add((Person, MO['deathDay'], Literal(row['date_of_death']+\"-01-01\", datatype=XSD.date)))\n",
    "        \n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"mo\", MO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'names.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle').decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Person-Movie Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "join = pd.read_csv(joinTableUrl, sep=',', index_col='imdb_title_id', keep_default_na=False, na_values=['_'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new graph\n",
    "g = Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regular expressions\n",
    "import re\n",
    "actor = re.compile('act*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "#measure execution time\n",
    "\n",
    "#iterate over the join table dataframe\n",
    "for index, row in join.iterrows():\n",
    "    # Create the node about the movie\n",
    "    # note that we do not add this resource to the database (created before)\n",
    "    Movie = URIRef(MO[index])\n",
    "    \n",
    "    # Create the node about the person\n",
    "    # note that we do not add this resource to the database (created before)\n",
    "    Person = URIRef(MO[row['imdb_name_id']])\n",
    "    # get the role of the person\n",
    "    role = row['category']\n",
    "    \n",
    "    # we have an actor or actress\n",
    "    if actor.match(role): \n",
    "        g.add((Person, MO['acted'], Movie))\n",
    "    elif (role=='director'):\n",
    "        g.add((Person, MO['directed'], Movie))\n",
    "    else:\n",
    "        # note that, with the defined ontology, we cannot caracterize the specific role of this person in the movie. \n",
    "        # why?\n",
    "        g.add((Person, MO['worked'], Movie))\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"mo\", MO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'name_movie_join.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle').decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Awards - Oscars data\n",
    "Note that if we do not check the referential integrity then we could produce ghost triple movie-nominee-oscar where the movie is not in the RDF graph.\n",
    "\n",
    "On the other hand, we can check if an actor or a movie exists by using the DataFrame in Python. Note that this is an external check and not a constraints met by the RDF DB.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "oscars = pd.read_csv(oscarsUrl, sep=',', keep_default_na=False, na_values=['_'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from num2words import num2words\n",
    "import string\n",
    "import re\n",
    "#create a new graph\n",
    "g = Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#iterate over the join table dataframe\n",
    "for index,row in oscars.iterrows():\n",
    "    #create the oscar with a custom id \n",
    "    cat = re.sub(r'[^\\w\\s]','',row['category'])\n",
    "    Oscar = URIRef(MO['oscar_'+cat.replace(\" \", \"\").lower()+'_'+ str(num2words(row['ceremony'], to='ordinal'))])\n",
    "    \n",
    "    # check if there already is at least a triple about this oscar\n",
    "    if not (Oscar, None, None) in g:    \n",
    "        # check if the oscar is already in the graph\n",
    "        g.add((Oscar, RDF.type, MO.Oscar))\n",
    "        g.add((Oscar, MO['category'], Literal(row['category'].lower(), datatype=XSD.string)))\n",
    "        g.add((Oscar, MO['year'], Literal(row['year_ceremony'], datatype=XSD.gYear)))\n",
    "    \n",
    "    # check if there is a name matching the people, meaning that the oscar can be associated to a person\n",
    "    if (people[\"name\"] == row[\"name\"]).any() == True :\n",
    "        #there is a person with this name\n",
    "        # Create the node about the person\n",
    "        # note that we do not add this resource to the database (created before)\n",
    "        Person = URIRef(MO[people[people[\"name\"]==row[\"name\"]].index[0]])\n",
    "        if row['winner']:\n",
    "            g.add((Person, MO['winner'], Oscar))\n",
    "        else:\n",
    "            g.add((Person, MO['nominated'], Oscar))\n",
    "    \n",
    "    # an oscar for a person is also to be considered an oscar for the movie\n",
    "    # check if the movie is in our DB\n",
    "    if (movies[\"original_title\"] == row[\"film\"]).any():\n",
    "        # there is a movie with this title\n",
    "        Movie = URIRef(MO[movies[movies[\"original_title\"]==row[\"film\"]].index[0]])\n",
    "        if row['winner']:\n",
    "            g.add((Movie, MO['winner'], Oscar))\n",
    "        else:\n",
    "            g.add((Movie, MO['nominated'], Oscar))\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"mo\", MO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'oscars.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle').decode(\"utf-8\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
