{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate our RDF database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# parameters and URLs\n",
    "path = str(Path(os.path.abspath(os.getcwd())).parent.absolute())\n",
    "stats1920Url = path + '/inDepthSoccerStats/2019-2020.csv'\n",
    "stats1819Url = path + '/inDepthSoccerStats/2018-2019.csv'\n",
    "stats1819FBrefUrl = path + '/inDepthSoccerStats/transfermarkt_fbref_201819.csv'\n",
    "playersUrl = path + '/inDepthSoccerStats/players.csv'\n",
    "teamsUrl = path + '/inDepthSoccerStats/clubs.csv'\n",
    "appUrl = path + '/inDepthSoccerStats/appearances.csv'\n",
    "gamesUrl = path + '/inDepthSoccerStats/games.csv'\n",
    "\n",
    "# country codes\n",
    "countriesURL = path + '/inDepthSoccerStats/wikipedia-iso-country-codes.csv'\n",
    "\n",
    "# saving folder\n",
    "savePath =  path + '/rdf/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "stats1920 = pd.read_csv(stats1920Url, sep=',', index_col='indCol')\n",
    "stats1819 = pd.read_csv(stats1819Url, sep=',', index_col='indCol')\n",
    "#these dataframes store data from Transfermarkt\n",
    "players = pd.read_csv(playersUrl, sep=',', index_col='player_id')\n",
    "teams = pd.read_csv(teamsUrl, sep=',', index_col='club_id')\n",
    "app = pd.read_csv(appUrl, sep=',', index_col='appearance_id')\n",
    "games = pd.read_csv(gamesUrl, sep=',', index_col='game_id')\n",
    "#FBref file used for completing some missing data\n",
    "stats1819FBref = pd.read_csv(stats1819FBrefUrl, sep=';', index_col='Column1', dtype={\"Attendance\": \"string\"})\n",
    "\n",
    "#load the country codes\n",
    "# we need to convert NaN values to something else otherwise NA strings are converted to NaN -> problem with Namibia\n",
    "countries = pd.read_csv(countriesURL, sep=',', index_col='English short name lower case', keep_default_na=False, na_values=['_'])\n",
    "\n",
    "#stats1920.info()\n",
    "#players.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the required libraries\n",
    "#!pip install rdflib\n",
    "from rdflib import Graph, Literal, RDF, URIRef, Namespace, term\n",
    "# rdflib knows about some namespaces, like FOAF\n",
    "from rdflib.namespace import FOAF, XSD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Namespace and prefixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the country and the movie ontology namespaces not known by RDFlib\n",
    "#CNS = Namespace(\"http://eulersharp.sourceforge.net/2003/03swap/countries#\")\n",
    "DCSSO = Namespace(\"http://www.dei.unipd.it/db2/dcsso#\")\n",
    "\n",
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "#g.bind(\"countries\", CNS)\n",
    "g.bind(\"dcsso\", DCSSO)\n",
    "\n",
    "#term.bind(\n",
    "#    XSD.double,\n",
    "#    float,\n",
    "#   constructor=float,\n",
    "#    lexicalizer=lambda val: f\"{val:f}\",\n",
    "#    datatype_specific=True\n",
    "#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing and matching utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "#!pip install unidecode\n",
    "#!pip install googlesearch-python\n",
    "from unidecode import unidecode\n",
    "from itertools import permutations\n",
    "from difflib import SequenceMatcher\n",
    "from googlesearch import search\n",
    "\n",
    "#string parsing\n",
    "\n",
    "def nameToRef(name):\n",
    "    return unidecode(name.replace(\" \",\"\"))\n",
    "\n",
    "def hyphenize(s):\n",
    "    return unidecode(s.lower().replace(\" \",\"-\").replace(\"&#039;\",\"'\"))\n",
    "\n",
    "def cleanChars(item):\n",
    "    return item.str.replace(\"ć\", \"c\").str.replace(\"ğ\",\"g\").str.replace(\"İ\",\"i\").str.replace(\"-scaron-\",\"s\")\n",
    "\n",
    "def cleanString(s):\n",
    "    return unidecode(s.replace(\"ć\", \"c\").replace(\"ğ\",\"g\").replace(\"İ\",\"i\"))\n",
    "    \n",
    "\n",
    "#permutations strategy\n",
    "\n",
    "def genSeqByLength(perm, length):\n",
    "    newName = \"\"\n",
    "    for j in range(0, length - 1):\n",
    "        newName = newName + perm[j] + \"-\"\n",
    "    newName = newName + perm[length - 1]\n",
    "    return newName\n",
    "\n",
    "def match_seq(splitS1, lis):\n",
    "    resultList = []\n",
    "    #iterate on all possible permutations\n",
    "    length = -1\n",
    "    if(len(lis) == 1):\n",
    "        length = len(lis[0].split(\"-\"))\n",
    "        uniqueItem = lis[0]\n",
    "    for perm in permutations(splitS1):\n",
    "        if(length != -1):\n",
    "            newName = genSeqByLength(perm, length)\n",
    "            if(uniqueItem == newName):\n",
    "                return [uniqueItem]\n",
    "        #consider all lengths from 2 to n\n",
    "        else:\n",
    "            for i in range(2, len(splitS1) + 1):\n",
    "                newName = genSeqByLength(perm, i)\n",
    "                for item in lis:\n",
    "                    if(item == newName):\n",
    "                        resultList.append(newName)\n",
    "    return resultList\n",
    "\n",
    "#multiple matches resolution\n",
    "\n",
    "def getAppsByID(ID, apps):\n",
    "    return np.size(apps[apps['player_id'] == ID], 0);\n",
    "\n",
    "#I: number of games in the season, candidate players, list describing single appearances\n",
    "#O: player from players file, or empty Series\n",
    "def solve_with_apps_approx(statsGames, somePlayers, appsCol, tol=5):\n",
    "    minDiff = 50\n",
    "    minInd = 0\n",
    "    for ind in somePlayers.index:\n",
    "        if(abs(statsGames - getAppsByID(ind, appsCol)) < minDiff):\n",
    "            minDiff = abs(statsGames - getAppsByID(ind, appsCol))\n",
    "            minInd = ind\n",
    "    if(minDiff <= tol):\n",
    "        player = somePlayers[somePlayers.index == minInd].iloc[0] \n",
    "        return player, minInd\n",
    "    else:\n",
    "        return pd.Series([]), -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching teams from different sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "statsTeamsSet = set()\n",
    "for ind, row in stats1819.iterrows():\n",
    "    currTeams = row['teams_played_for'].split(\",\")\n",
    "    statsTeamsSet.update(currTeams)\n",
    "\n",
    "statsTeams = list(statsTeamsSet)\n",
    "teamIDDict = dict()\n",
    "i = 0\n",
    "for statsTeam in statsTeams:\n",
    "    i += 1\n",
    "    maxS = 0\n",
    "    maxId = 0\n",
    "    for tind, trow in teams.iterrows():\n",
    "        sm = SequenceMatcher(None, statsTeam, trow['name'])\n",
    "        sim = sm.ratio()\n",
    "        if(sim > maxS):\n",
    "            maxS = sim\n",
    "            maxId = tind\n",
    "\n",
    "    if(maxS < 0.8):\n",
    "        splitURL = next(search(statsTeam+\" transfermarkt startseite verein\", num_results=1)).split(\"/\")\n",
    "        #some teams contain numbers in their name, so we need to take only the suffix of the URL\n",
    "        trID = splitURL[len(splitURL) - 1]\n",
    "        if(trID != \"\"):\n",
    "            if(len(teams[teams.index == int(trID)]) == 1):\n",
    "                maxId = int(trID)\n",
    "                print(\"{:3d}\".format(i)+\" out of \"+\"{:3d}\".format(len(statsTeams))+\" GOOGLE: \"+statsTeam+\" --> \"+teams.at[maxId, 'name'])\n",
    "            else:\n",
    "                print(\"Invalid ID extracted from \"+URL)\n",
    "        else:\n",
    "            print(\"No ID in URL \"+URL)\n",
    "\n",
    "    teamIDDict[statsTeam] = maxId"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching players from different sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#select only months and years from appearance dates\n",
    "appMonth = app['date'].str.split(\"/\").str[1]\n",
    "appYear = app['date'].str.split(\"/\").str[2]\n",
    "#select appearances from 18/19 season\n",
    "is1819 = ((appYear == \"2018\") & (appMonth >= \"08\")) | ((appYear == \"2019\") & (appMonth <= \"06\"))\n",
    "app1819 = app[is1819]\n",
    "playerCodes = cleanChars(players['player_code'])\n",
    "\n",
    "#new column to store transfermarkt ID\n",
    "stats1819['trID'] = [0] * len(stats1819)\n",
    "\n",
    "#iterate on stats file\n",
    "statsRows = np.size(stats1819, 0);\n",
    "exact_matches = no_matches = resolved_google = resolved_permS = resolved_permP = resolved_max_sim = resolved_pres = i = 0\n",
    "for index, row in stats1819.iterrows():\n",
    "    i += 1\n",
    "    mode = \"NONE\"\n",
    "    player = pd.Series([])\n",
    "    statsName = hyphenize(row['player_name']).replace(\"'\",\"\")\n",
    "    \n",
    "    matchedPlayers = players[playerCodes == statsName]\n",
    "    \n",
    "    #multiple rows with same name in stats mapped to a single player are ok (he has changed team during the season)\n",
    "    if(np.size(matchedPlayers, 0) > 0):\n",
    "        mode = \"MATCH\"\n",
    "       \n",
    "    if(mode == \"NONE\"):\n",
    "        #split name in stats and use permutations strategy\n",
    "        splitStatsName = statsName.split(\"-\")\n",
    "        if(len(splitStatsName) >= 2):\n",
    "            matchedCodes = match_seq(splitStatsName, playerCodes)\n",
    "            if(len(matchedCodes) > 0):\n",
    "                matchedPlayers = players[playerCodes.isin(matchedCodes)]\n",
    "                mode = \"PERM1\"\n",
    "\n",
    "    if(mode == \"NONE\"):\n",
    "        maxSim = 0\n",
    "        maxC = \"\"\n",
    "        for c in playerCodes:\n",
    "            sm = SequenceMatcher(None, statsName, c)\n",
    "            #do not proceed if the upper bound is too small\n",
    "            if(sm.real_quick_ratio() >= 0.5):\n",
    "                #remember: similarity is not commutative\n",
    "                simm = sm.ratio()                  \n",
    "                #if sim is big enough, try permutation strategy with name from players file\n",
    "                if(simm >= 0.6):\n",
    "                    splitC = c.split(\"-\")\n",
    "                    if(len(splitC) >= len(splitStatsName) and len(splitStatsName) >= 2):\n",
    "                        matchedCodes = match_seq(splitC, [statsName])\n",
    "                        if(len(matchedCodes) > 0):\n",
    "                            newMatchedPlayers = players[players['player_code'] == c]\n",
    "                            matchedPlayers = pd.concat([matchedPlayers, newMatchedPlayers])\n",
    "                if(simm > maxSim):\n",
    "                    maxSim = simm\n",
    "                    maxC = c\n",
    "\n",
    "        if(maxSim >= 0.95):\n",
    "            matchedPlayers = players[playerCodes == maxC]\n",
    "            mode = \"MAXSIM\"\n",
    "        elif(np.size(matchedPlayers, 0) > 0):\n",
    "            mode = \"PERM2\"\n",
    "\n",
    "    #managing results of any method\n",
    "    matches = np.size(matchedPlayers, 0)\n",
    "    if(matches == 1):\n",
    "        player = matchedPlayers.iloc[0]\n",
    "        trID = matchedPlayers.index[0]\n",
    "    if(matches > 1):\n",
    "        player, trID = solve_with_apps_approx(row['games'], matchedPlayers, app1819, 3)\n",
    "        if(trID == -1):\n",
    "            mode = \"NONE\"\n",
    "        else:\n",
    "            mode = \"PRES\"\n",
    "    if(mode == \"NONE\"):\n",
    "        splitURL = next(search(row['player_name']+\" \"+row['teams_played_for']+\" transfermarkt profil spieler\", num_results=1)).split(\"/\")\n",
    "        trID = splitURL[len(splitURL) - 1]\n",
    "        if(trID != \"\"):\n",
    "            urlPlayers = players[players.index == int(trID)]\n",
    "            if(len(urlPlayers) != 0):\n",
    "                mode = \"GOOGLE\"\n",
    "                player = urlPlayers.iloc[0]\n",
    "            else:\n",
    "                print(\"Invalid ID \"+trID+\" extracted from \"+str(splitURL))\n",
    "                mode = \"NONE\"\n",
    "        else:\n",
    "            print(\"No ID in URL \"+str(splitURL))\n",
    "            mode = \"NONE\"\n",
    "    \n",
    "    \n",
    "    if(mode == \"NONE\"):\n",
    "        no_matches += 1\n",
    "    elif(mode == \"PERM1\"):\n",
    "        resolved_permS += 1\n",
    "    elif(mode == \"PERM2\"):\n",
    "        resolved_permP += 1\n",
    "    elif(mode == \"GOOGLE\"):\n",
    "        resolved_google += 1\n",
    "    elif(mode == \"MATCH\"):\n",
    "        exact_matches += 1\n",
    "    elif(mode == \"MAXSIM\"):\n",
    "        resolved_max_sim += 1\n",
    "    elif(mode == \"PRES\"):\n",
    "        resolved_pres += 1\n",
    "\n",
    "    if(mode == \"NONE\"):\n",
    "        print(\"{:4d}\".format(i)+\" out of \"+str(statsRows)+\" NONE  : \"+statsName+\", matches: \"+str(matches))\n",
    "    elif(mode != \"MATCH\"):\n",
    "        print(\"{:4d}\".format(i)+\" out of \"+str(statsRows)+\" \"+mode.ljust(6)+\": \"+statsName+\" --> \"+player['player_code'])\n",
    "\n",
    "    if(mode != \"NONE\"):\n",
    "        stats1819.at[index, 'trID'] = int(trID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print statistics\n",
    "print(\"   --- STATISTICS ---\")\n",
    "tot_matches = exact_matches + + resolved_google + resolved_permS + resolved_permP + resolved_max_sim + resolved_pres\n",
    "print(\"Total matches:                            \"+\"{:5d}\".format(tot_matches)+\" -- percentage: \" + \"{:.2f}%\".format(tot_matches*100/statsRows))\n",
    "print(\"  ---> exact matches:                     \"+\"{:5d}\".format(exact_matches)+\" -- percentage: \" + \"{:.2f}%\".format(exact_matches*100/statsRows)) \n",
    "print(\"  ---> resolved permutating statsName:    \"+\"{:5d}\".format(resolved_permS)+\" -- percentage: \" + \"{:.2f}%\".format(resolved_permS*100/statsRows))\n",
    "print(\"  ---> resolved permutating player code:  \"+\"{:5d}\".format(resolved_permP)+\" -- percentage: \" + \"{:.2f}%\".format(resolved_permP*100/statsRows))\n",
    "print(\"  ---> resolved with max sim.:            \"+\"{:5d}\".format(resolved_max_sim)+\" -- percentage: \" + \"{:.2f}%\".format(resolved_max_sim*100/statsRows))\n",
    "print(\"  ---> resolved with apps:                \"+\"{:5d}\".format(resolved_pres)+\" -- percentage: \" + \"{:.2f}%\".format(resolved_pres*100/statsRows))\n",
    "print(\"  ---> resolved with google:              \"+\"{:5d}\".format(resolved_google)+\" -- percentage: \" + \"{:.2f}%\".format(resolved_google*100/statsRows))\n",
    "print(\"No matches:                               \"+\"{:5d}\".format(no_matches)+\" -- percentage: \" + \"{:.2f}%\".format(no_matches*100/statsRows))\n",
    "print(\"  ---> zero matches found:                \"+\"{:5d}\".format(no_matches)+\" -- percentage: \" + \"{:.2f}%\".format((no_matches)*100/statsRows))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Completing and correcting statistics in corner cases\n",
    "We need to manage the fact some rows contain total information about a player switching team in the same league during the season. <br>\n",
    "We can use information in the FBref file to complete our data; we have observed that, in this situation, it contains correct information only for the row of the two which has lower index: some statistics in the second row can be therefore corrected by subtracting the ones in the first row from the total ones. <br>\n",
    "In these cases, we add two columns to our main dataframe, to specify:\n",
    "* the Transfermarkt IDs of the 2 teams;\n",
    "* the indexes of the rows in the FBref file corresponding to the memberships of the player in the 2 teams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#statsCopy = stats1819\n",
    "stats1819['teamIDs'] = [list()] * len(stats1819)\n",
    "stats1819['fbref_indexes'] = [list()] * len(stats1819)\n",
    "\n",
    "for ind, row in stats1819.iterrows():\n",
    "    teamIDs = list()\n",
    "    for team in row['teams_played_for'].split(\",\"):\n",
    "        teamIDs.append(int(teamIDDict[team]))\n",
    "    stats1819.at[ind, 'teamIDs'] = teamIDs\n",
    "    if(len(teamIDs) == 2):\n",
    "        player = players.loc[row['trID']]\n",
    "        fbmatch = stats1819FBref[stats1819FBref['player'] == player['name']]\n",
    "        if(len(fbmatch) == 2):\n",
    "            team0 = teams.loc[teamIDs[0]]['name']\n",
    "\n",
    "            #select the index pointing to wrong row\n",
    "            if(fbmatch.index[0] < fbmatch.index[1]):\n",
    "                toCorrectInd = fbmatch.index[1]\n",
    "            else:\n",
    "                toCorrectInd = fbmatch.index[0]\n",
    "            #correct the row with higher index\n",
    "            stats1819FBref.at[toCorrectInd, 'goals'] = row['goals'] - stats1819FBref.at[toCorrectInd, 'goals']\n",
    "            stats1819FBref.at[toCorrectInd, 'minutes'] = row['minutes_played'] - stats1819FBref.at[toCorrectInd, 'minutes']\n",
    "            stats1819FBref.at[toCorrectInd, 'pens_made'] = row['goals'] - row['npg'] - stats1819FBref.at[toCorrectInd, 'pens_made']\n",
    "            stats1819FBref.at[toCorrectInd, 'assists'] = row['assists'] - stats1819FBref.at[toCorrectInd, 'assists']\n",
    "            \n",
    "            \n",
    "            sm0 = SequenceMatcher(None, team0, fbmatch.iloc[0]['squad'])\n",
    "            sm1 = SequenceMatcher(None, team0, fbmatch.iloc[1]['squad'])\n",
    "\n",
    "            #if the team corresponding to first ID matches with fbref row with index fbmatch.index[1], swap team IDs\n",
    "            if(sm0.ratio() < sm1.ratio()):\n",
    "                stats1819.at[ind, 'teamIDs'] = [teamIDs[1], teamIDs[0]]\n",
    "\n",
    "            stats1819.at[ind, 'fbref_indexes'] = [fbmatch.index[0], fbmatch.index[1]]\n",
    "            \n",
    "for ind, row in stats1819.iterrows():\n",
    "    fbrefin = row['fbref_indexes']\n",
    "    if(len(fbrefin) == 2):\n",
    "        print(row['player_name']+\" had \"+str(stats1819FBref.loc[row['fbref_indexes'][0]]['games'])+\" apps for \"+teams.loc[row['teamIDs'][0]]['name'])\n",
    "        print(row['player_name']+\" had \"+str(stats1819FBref.loc[row['fbref_indexes'][1]]['games'])+\" apps for \"+teams.loc[row['teamIDs'][1]]['name'])\n",
    "            \n",
    "#result: teamIDs = ID of team0, ID of team1; fbred_indexes = index of fbref row describing membership to team0, // to team1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for ind, row in countries.iterrows():\n",
    "    country = URIRef(DCSSO[row['Alpha-2 code']])\n",
    "    g.add((country, RDF.type, DCSSO.Country))\n",
    "    g.add((country, FOAF.name, Literal(cleanString(ind), datatype=XSD.string)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Leagues\n",
    "Leagues are added manually because we need to store only five using very limited information from the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "SerieA = URIRef(DCSSO[\"IT1\"])\n",
    "g.add((SerieA, RDF.type, DCSSO.League))\n",
    "g.add((SerieA, FOAF['name'], Literal(\"Serie A\", datatype=XSD.string)))\n",
    "g.add((SerieA, DCSSO['hasCountry'], URIRef(DCSSO[\"IT\"])))\n",
    "\n",
    "Ligue1 = URIRef(DCSSO[\"FR1\"])\n",
    "g.add((Ligue1, RDF.type, DCSSO.League))\n",
    "g.add((Ligue1, FOAF['name'], Literal(\"Ligue 1\", datatype=XSD.string)))\n",
    "g.add((Ligue1, DCSSO['hasCountry'], URIRef(DCSSO[\"FR\"])))\n",
    "\n",
    "LaLiga = URIRef(DCSSO[\"ES1\"])\n",
    "g.add((LaLiga, RDF.type, DCSSO.League))\n",
    "g.add((LaLiga, FOAF['name'], Literal(\"LaLiga\", datatype=XSD.string)))\n",
    "g.add((SerieA, DCSSO['hasCountry'], URIRef(DCSSO[\"ES\"])))\n",
    "\n",
    "Premier = URIRef(DCSSO[\"GB1\"])\n",
    "g.add((Premier, RDF.type, DCSSO.League))\n",
    "g.add((Premier, FOAF['name'], Literal(\"Premier League\", datatype=XSD.string)))\n",
    "g.add((Premier, DCSSO['hasCountry'], URIRef(DCSSO[\"GB\"])))\n",
    "\n",
    "Bundesliga = URIRef(DCSSO[\"L1\"])\n",
    "g.add((Bundesliga, RDF.type, DCSSO.League))\n",
    "g.add((Bundesliga, FOAF['name'], Literal(\"Bundesliga\", datatype=XSD.string)))\n",
    "g.add((Bundesliga, DCSSO['hasCountry'], URIRef(DCSSO[\"DE\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Teams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for ind, row in teams.iterrows():\n",
    "    Team = URIRef(DCSSO[\"team\"+str(ind)])\n",
    "    g.add((Team, RDF.type, DCSSO.Team))\n",
    "    g.add((Team, FOAF.name, Literal(cleanString(row['name']), datatype=XSD.string)))\n",
    "    g.add((Team, DCSSO['participatesIn'], URIRef(DCSSO[row['domestic_competition_id']])))\n",
    "    for y in range(2015, 2020):\n",
    "        #check if there are any games for this team in season starting in year y\n",
    "        if((games[games['season'] == y]['home_club_id'] == ind).any() == True):\n",
    "            #if there are any, y represents a season in which this team has played in its domestic top league\n",
    "            Participation = URIRef(DCSSO[\"part\"+str(ind)+\"s\"+str(y)])\n",
    "            g.add((Participation, RDF.type, DCSSO.SeasonalParticipation))\n",
    "            g.add((Team, DCSSO['hasParticipation'], Participation))\n",
    "            g.add((Participation, DCSSO['season'], Literal(y, datatype=XSD.int)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "players['first_name'] = players['first_name'].fillna(\"\")\n",
    "players['last_name'] = players['last_name'].fillna(\"\")\n",
    "\n",
    "# iterate over the movies dataframe\n",
    "for index, row in stats1819.iterrows():\n",
    "    tmId = str(row['trID'])\n",
    "    player = players.loc[row['trID']]\n",
    "    # the node has the namespace + the transfermarkt ID as URI\n",
    "    ref = \"player\"+tmId\n",
    "    Footballer = URIRef(DCSSO[ref])\n",
    "    g.add((Footballer, RDF.type, DCSSO.Footballer))\n",
    "    if(player['first_name'] != \"\"):\n",
    "        g.add((Footballer, FOAF['firstName'], Literal(cleanString(player['first_name']), datatype=XSD.string)))\n",
    "    if(player['last_name'] != \"\"):\n",
    "        g.add((Footballer, FOAF['familyName'], Literal(cleanString(player['last_name']), datatype=XSD.string)))\n",
    "\n",
    "    if(len(row['fbref_indexes']) == 2):\n",
    "        for tid in row['teamIDs']:\n",
    "            teamId = \"team\"+str(tid)\n",
    "\n",
    "            Memb = URIRef(DCSSO[\"memb\"+tmId+\"s\"+str(y)+teamId])\n",
    "            g.add((Memb, RDF.type, DCSSO.SeasonalMembership))\n",
    "            g.add((Footballer, DCSSO['hasMembership'], Memb))\n",
    "            g.add((Memb, DCSSO['season'], Literal(\"2018\", datatype=XSD.int)))\n",
    "            g.add((Memb, DCSSO['forTeam'], URIRef(DCSSO[teamId])))\n",
    "\n",
    "            #statistics\n",
    "            g.add((Memb, DCSSO['games'], Literal(row['games'], datatype=XSD.int)))\n",
    "            g.add((Memb, DCSSO['minutes'], Literal(row['minutes_played'], datatype=XSD.int)))\n",
    "            g.add((Memb, DCSSO['goals'], Literal(row['goals'], datatype=XSD.int)))\n",
    "            g.add((Memb, DCSSO['npg'], Literal(row['npg'], datatype=XSD.int)))\n",
    "            g.add((Memb, DCSSO['assists'], Literal(row['assists'], datatype=XSD.int)))\n",
    "            g.add((Memb, DCSSO['keyPasses'], Literal(row['key_passes'], datatype=XSD.int)))\n",
    "            g.add((Memb, DCSSO['shots'], Literal(row['shots'], datatype=XSD.int)))\n",
    "            g.add((Memb, DCSSO['yellowCards'], Literal(row['yellow_cards'], datatype=XSD.int)))\n",
    "            g.add((Memb, DCSSO['redCards'], Literal(row['red_cards'], datatype=XSD.int)))\n",
    "    \n",
    "            g.add((Memb, DCSSO['xG'], Literal(row['xG'], datatype=XSD.double)))\n",
    "            g.add((Memb, DCSSO['xA'], Literal(row['xA'], datatype=XSD.double)))\n",
    "            g.add((Memb, DCSSO['npxG'], Literal(row['npxG'], datatype=XSD.double)))\n",
    "            g.add((Memb, DCSSO['xG90'], Literal(row['xG90'], datatype=XSD.double)))\n",
    "            g.add((Memb, DCSSO['xA90'], Literal(row['xA90'], datatype=XSD.double)))\n",
    "            g.add((Memb, DCSSO['npxG90'], Literal(row['npxG90'], datatype=XSD.double)))\n",
    "            g.add((Memb, DCSSO['xGBuildup'], Literal(row['xGBuildup'], datatype=XSD.double)))\n",
    "            g.add((Memb, DCSSO['xGChain'], Literal(row['xGChain'], datatype=XSD.double)))\n",
    "    \n",
    "    \n",
    "    \n",
    "    if(player['position'] != \"Missing\"):\n",
    "        subPosition = player['sub_position'].replace(\" \", \"\").replace(\"-\", \"\")\n",
    "        if(player['position'] == \"Goalkeeper\" or player['position'] == \"Defender\"):\n",
    "            g.add((Footballer, DCSSO['position'], DCSSO[player['position']]))\n",
    "        elif(player['position'] == \"Midfield\"):\n",
    "            g.add((Footballer, DCSSO['position'], DCSSO[\"Midfielder\"]))\n",
    "            subPosition += \"er\"\n",
    "        else:\n",
    "            g.add((Footballer, DCSSO['position'], DCSSO[\"Forward\"]))\n",
    "\n",
    "        g.add((Footballer, DCSSO['subPosition'], DCSSO[subPosition]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'stats1819.rdf', 'w') as file:\n",
    "    file.write(g.serialize(format='xml'))\n",
    "    #.decode(\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sparqlwrapper\n",
    "# https://rdflib.github.io/sparqlwrapper/\n",
    "\n",
    "import sys\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "\n",
    "endpoint_url = \"https://query.wikidata.org/sparql\"\n",
    "\n",
    "query = \"\"\"SELECT ?oLabel\n",
    "WHERE\n",
    "{\n",
    "wd:Q192923 skos:altLabel ?o.\n",
    "FILTER(isLiteral(?o))\n",
    "SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\" }\n",
    "}\"\"\"\n",
    "\n",
    "\n",
    "def get_results(endpoint_url, query):\n",
    "    user_agent = \"WDQS-example Python/%s.%s\" % (sys.version_info[0], sys.version_info[1])\n",
    "    # TODO adjust user agent; see https://w.wiki/CX6\n",
    "    sparql = SPARQLWrapper(endpoint_url, agent=user_agent)\n",
    "    sparql.setQuery(query)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    return sparql.query().convert()\n",
    "\n",
    "\n",
    "results = get_results(endpoint_url, query)\n",
    "\n",
    "for result in results[\"results\"][\"bindings\"]:\n",
    "    print(result[\"oLabel\"][\"value\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Referential integrity\n",
    "Note that in RDF we are in an open world situation. We cannot guarantee the referential integrity between the entities. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Person\n",
    "\n",
    "Let us generate the RDF data relative to the movie workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "people = pd.read_csv(namesUrl, sep=',', index_col='imdb_name_id', keep_default_na=False, na_values=['_'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "People are modeled with the FOAF ontology. \n",
    "Refer to [FOAF Documentation](http://xmlns.com/foaf/spec/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new graph\n",
    "g = Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "#measure execution time\n",
    "\n",
    "#iterate over the person dataframe\n",
    "for index, row in people.iterrows():\n",
    "    # Create the node to add to the Graph\n",
    "    # the node has the namespace + the person id as URI\n",
    "    Person = URIRef(MO[index])\n",
    "    g.add((Person, RDF.type, FOAF.Person))\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Person, FOAF['name'], Literal(row['name'], datatype=XSD.string)))\n",
    "    if row['date_of_birth'] != '':\n",
    "        try:\n",
    "            datetime.datetime.strptime(str(row['date_of_birth']), '%Y-%m-%d')\n",
    "            g.add((Person, MO['birthday'], Literal(row['date_of_birth'], datatype=XSD.date)))\n",
    "        except ValueError:\n",
    "            # probably it's the year alone\n",
    "            # check length\n",
    "            if (len(row['date_of_birth'])==4):\n",
    "                #it is a year\n",
    "                g.add((Person, MO['birthday'], Literal(row['date_of_birth']+\"-01-01\", datatype=XSD.date)))\n",
    "    \n",
    "    if row['place_of_birth'] != '':\n",
    "        g.add((Person, MO['birthplace'], Literal(row['place_of_birth'], datatype=XSD.string)))\n",
    "    \n",
    "    # check if the death day is not empty--i.e., the person is still alive\n",
    "    if row['date_of_death'] != '':\n",
    "        try:\n",
    "            datetime.datetime.strptime(str(row['date_of_death']), '%Y-%m-%d')\n",
    "            g.add((Person, MO['deathDay'], Literal(row['date_of_death'], datatype=XSD.date)))\n",
    "        except ValueError:\n",
    "            # probably it's the year alone\n",
    "            # check length\n",
    "            if (len(row['date_of_death'])==4):\n",
    "                #it is a year\n",
    "                g.add((Person, MO['deathDay'], Literal(row['date_of_death']+\"-01-01\", datatype=XSD.date)))\n",
    "        \n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"mo\", MO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'names.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle').decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Person-Movie Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "join = pd.read_csv(joinTableUrl, sep=',', index_col='imdb_title_id', keep_default_na=False, na_values=['_'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new graph\n",
    "g = Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regular expressions\n",
    "import re\n",
    "actor = re.compile('act*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "#measure execution time\n",
    "\n",
    "#iterate over the join table dataframe\n",
    "for index, row in join.iterrows():\n",
    "    # Create the node about the movie\n",
    "    # note that we do not add this resource to the database (created before)\n",
    "    Movie = URIRef(MO[index])\n",
    "    \n",
    "    # Create the node about the person\n",
    "    # note that we do not add this resource to the database (created before)\n",
    "    Person = URIRef(MO[row['imdb_name_id']])\n",
    "    # get the role of the person\n",
    "    role = row['category']\n",
    "    \n",
    "    # we have an actor or actress\n",
    "    if actor.match(role): \n",
    "        g.add((Person, MO['acted'], Movie))\n",
    "    elif (role=='director'):\n",
    "        g.add((Person, MO['directed'], Movie))\n",
    "    else:\n",
    "        # note that, with the defined ontology, we cannot caracterize the specific role of this person in the movie. \n",
    "        # why?\n",
    "        g.add((Person, MO['worked'], Movie))\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"mo\", MO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'name_movie_join.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle').decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Awards - Oscars data\n",
    "Note that if we do not check the referential integrity then we could produce ghost triple movie-nominee-oscar where the movie is not in the RDF graph.\n",
    "\n",
    "On the other hand, we can check if an actor or a movie exists by using the DataFrame in Python. Note that this is an external check and not a constraints met by the RDF DB.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "oscars = pd.read_csv(oscarsUrl, sep=',', keep_default_na=False, na_values=['_'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from num2words import num2words\n",
    "import string\n",
    "import re\n",
    "#create a new graph\n",
    "g = Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#iterate over the join table dataframe\n",
    "for index,row in oscars.iterrows():\n",
    "    #create the oscar with a custom id \n",
    "    cat = re.sub(r'[^\\w\\s]','',row['category'])\n",
    "    Oscar = URIRef(MO['oscar_'+cat.replace(\" \", \"\").lower()+'_'+ str(num2words(row['ceremony'], to='ordinal'))])\n",
    "    \n",
    "    # check if there already is at least a triple about this oscar\n",
    "    if not (Oscar, None, None) in g:    \n",
    "        # check if the oscar is already in the graph\n",
    "        g.add((Oscar, RDF.type, MO.Oscar))\n",
    "        g.add((Oscar, MO['category'], Literal(row['category'].lower(), datatype=XSD.string)))\n",
    "        g.add((Oscar, MO['year'], Literal(row['year_ceremony'], datatype=XSD.gYear)))\n",
    "    \n",
    "    # check if there is a name matching the people, meaning that the oscar can be associated to a person\n",
    "    if (people[\"name\"] == row[\"name\"]).any() == True :\n",
    "        #there is a person with this name\n",
    "        # Create the node about the person\n",
    "        # note that we do not add this resource to the database (created before)\n",
    "        Person = URIRef(MO[people[people[\"name\"]==row[\"name\"]].index[0]])\n",
    "        if row['winner']:\n",
    "            g.add((Person, MO['winner'], Oscar))\n",
    "        else:\n",
    "            g.add((Person, MO['nominated'], Oscar))\n",
    "    \n",
    "    # an oscar for a person is also to be considered an oscar for the movie\n",
    "    # check if the movie is in our DB\n",
    "    if (movies[\"original_title\"] == row[\"film\"]).any():\n",
    "        # there is a movie with this title\n",
    "        Movie = URIRef(MO[movies[movies[\"original_title\"]==row[\"film\"]].index[0]])\n",
    "        if row['winner']:\n",
    "            g.add((Movie, MO['winner'], Oscar))\n",
    "        else:\n",
    "            g.add((Movie, MO['nominated'], Oscar))\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"mo\", MO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'oscars.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle').decode(\"utf-8\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
