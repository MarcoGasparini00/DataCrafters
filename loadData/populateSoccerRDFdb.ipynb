{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate our RDF database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# parameters and URLs\n",
    "path = str(Path(os.path.abspath(os.getcwd())).parent.absolute())\n",
    "stats1920Url = path + '/inDepthSoccerStats/2019-2020.csv'\n",
    "stats1819Url = path + '/inDepthSoccerStats/2018-2019.csv'\n",
    "stats1819FBrefUrl = path + '/inDepthSoccerStats/transfermarkt_fbref_201819.csv'\n",
    "playersUrl = path + '/inDepthSoccerStats/players.csv'\n",
    "teamsUrl = path + '/inDepthSoccerStats/clubs.csv'\n",
    "appUrl = path + '/inDepthSoccerStats/appearances.csv'\n",
    "gamesUrl = path + '/inDepthSoccerStats/games.csv'\n",
    "\n",
    "# country codes\n",
    "countriesURL = path + '/inDepthSoccerStats/wikipedia-iso-country-codes.csv'\n",
    "\n",
    "# saving folder\n",
    "savePath =  path + '/rdf/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "stats1920 = pd.read_csv(stats1920Url, sep=',', index_col='indCol')\n",
    "stats1819 = pd.read_csv(stats1819Url, sep=',', index_col='indCol')\n",
    "#these dataframes store data from Transfermarkt\n",
    "players = pd.read_csv(playersUrl, sep=',', index_col='player_id')\n",
    "teams = pd.read_csv(teamsUrl, sep=',', index_col='club_id')\n",
    "app = pd.read_csv(appUrl, sep=',', index_col='appearance_id')\n",
    "games = pd.read_csv(gamesUrl, sep=',', index_col='game_id')\n",
    "#FBref file used for completing some missing data\n",
    "stats1819FBref = pd.read_csv(stats1819FBrefUrl, sep=';', index_col='Column1', dtype={\"Attendance\": \"string\"})\n",
    "\n",
    "#load the country codes\n",
    "# we need to convert NaN values to something else otherwise NA strings are converted to NaN -> problem with Namibia\n",
    "countries = pd.read_csv(countriesURL, sep=',', index_col='English short name lower case', keep_default_na=False, na_values=['_'])\n",
    "\n",
    "#stats1920.info()\n",
    "#players.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the required libraries\n",
    "#!pip install rdflib\n",
    "from rdflib import Graph, Literal, RDF, URIRef, Namespace, term\n",
    "# rdflib knows about some namespaces, like FOAF\n",
    "from rdflib.namespace import FOAF, XSD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Namespace and prefixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the country and the movie ontology namespaces not known by RDFlib\n",
    "#CNS = Namespace(\"http://eulersharp.sourceforge.net/2003/03swap/countries#\")\n",
    "DCSSO = Namespace(\"http://www.dei.unipd.it/db2/dcsso#\")\n",
    "\n",
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "#g.bind(\"countries\", CNS)\n",
    "g.bind(\"dcsso\", DCSSO)\n",
    "\n",
    "#term.bind(\n",
    "#    XSD.double,\n",
    "#    float,\n",
    "#   constructor=float,\n",
    "#    lexicalizer=lambda val: f\"{val:f}\",\n",
    "#    datatype_specific=True\n",
    "#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing and matching utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "#!pip install unidecode\n",
    "#!pip install googlesearch-python\n",
    "from unidecode import unidecode\n",
    "from itertools import permutations\n",
    "from difflib import SequenceMatcher\n",
    "from googlesearch import search\n",
    "\n",
    "#string parsing\n",
    "\n",
    "def nameToRef(name):\n",
    "    return unidecode(name.replace(\" \",\"\"))\n",
    "\n",
    "def hyphenize(s):\n",
    "    return unidecode(s.lower().replace(\" \",\"-\").replace(\"&#039;\",\"'\"))\n",
    "\n",
    "def cleanChars(item):\n",
    "    return item.str.replace(\"ć\", \"c\").str.replace(\"ğ\",\"g\").str.replace(\"İ\",\"i\").str.replace(\"-scaron-\",\"s\")\n",
    "\n",
    "def cleanString(s):\n",
    "    return unidecode(s.replace(\"ć\", \"c\").replace(\"ğ\",\"g\").replace(\"İ\",\"i\"))\n",
    "    \n",
    "\n",
    "#permutations strategy\n",
    "\n",
    "def genSeqByLength(perm, length):\n",
    "    newName = \"\"\n",
    "    for j in range(0, length - 1):\n",
    "        newName = newName + perm[j] + \"-\"\n",
    "    newName = newName + perm[length - 1]\n",
    "    return newName\n",
    "\n",
    "def match_seq(splitS1, lis):\n",
    "    resultList = []\n",
    "    #iterate on all possible permutations\n",
    "    length = -1\n",
    "    if(len(lis) == 1):\n",
    "        length = len(lis[0].split(\"-\"))\n",
    "        uniqueItem = lis[0]\n",
    "    for perm in permutations(splitS1):\n",
    "        if(length != -1):\n",
    "            newName = genSeqByLength(perm, length)\n",
    "            if(uniqueItem == newName):\n",
    "                return [uniqueItem]\n",
    "        #consider all lengths from 2 to n\n",
    "        else:\n",
    "            for i in range(2, len(splitS1) + 1):\n",
    "                newName = genSeqByLength(perm, i)\n",
    "                for item in lis:\n",
    "                    if(item == newName):\n",
    "                        resultList.append(newName)\n",
    "    return resultList\n",
    "\n",
    "#multiple matches resolution\n",
    "\n",
    "def getAppsByID(ID, apps):\n",
    "    return np.size(apps[apps['player_id'] == ID], 0);\n",
    "\n",
    "#I: number of games in the season, candidate players, list describing single appearances\n",
    "#O: player from players file, or empty Series\n",
    "def solve_with_apps_approx(statsGames, somePlayers, appsCol, tol=5):\n",
    "    minDiff = 50\n",
    "    minInd = 0\n",
    "    for ind in somePlayers.index:\n",
    "        if(abs(statsGames - getAppsByID(ind, appsCol)) < minDiff):\n",
    "            minDiff = abs(statsGames - getAppsByID(ind, appsCol))\n",
    "            minInd = ind\n",
    "    if(minDiff <= tol):\n",
    "        player = somePlayers[somePlayers.index == minInd].iloc[0] \n",
    "        return player, minInd\n",
    "    else:\n",
    "        return pd.Series([]), -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching teams from different sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4 out of  98 GOOGLE: Inter --> Inter Milan\n",
      "  5 out of  98 GOOGLE: Bologna --> Bologna FC 1909\n",
      "  8 out of  98 GOOGLE: Leicester --> Leicester City\n",
      " 10 out of  98 GOOGLE: Reims --> Stade Reims\n",
      " 11 out of  98 GOOGLE: Marseille --> Olympique Marseille\n",
      " 12 out of  98 GOOGLE: Nuernberg --> 1.FC Nuremberg\n",
      " 17 out of  98 GOOGLE: Espanyol --> RCD Espanyol Barcelona\n",
      " 19 out of  98 GOOGLE: Udinese --> Udinese Calcio\n",
      " 20 out of  98 GOOGLE: Eibar --> SD Eibar\n",
      " 23 out of  98 GOOGLE: Brighton --> Brighton & Hove Albion\n",
      " 29 out of  98 GOOGLE: Nice --> OGC Nice\n",
      " 37 out of  98 GOOGLE: Real Betis --> Real Betis Balompié\n",
      " 38 out of  98 GOOGLE: Strasbourg --> RC Strasbourg Alsace\n",
      " 39 out of  98 GOOGLE: Cardiff --> Cardiff City\n",
      " 41 out of  98 GOOGLE: SPAL 2013 --> SPAL\n",
      " 44 out of  98 GOOGLE: Lille --> LOSC Lille\n",
      " 46 out of  98 GOOGLE: Rennes --> Stade Rennais FC\n",
      " 47 out of  98 GOOGLE: Athletic Club --> Athletic Bilbao\n",
      " 49 out of  98 GOOGLE: Hoffenheim --> TSG 1899 Hoffenheim\n",
      " 55 out of  98 GOOGLE: Angers --> Angers SCO\n",
      " 56 out of  98 GOOGLE: Dijon --> Dijon FCO\n",
      " 57 out of  98 GOOGLE: Hertha Berlin --> Hertha BSC\n",
      " 58 out of  98 GOOGLE: RasenBallsport Leipzig --> RB Leipzig\n",
      " 60 out of  98 GOOGLE: Bordeaux --> FC Girondins Bordeaux\n",
      " 61 out of  98 GOOGLE: Caen --> SM Caen\n",
      " 62 out of  98 GOOGLE: Genoa --> Genoa CFC\n",
      " 63 out of  98 GOOGLE: Roma --> AS Roma\n",
      " 65 out of  98 GOOGLE: Alaves --> Deportivo Alavés\n",
      " 73 out of  98 GOOGLE: Tottenham --> Tottenham Hotspur\n",
      " 75 out of  98 GOOGLE: Borussia M.Gladbach --> Borussia Mönchengladbach\n",
      " 76 out of  98 GOOGLE: Nimes --> Nîmes Olympique\n",
      " 78 out of  98 GOOGLE: West Ham --> West Ham United\n",
      " 80 out of  98 GOOGLE: Frosinone --> Frosinone Calcio\n",
      " 85 out of  98 GOOGLE: Leganes --> CD Leganés\n",
      " 86 out of  98 GOOGLE: Napoli --> SSC Napoli\n",
      " 91 out of  98 GOOGLE: Lyon --> Olympique Lyon\n",
      " 92 out of  98 GOOGLE: Lazio --> SS Lazio\n",
      " 95 out of  98 GOOGLE: Cagliari --> Cagliari Calcio\n",
      " 96 out of  98 GOOGLE: Mainz 05 --> 1.FSV Mainz 05\n",
      " 98 out of  98 GOOGLE: Chievo --> Chievo Verona\n",
      "CPU times: total: 21.3 s\n",
      "Wall time: 1min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "statsTeamsSet = set()\n",
    "for ind, row in stats1819.iterrows():\n",
    "    currTeams = row['teams_played_for'].split(\",\")\n",
    "    statsTeamsSet.update(currTeams)\n",
    "\n",
    "statsTeams = list(statsTeamsSet)\n",
    "teamIDDict = dict()\n",
    "i = 0\n",
    "for statsTeam in statsTeams:\n",
    "    i += 1\n",
    "    maxS = 0\n",
    "    maxId = 0\n",
    "    for tind, trow in teams.iterrows():\n",
    "        sm = SequenceMatcher(None, statsTeam, trow['name'])\n",
    "        sim = sm.ratio()\n",
    "        if(sim > maxS):\n",
    "            maxS = sim\n",
    "            maxId = tind\n",
    "\n",
    "    if(maxS < 0.8):\n",
    "        splitURL = next(search(statsTeam+\" transfermarkt startseite verein\", num_results=1)).split(\"/\")\n",
    "        #some teams contain numbers in their name, so we need to take only the suffix of the URL\n",
    "        trID = splitURL[len(splitURL) - 1]\n",
    "        if(trID != \"\"):\n",
    "            if(len(teams[teams.index == int(trID)]) == 1):\n",
    "                maxId = int(trID)\n",
    "                print(\"{:3d}\".format(i)+\" out of \"+\"{:3d}\".format(len(statsTeams))+\" GOOGLE: \"+statsTeam+\" --> \"+teams.at[maxId, 'name'])\n",
    "            else:\n",
    "                print(\"Invalid ID extracted from \"+URL)\n",
    "        else:\n",
    "            print(\"No ID in URL \"+URL)\n",
    "\n",
    "    teamIDDict[statsTeam] = maxId"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching players from different sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   9 out of 2595 PRES  : callum-wilson --> callum-wilson\n",
      "  16 out of 2595 PERM1 : son-heung-min --> heung-min-son\n",
      "  37 out of 2595 PRES  : pedro --> pedro\n",
      "  52 out of 2595 PRES  : david-silva --> david-silva\n",
      "  72 out of 2595 PERM2 : bobby-reid --> bobby-de-cordova-reid\n",
      "  76 out of 2595 PRES  : aaron-ramsey --> aaron-ramsey\n",
      "  97 out of 2595 GOOGLE: jose-holebas --> jose-cholevas\n",
      " 102 out of 2595 PRES  : willian --> willian\n",
      " 143 out of 2595 PRES  : jorginho --> jorginho\n",
      " 167 out of 2595 GOOGLE: sokratis --> sokratis-papastathopoulos\n",
      " 187 out of 2595 PRES  : fernandinho --> fernandinho\n",
      " 202 out of 2595 PRES  : adam-smith --> adam-smith\n",
      " 208 out of 2595 PRES  : adama-traore --> adama-traore\n",
      " 224 out of 2595 PRES  : danilo --> danilo\n",
      " 226 out of 2595 GOOGLE: jonny --> jonny-otto\n",
      " 228 out of 2595 PRES  : andre-gomes --> andre-gomes\n",
      " 234 out of 2595 PRES  : fabinho --> fabinho\n",
      " 235 out of 2595 PRES  : joao-moutinho --> joao-moutinho\n",
      " 257 out of 2595 PRES  : danny-ward --> danny-ward\n",
      " 308 out of 2595 PRES  : alberto-moreno --> alberto-moreno\n",
      " 322 out of 2595 PRES  : ben-davies --> ben-davies\n",
      " 331 out of 2595 PERM1 : ki-sung-yueng --> sung-yueng-ki\n",
      " 362 out of 2595 GOOGLE: joseph-gomez --> joe-gomez\n",
      " 365 out of 2595 PRES  : tom-davies --> tom-davies\n",
      " 402 out of 2595 GOOGLE: mat-ryan --> mathew-ryan\n",
      " 415 out of 2595 GOOGLE: kepa --> kepa-arrizabalaga\n",
      " 420 out of 2595 PRES  : bernardo --> bernardo\n",
      " 432 out of 2595 PRES  : tommy-smith --> tommy-smith\n",
      " 436 out of 2595 PRES  : danny-williams --> danny-williams\n",
      " 441 out of 2595 GOOGLE: bruno --> bruno-saltor\n",
      " 442 out of 2595 PRES  : ederson --> ederson\n",
      " 451 out of 2595 GOOGLE: franck-zambo --> frank-anguissa\n",
      " 471 out of 2595 PRES  : ibrahima-cisse --> ibrahima-cisse\n",
      " 507 out of 2595 GOOGLE: luis-suarez --> luis-suarez\n",
      " 515 out of 2595 GOOGLE: charles --> charles\n",
      " 520 out of 2595 GOOGLE: roger --> roger-marti\n",
      " 522 out of 2595 GOOGLE: morales --> jose-luis-morales\n",
      " 526 out of 2595 GOOGLE: ezequiel-avila --> chimy-avila\n",
      " 528 out of 2595 GOOGLE: daniel-parejo --> dani-parejo\n",
      " 529 out of 2595 GOOGLE: raul-garcia --> raul-garcia\n",
      " 533 out of 2595 GOOGLE: andre-silva --> andre-silva\n",
      " 538 out of 2595 GOOGLE: angel --> angel-rodriguez\n",
      " 541 out of 2595 GOOGLE: muniain --> iker-muniain\n",
      " 561 out of 2595 GOOGLE: santiago-cazorla --> santi-cazorla\n",
      " 569 out of 2595 GOOGLE: etxeita --> xabi-etxeita\n",
      " 575 out of 2595 PERM1 : juan-camilo-hernandez --> juan-hernandez\n",
      " 576 out of 2595 GOOGLE: alvaro-garcia --> alvaro-garcia\n",
      " 597 out of 2595 PRES  : rodri --> rodri\n",
      " 599 out of 2595 GOOGLE: alfonso --> alfonso-pedraza\n",
      " 616 out of 2595 PERM1 : wu-lei --> lei-wu\n",
      " 627 out of 2595 GOOGLE: keko --> keko-gontan\n",
      " 630 out of 2595 GOOGLE: yuri --> yuri-berchiche\n",
      " 634 out of 2595 GOOGLE: marcelo --> marcelo\n",
      " 642 out of 2595 GOOGLE: aduriz --> aritz-aduriz\n",
      " 649 out of 2595 PRES  : ximo-navarro --> ximo-navarro\n",
      " 650 out of 2595 GOOGLE: sergio-garcia --> sergio-garcia\n",
      " 654 out of 2595 MAXSIM: cristian-rivera --> christian-rivera\n",
      " 672 out of 2595 GOOGLE: nyom --> allan-nyom\n",
      " 678 out of 2595 GOOGLE: bernardo --> bernardo-espinosa\n",
      " 682 out of 2595 GOOGLE: papakouly-diop --> pape-diop\n",
      " 688 out of 2595 GOOGLE: tono --> tono-garcia\n",
      " 690 out of 2595 GOOGLE: zaldua --> joseba-zaldua\n",
      " 703 out of 2595 GOOGLE: rober --> rober-pier\n",
      " 707 out of 2595 GOOGLE: benat --> benat-etxebarria\n",
      " 709 out of 2595 GOOGLE: alvaro-gonzalez --> alvaro-gonzalez\n",
      " 711 out of 2595 PERM2 : de-marcos --> oscar-de-marcos\n",
      " 714 out of 2595 GOOGLE: amath-diedhiou --> amath-ndiaye\n",
      " 727 out of 2595 GOOGLE: borja --> borja-garcia\n",
      " 728 out of 2595 PRES  : naldo --> naldo\n",
      " 731 out of 2595 GOOGLE: vitorino-antunes --> antunes\n",
      " 735 out of 2595 PRES  : manu-garcia --> manu-garcia\n",
      " 754 out of 2595 GOOGLE: anuar-mohamed --> anuar\n",
      " 760 out of 2595 GOOGLE: santiago-comesana --> santi-comesana\n",
      " 765 out of 2595 PRES  : joaquin-fernandez --> joaquin-fernandez\n",
      " 767 out of 2595 PERM1 : bryan-gil-salvatierra --> bryan-gil\n",
      " 784 out of 2595 GOOGLE: neto --> neto\n",
      " 788 out of 2595 GOOGLE: diego-lopez --> diego-lopez\n",
      " 800 out of 2595 MAXSIM: robert-ibanez --> rober-ibanez\n",
      " 803 out of 2595 PRES  : rafinha --> rafinha\n",
      " 808 out of 2595 GOOGLE: javi-lopez --> javi-lopez\n",
      " 818 out of 2595 GOOGLE: yoel --> yoel-rodriguez\n",
      " 820 out of 2595 PRES  : diego-llorente --> diego-llorente\n",
      " 821 out of 2595 GOOGLE: jozabed --> jozabed-sanchez\n",
      " 822 out of 2595 PRES  : javi-guerra --> javi-guerra\n",
      " 823 out of 2595 PRES  : pedro-lopez --> pedro-lopez\n",
      " 824 out of 2595 GOOGLE: vigaray --> carlos-vigaray\n",
      " 828 out of 2595 PRES  : sergio-alvarez --> sergio-alvarez\n",
      " 832 out of 2595 PRES  : diego-reyes --> diego-reyes\n",
      " 833 out of 2595 GOOGLE: oier --> oier-olazabal\n",
      " 838 out of 2595 PRES  : stefan-savic --> stefan-savic\n",
      " 839 out of 2595 GOOGLE: moya --> miguel-angel-moya\n",
      " 840 out of 2595 GOOGLE: gimenez --> jose-maria-gimenez\n",
      " 841 out of 2595 PRES  : sergio-alvarez --> sergio-alvarez\n",
      " 843 out of 2595 GOOGLE: planas --> carles-planas\n",
      " 855 out of 2595 PERM2 : san-jose --> mikel-san-jose\n",
      " 856 out of 2595 GOOGLE: balenziaga --> mikel-balenziaga\n",
      " 857 out of 2595 GOOGLE: iturraspe --> ander-iturraspe\n",
      " 859 out of 2595 GOOGLE: elustondo --> gorka-elustondo\n",
      " 862 out of 2595 GOOGLE: eraso --> javi-eraso\n",
      " 870 out of 2595 GOOGLE: bruno --> bruno-gonzalez\n",
      " 872 out of 2595 GOOGLE: jaume --> jaume-domenech\n",
      " 876 out of 2595 GOOGLE: sabin --> sabin-merino\n",
      " 877 out of 2595 GOOGLE: luisinho --> luisinho\n",
      " 878 out of 2595 PRES  : vitolo --> vitolo\n",
      " 880 out of 2595 GOOGLE: ivi --> ivi-lopez\n",
      " 883 out of 2595 GOOGLE: miguelon --> miguel-llambrich\n",
      " 899 out of 2595 GOOGLE: borja --> borja-fernandez\n",
      " 902 out of 2595 PRES  : aitor-fernandez --> aitor-fernandez\n",
      " 903 out of 2595 GOOGLE: kevin --> kevin-vazquez\n",
      " 909 out of 2595 GOOGLE: fernando-pacheco --> fernando-pacheco\n",
      " 910 out of 2595 GOOGLE: daniel-torres --> dani-torres\n",
      " 916 out of 2595 GOOGLE: raul-garcia --> raul-garcia\n",
      " 919 out of 2595 GOOGLE: antonio-moya --> toni-moya\n",
      " 922 out of 2595 GOOGLE: raba --> dani-raba\n",
      " 938 out of 2595 GOOGLE: djene-dakonam --> djene\n",
      " 953 out of 2595 GOOGLE: hacen --> moctar-el-hacen\n",
      " 955 out of 2595 PERM2 : jose-arnaiz --> jose-manuel-arnaiz\n",
      " 958 out of 2595 GOOGLE: anthony-lozano --> choco-lozano\n",
      " 961 out of 2595 GOOGLE: manuel-morlanes --> manu-morlanes\n",
      " 964 out of 2595 PERM2 : dennis-eckert --> dennis-eckert-ayensa\n",
      " 974 out of 2595 PRES  : alex-lopez --> alex-lopez\n",
      " 976 out of 2595 GOOGLE: arthur --> arthur-melo\n",
      " 986 out of 2595 GOOGLE: daniel-ojeda --> dani-ojeda\n",
      " 987 out of 2595 PERM1 : paik-seung-ho --> seung-ho-paik\n",
      " 993 out of 2595 GOOGLE: francisco-montero --> javi-montero\n",
      " 996 out of 2595 GOOGLE: rodrigo-tarin --> rodri-tarin\n",
      " 999 out of 2595 GOOGLE: moises-delgado --> moi-delgado\n",
      "1010 out of 2595 GOOGLE: camacho --> juanjo-camacho\n",
      "1012 out of 2595 PERM1 : lee-kang-in --> kang-in-lee\n",
      "1013 out of 2595 GOOGLE: cristo-gonzalez --> cristo\n",
      "1026 out of 2595 GOOGLE: xavier-quintilla --> xavi-quintilla\n",
      "1028 out of 2595 GOOGLE: javier-diaz --> javi-diaz\n",
      "1061 out of 2595 PERM1 : gian-luca-waldschmidt --> luca-waldschmidt\n",
      "1074 out of 2595 PRES  : mario-gomez --> mario-gomez\n",
      "1092 out of 2595 PRES  : jonas-hofmann --> jonas-hofmann\n",
      "1125 out of 2595 PERM2 : john-brooks --> john-anthony-brooks\n",
      "1136 out of 2595 PRES  : javi-martinez --> javi-martinez\n",
      "1157 out of 2595 PRES  : matheus-pereira --> matheus-pereira\n",
      "1165 out of 2595 GOOGLE: felipe --> felipe-trevizan\n",
      "1169 out of 2595 PRES  : wendell --> wendell\n",
      "1176 out of 2595 GOOGLE: thiago-alcantara --> thiago\n",
      "1197 out of 2595 PRES  : william --> william\n",
      "1206 out of 2595 GOOGLE: joshua-sargent --> josh-sargent\n",
      "1210 out of 2595 GOOGLE: yunus-malli --> yunus-mallı\n",
      "1220 out of 2595 PRES  : rafinha --> rafinha\n",
      "1236 out of 2595 GOOGLE: yevhen-konoplyanka --> yevgen-konoplyanka\n",
      "1286 out of 2595 PERM2 : per-skjelbred --> per-ciljan-skjelbred\n",
      "1309 out of 2595 GOOGLE: naldo --> naldo\n",
      "1389 out of 2595 PERM2 : isaac-thelin --> isaac-kiese-thelin\n",
      "1397 out of 2595 PRES  : aaron-martin --> aaron-martin\n",
      "1401 out of 2595 PRES  : allan --> allan\n",
      "1404 out of 2595 GOOGLE: dayotchanculle-upamecano --> dayot-upamecano\n",
      "1417 out of 2595 PERM1 : gian-luca-itter --> luca-itter\n",
      "1442 out of 2595 PERM1 : chima-sean-okoroji --> chima-okoroji\n",
      "1444 out of 2595 GOOGLE: louis-beyer --> jordan-beyer\n",
      "1449 out of 2595 GOOGLE: bote-baku --> ridle-baku\n",
      "1450 out of 2595 GOOGLE: kasim-nuhu --> kasim-adams\n",
      "1470 out of 2595 PRES  : paulinho --> paulinho\n",
      "1476 out of 2595 MAXSIM: philipp-mwene --> phillipp-mwene\n",
      "1478 out of 2595 PERM1 : leandro-barreiro-martins --> leandro-barreiro\n",
      "1482 out of 2595 PERM1 : hans-nunoo-sarpei --> nunoo-sarpei\n",
      "1485 out of 2595 PRES  : ewerton --> ewerton\n",
      "1538 out of 2595 PRES  : joao-pedro --> joao-pedro\n",
      "1558 out of 2595 GOOGLE: keita --> keita-balde\n",
      "1567 out of 2595 GOOGLE: fabian --> fabian-ruiz\n",
      "1583 out of 2595 PRES  : luis-alberto --> luis-alberto\n",
      "1585 out of 2595 MAXSIM: gianmarco-ferrari --> gian-marco-ferrari\n",
      "1587 out of 2595 PRES  : bruno-alves --> bruno-alves\n",
      "1602 out of 2595 PRES  : felipe --> felipe\n",
      "1639 out of 2595 GOOGLE: samir --> samir-caetano\n",
      "1672 out of 2595 MAXSIM: antonio-la-gumina --> antonino-la-gumina\n",
      "1692 out of 2595 PRES  : danilo --> danilo\n",
      "1697 out of 2595 GOOGLE: kostas-manolas --> konstantinos-manolas\n",
      "1707 out of 2595 PRES  : allan --> allan\n",
      "1723 out of 2595 PRES  : joao-mario --> joao-mario\n",
      "1736 out of 2595 PRES  : romulo --> romulo\n",
      "1741 out of 2595 GOOGLE: berat-gjimshiti --> berat-djimsiti\n",
      "1754 out of 2595 GOOGLE: marlon-santos --> marlon\n",
      "1758 out of 2595 PRES  : luiz-felipe --> luiz-felipe\n",
      "1764 out of 2595 PRES  : rogerio --> rogerio\n",
      "1781 out of 2595 PRES  : sandro --> sandro\n",
      "1806 out of 2595 PRES  : wallace --> wallace\n",
      "1815 out of 2595 PRES  : pedro-pereira --> pedro-pereira\n",
      "1832 out of 2595 PRES  : matheus-pereira --> matheus-pereira\n",
      "1847 out of 2595 GOOGLE: jose-reina --> pepe-reina\n",
      "1914 out of 2595 GOOGLE: pepin-machin --> jose-machin\n",
      "1943 out of 2595 PERM1 : juan-manuel-valencia --> juan-valencia\n",
      "1945 out of 2595 MAXSIM: nicolas-schiappacase --> nicolas-schiappacasse\n",
      "1950 out of 2595 PRES  : vitor-hugo --> vitor-hugo\n",
      "2028 out of 2595 PRES  : francesco-verde --> francesco-verde\n",
      "2033 out of 2595 MAXSIM: dimitris-nikolaou --> dimitrios-nikolaou\n",
      "2035 out of 2595 GOOGLE: ibanez --> roger-ibanez\n",
      "2043 out of 2595 PERM1 : paolo-gozzi-iweru --> paolo-gozzi\n",
      "2045 out of 2595 PERM1 : nicky-medja-beloko --> nicky-beloko\n",
      "2048 out of 2595 PERM1 : kylian-mbappe-lottin --> kylian-mbappe\n",
      "2052 out of 2595 GOOGLE: falcao --> radamel-falcao\n",
      "2078 out of 2595 PERM1 : joia-nuno-da-costa --> nuno-da-costa\n",
      "2139 out of 2595 PRES  : marquinhos --> marquinhos\n",
      "2143 out of 2595 PRES  : pablo-martinez --> pablo-martinez\n",
      "2150 out of 2595 PRES  : abdoulaye-toure --> abdoulaye-toure\n",
      "2151 out of 2595 GOOGLE: jordan-siebatcheu --> jordan\n",
      "2152 out of 2595 GOOGLE: sehrou-guirassy --> serhou-guirassy\n",
      "2167 out of 2595 PERM1 : suk-hyun-jun --> hyun-jun-suk\n",
      "2168 out of 2595 GOOGLE: kostas-mitroglou --> konstantinos-mitroglou\n",
      "2170 out of 2595 GOOGLE: naif-aguerd --> nayef-aguerd\n",
      "2174 out of 2595 PRES  : luiz-gustavo --> luiz-gustavo\n",
      "2196 out of 2595 PERM1 : kwon-chang-hoon --> chang-hoon-kwon\n",
      "2205 out of 2595 GOOGLE: john-mendoza --> stiven-mendoza\n",
      "2213 out of 2595 PERM1 : juan-ferney-otero --> juan-otero\n",
      "2237 out of 2595 PRES  : pedro-mendes --> pedro-mendes\n",
      "2241 out of 2595 PRES  : pablo --> pablo\n",
      "2272 out of 2595 PERM1 : firmin-ndombe-mubele --> firmin-mubele\n",
      "2277 out of 2595 GOOGLE: prince --> prince-gouano\n",
      "2278 out of 2595 PERM1 : tanguy-ndombele-alvaro --> tanguy-ndombele\n",
      "2287 out of 2595 PERM1 : mehmet-zeki-celik --> zeki-celik\n",
      "2296 out of 2595 PRES  : jakob-johansson --> jakob-johansson\n",
      "2298 out of 2595 PERM1 : benoit-badiashile-mukinayi --> benoit-badiashile\n",
      "2301 out of 2595 PRES  : marcelo --> marcelo\n",
      "2303 out of 2595 GOOGLE: naldo --> naldo\n",
      "2309 out of 2595 PRES  : manu-garcia --> manu-garcia\n",
      "2321 out of 2595 PRES  : fabio --> fabio\n",
      "2323 out of 2595 PRES  : gabriel-silva --> gabriel-silva\n",
      "2328 out of 2595 GOOGLE: danilo --> danilo-barbosa\n",
      "2343 out of 2595 PRES  : rafael --> rafael\n",
      "2346 out of 2595 PRES  : thiago-silva --> thiago-silva\n",
      "2355 out of 2595 PERM1 : jacques-alaixys-romao --> alaixys-romao\n",
      "2375 out of 2595 PRES  : abdoulaye-diallo --> abdoulaye-diallo\n",
      "2378 out of 2595 GOOGLE: lass-diarra --> lassana-diarra\n",
      "2380 out of 2595 MAXSIM: moustapha-diallo --> mustapha-diallo\n",
      "2399 out of 2595 PRES  : adama-traore --> adama-traore\n",
      "2408 out of 2595 PERM2 : denis-poha --> denis-will-poha\n",
      "2414 out of 2595 GOOGLE: kelvin-adou --> kelvin-amian\n",
      "2448 out of 2595 GOOGLE: fernando-marcal --> marcal\n",
      "2455 out of 2595 PERM1 : james-edward-lea-siliki --> james-lea-siliki\n",
      "2456 out of 2595 GOOGLE: zaydou-youssef --> zaydou-youssouf\n",
      "2459 out of 2595 PERM2 : fode-toure --> fode-ballo-toure\n",
      "2467 out of 2595 MAXSIM: dmitri-lienard --> dimitri-lienard\n",
      "2482 out of 2595 PRES  : otavio --> otavio\n",
      "2485 out of 2595 PERM2 : kouadio-dabila --> kouadio-yves-dabila\n",
      "2490 out of 2595 MAXSIM: ismail-aaneb --> ismail-aaneba\n",
      "2507 out of 2595 MAXSIM: yannis-ammour --> yanis-ammour\n",
      "2520 out of 2595 PRES  : pele --> pele\n",
      "2526 out of 2595 PERM1 : mohamed-lamine-diaby --> mohamed-diaby\n",
      "2548 out of 2595 GOOGLE: hakim-el-mokkedem --> hakim-el-mokeddem\n",
      "2552 out of 2595 GOOGLE: serigne-modou-kara-mbodji --> kara-mbodj\n",
      "2563 out of 2595 PERM1 : thody-elie-youan --> elie-youan\n",
      "2565 out of 2595 PERM1 : charles-nathan-abi --> charles-abi\n",
      "2578 out of 2595 GOOGLE: reinildo --> reinildo-mandava\n",
      "2580 out of 2595 PERM2 : elhadji-diaw --> elhadji-pape-diaw\n",
      "2582 out of 2595 GOOGLE: kouadio-kone --> manu-kone\n",
      "2585 out of 2595 PERM1 : albert-nicolas-lottin --> albert-lottin\n",
      "2589 out of 2595 MAXSIM: yassin-benrahou --> yassine-benrahou\n",
      "2595 out of 2595 PERM1 : nathan-ngoumou-minpol --> nathan-ngoumou\n",
      "CPU times: total: 2min 33s\n",
      "Wall time: 6min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#select only months and years from appearance dates\n",
    "appMonth = app['date'].str.split(\"/\").str[1]\n",
    "appYear = app['date'].str.split(\"/\").str[2]\n",
    "#select appearances from 18/19 season\n",
    "is1819 = ((appYear == \"2018\") & (appMonth >= \"08\")) | ((appYear == \"2019\") & (appMonth <= \"06\"))\n",
    "app1819 = app[is1819]\n",
    "playerCodes = cleanChars(players['player_code'])\n",
    "\n",
    "#new column to store transfermarkt ID\n",
    "stats1819['trID'] = [0] * len(stats1819)\n",
    "\n",
    "#iterate on stats file\n",
    "statsRows = np.size(stats1819, 0);\n",
    "exact_matches = no_matches = resolved_google = resolved_permS = resolved_permP = resolved_max_sim = resolved_pres = i = 0\n",
    "for index, row in stats1819.iterrows():\n",
    "    i += 1\n",
    "    mode = \"NONE\"\n",
    "    player = pd.Series([])\n",
    "    statsName = hyphenize(row['player_name']).replace(\"'\",\"\")\n",
    "    \n",
    "    matchedPlayers = players[playerCodes == statsName]\n",
    "    \n",
    "    #multiple rows with same name in stats mapped to a single player are ok (he has changed team during the season)\n",
    "    if(np.size(matchedPlayers, 0) > 0):\n",
    "        mode = \"MATCH\"\n",
    "       \n",
    "    if(mode == \"NONE\"):\n",
    "        #split name in stats and use permutations strategy\n",
    "        splitStatsName = statsName.split(\"-\")\n",
    "        if(len(splitStatsName) >= 2):\n",
    "            matchedCodes = match_seq(splitStatsName, playerCodes)\n",
    "            if(len(matchedCodes) > 0):\n",
    "                matchedPlayers = players[playerCodes.isin(matchedCodes)]\n",
    "                mode = \"PERM1\"\n",
    "\n",
    "    if(mode == \"NONE\"):\n",
    "        maxSim = 0\n",
    "        maxC = \"\"\n",
    "        for c in playerCodes:\n",
    "            sm = SequenceMatcher(None, statsName, c)\n",
    "            #do not proceed if the upper bound is too small\n",
    "            if(sm.real_quick_ratio() >= 0.5):\n",
    "                #remember: similarity is not commutative\n",
    "                simm = sm.ratio()                  \n",
    "                #if sim is big enough, try permutation strategy with name from players file\n",
    "                if(simm >= 0.6):\n",
    "                    splitC = c.split(\"-\")\n",
    "                    if(len(splitC) >= len(splitStatsName) and len(splitStatsName) >= 2):\n",
    "                        matchedCodes = match_seq(splitC, [statsName])\n",
    "                        if(len(matchedCodes) > 0):\n",
    "                            newMatchedPlayers = players[players['player_code'] == c]\n",
    "                            matchedPlayers = pd.concat([matchedPlayers, newMatchedPlayers])\n",
    "                if(simm > maxSim):\n",
    "                    maxSim = simm\n",
    "                    maxC = c\n",
    "\n",
    "        if(maxSim >= 0.95):\n",
    "            matchedPlayers = players[playerCodes == maxC]\n",
    "            mode = \"MAXSIM\"\n",
    "        elif(np.size(matchedPlayers, 0) > 0):\n",
    "            mode = \"PERM2\"\n",
    "\n",
    "    #managing results of any method\n",
    "    matches = np.size(matchedPlayers, 0)\n",
    "    if(matches == 1):\n",
    "        player = matchedPlayers.iloc[0]\n",
    "        trID = matchedPlayers.index[0]\n",
    "    if(matches > 1):\n",
    "        player, trID = solve_with_apps_approx(row['games'], matchedPlayers, app1819, 3)\n",
    "        if(trID == -1):\n",
    "            mode = \"NONE\"\n",
    "        else:\n",
    "            mode = \"PRES\"\n",
    "    if(mode == \"NONE\"):\n",
    "        splitURL = next(search(row['player_name']+\" \"+row['teams_played_for']+\" transfermarkt profil spieler\", num_results=1)).split(\"/\")\n",
    "        trID = splitURL[len(splitURL) - 1]\n",
    "        if(trID != \"\"):\n",
    "            urlPlayers = players[players.index == int(trID)]\n",
    "            if(len(urlPlayers) != 0):\n",
    "                mode = \"GOOGLE\"\n",
    "                player = urlPlayers.iloc[0]\n",
    "            else:\n",
    "                print(\"Invalid ID \"+trID+\" extracted from \"+str(splitURL))\n",
    "                mode = \"NONE\"\n",
    "        else:\n",
    "            print(\"No ID in URL \"+str(splitURL))\n",
    "            mode = \"NONE\"\n",
    "    \n",
    "    \n",
    "    if(mode == \"NONE\"):\n",
    "        no_matches += 1\n",
    "    elif(mode == \"PERM1\"):\n",
    "        resolved_permS += 1\n",
    "    elif(mode == \"PERM2\"):\n",
    "        resolved_permP += 1\n",
    "    elif(mode == \"GOOGLE\"):\n",
    "        resolved_google += 1\n",
    "    elif(mode == \"MATCH\"):\n",
    "        exact_matches += 1\n",
    "    elif(mode == \"MAXSIM\"):\n",
    "        resolved_max_sim += 1\n",
    "    elif(mode == \"PRES\"):\n",
    "        resolved_pres += 1\n",
    "\n",
    "    if(mode == \"NONE\"):\n",
    "        print(\"{:4d}\".format(i)+\" out of \"+str(statsRows)+\" NONE  : \"+statsName+\", matches: \"+str(matches))\n",
    "    elif(mode != \"MATCH\"):\n",
    "        print(\"{:4d}\".format(i)+\" out of \"+str(statsRows)+\" \"+mode.ljust(6)+\": \"+statsName+\" --> \"+player['player_code'])\n",
    "\n",
    "    if(mode != \"NONE\"):\n",
    "        stats1819.at[index, 'trID'] = int(trID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   --- STATISTICS ---\n",
      "Total matches:                             2595 -- percentage: 100.00%\n",
      "  ---> exact matches:                      2342 -- percentage: 90.25%\n",
      "  ---> resolved permutating statsName:       31 -- percentage: 1.19%\n",
      "  ---> resolved permutating player code:     12 -- percentage: 0.46%\n",
      "  ---> resolved with max sim.:               12 -- percentage: 0.46%\n",
      "  ---> resolved with apps:                   82 -- percentage: 3.16%\n",
      "  ---> resolved with google:                116 -- percentage: 4.47%\n",
      "No matches:                                   0 -- percentage: 0.00%\n",
      "  ---> zero matches found:                    0 -- percentage: 0.00%\n"
     ]
    }
   ],
   "source": [
    "#print statistics\n",
    "print(\"   --- STATISTICS ---\")\n",
    "tot_matches = exact_matches + + resolved_google + resolved_permS + resolved_permP + resolved_max_sim + resolved_pres\n",
    "print(\"Total matches:                            \"+\"{:5d}\".format(tot_matches)+\" -- percentage: \" + \"{:.2f}%\".format(tot_matches*100/statsRows))\n",
    "print(\"  ---> exact matches:                     \"+\"{:5d}\".format(exact_matches)+\" -- percentage: \" + \"{:.2f}%\".format(exact_matches*100/statsRows)) \n",
    "print(\"  ---> resolved permutating statsName:    \"+\"{:5d}\".format(resolved_permS)+\" -- percentage: \" + \"{:.2f}%\".format(resolved_permS*100/statsRows))\n",
    "print(\"  ---> resolved permutating player code:  \"+\"{:5d}\".format(resolved_permP)+\" -- percentage: \" + \"{:.2f}%\".format(resolved_permP*100/statsRows))\n",
    "print(\"  ---> resolved with max sim.:            \"+\"{:5d}\".format(resolved_max_sim)+\" -- percentage: \" + \"{:.2f}%\".format(resolved_max_sim*100/statsRows))\n",
    "print(\"  ---> resolved with apps:                \"+\"{:5d}\".format(resolved_pres)+\" -- percentage: \" + \"{:.2f}%\".format(resolved_pres*100/statsRows))\n",
    "print(\"  ---> resolved with google:              \"+\"{:5d}\".format(resolved_google)+\" -- percentage: \" + \"{:.2f}%\".format(resolved_google*100/statsRows))\n",
    "print(\"No matches:                               \"+\"{:5d}\".format(no_matches)+\" -- percentage: \" + \"{:.2f}%\".format(no_matches*100/statsRows))\n",
    "print(\"  ---> zero matches found:                \"+\"{:5d}\".format(no_matches)+\" -- percentage: \" + \"{:.2f}%\".format((no_matches)*100/statsRows))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Completing and correcting statistics in corner cases\n",
    "We need to manage the fact some rows contain total information about a player switching team in the same league during the season. <br>\n",
    "We can use information in the FBref file to complete our data; we have observed that, in this situation, it contains correct information only for the row of the two which has lower index: some statistics in the second row can be therefore corrected by subtracting the ones in the first row from the total ones. <br>\n",
    "In these cases, we add two columns to our main dataframe, to specify:\n",
    "* the Transfermarkt IDs of the 2 teams;\n",
    "* the indexes of the rows in the FBref file corresponding to the memberships of the player in the 2 teams.\n",
    "\n",
    "Observe that we load FBref stats to make this cell idempotent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Munir\n",
      "Raúl García\n",
      "Nicolai Müller\n",
      "Gregor Kobel\n",
      "Krzysztof Piatek\n",
      "Marko Pajac\n",
      "Emil Hallfredsson\n",
      "Ivan Radovanovic\n",
      "Bartlomiej Dragowski\n",
      "Could match: 52 could not: 9\n"
     ]
    }
   ],
   "source": [
    "#we will use copies of row of the df, so we silence the warnings that would arise\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "stats1819FBref = pd.read_csv(stats1819FBrefUrl, sep=';', index_col='Column1', dtype={\"Attendance\": \"string\"})\n",
    "path = str(Path(os.path.abspath(os.getcwd())))\n",
    "stats1819Url = path + '/stats1819_IDs.csv'\n",
    "stats1819 = pd.read_csv(stats1819Url, sep=',', index_col='indCol')\n",
    "\n",
    "stats1819['teamIDs'] = [list()] * len(stats1819)\n",
    "\n",
    "m = nm = 0\n",
    "newRows = []\n",
    "for ind, row in stats1819.iterrows():\n",
    "    teamIDs = list()\n",
    "    for team in row['teams_played_for'].split(\",\"):\n",
    "        teamIDs.append(int(teamIDDict[team]))\n",
    "    stats1819.at[ind, 'teamIDs'] = teamIDs\n",
    "    stats1819.at[ind, 'teams_played_for'] = row['teams_played_for'].split(\",\")\n",
    "    if(len(teamIDs) == 2):\n",
    "        player = players.loc[row['trID']]\n",
    "        fbmatch = stats1819FBref[stats1819FBref['player'] == player['name']]\n",
    "        if(len(fbmatch) == 2):\n",
    "            m += 1\n",
    "            team0ID, team1ID = teamIDs\n",
    "            team0 = teams.loc[team0ID]['name']\n",
    "\n",
    "            #select the index pointing to wrong row\n",
    "            if(fbmatch.index[0] < fbmatch.index[1]):\n",
    "                corrInd, wrongInd = fbmatch.index\n",
    "            else:\n",
    "                wrongInd, corrInd = fbmatch.index\n",
    "\n",
    "            sm0 = SequenceMatcher(None, team0, fbmatch.loc[corrInd]['squad'])\n",
    "            sm1 = SequenceMatcher(None, team0, fbmatch.loc[wrongInd]['squad'])\n",
    "\n",
    "            #if the team corresponding to first ID matches with fbref row with index wrongInd, swap team IDs\n",
    "            if(sm0.ratio() < sm1.ratio()):\n",
    "                team0ID, team1ID = teamIDs[1], teamIDs[0]\n",
    "\n",
    "            corrRow = stats1819.loc[ind]\n",
    "            wrongRow = stats1819.loc[ind]\n",
    "            \n",
    "            #we do not modify team name: we will only use transfermarkt team ID later\n",
    "            corrRow.at['teams_played_for'] = [teams.loc[team0ID]['name']]\n",
    "            corrRow.at['teamIDs'] = [team0ID]\n",
    "            corrRow.at['games'] = stats1819FBref.at[corrInd, 'games']\n",
    "            corrRow.at['goals'] = stats1819FBref.at[corrInd, 'goals']\n",
    "            corrRow.at['minutes_played'] = stats1819FBref.at[corrInd, 'minutes']\n",
    "            corrRow.at['npg'] = stats1819FBref.at[corrInd, 'goals'] - stats1819FBref.at[corrInd, 'pens_made']\n",
    "            corrRow.at['assists'] = stats1819FBref.at[corrInd, 'assists']\n",
    "            corrRow.at['xG'] = stats1819FBref.at[corrInd, 'xg']\n",
    "            corrRow.at['yellow_cards'] = stats1819FBref.at[corrInd, 'cards_yellow']\n",
    "            corrRow.at['red_cards'] = stats1819FBref.at[corrInd, 'cards_red']\n",
    "            corrRow.at['shots'] = stats1819FBref.at[corrInd, 'shots_total']\n",
    "\n",
    "            wrongRow.at['teams_played_for'] = [teams.loc[team1ID]['name']]\n",
    "            wrongRow.at['teamIDs'] = [team1ID]\n",
    "            wrongRow.at['games'] = row['games'] - corrRow['games']\n",
    "            wrongRow.at['goals'] = row['goals'] - corrRow['goals']\n",
    "            wrongRow.at['minutes_played'] = row['minutes_played'] - corrRow['minutes_played']\n",
    "            wrongRow.at['npg'] = row['npg'] - corrRow['npg']\n",
    "            wrongRow.at['assists'] = row['assists'] - corrRow['assists']\n",
    "            wrongRow.at['xG'] = row['xG'] - corrRow['xG']\n",
    "            wrongRow.at['yellow_cards'] = row['yellow_cards'] - corrRow['yellow_cards']\n",
    "            wrongRow.at['shots'] = row['shots'] - corrRow['shots']\n",
    "            \n",
    "            if(wrongRow.at['shots'] < 0):\n",
    "                print(corrRow)\n",
    "                print(wrongRow)\n",
    "            \n",
    "            newRows.append(corrRow.values)\n",
    "            newRows.append(wrongRow.values)\n",
    "\n",
    "        else:\n",
    "            nm += 1\n",
    "            print(row['player_name'])\n",
    "            \n",
    "print(\"Could match: \"+str(m)+\" could not: \"+str(nm))\n",
    "stats1819 = pd.concat([stats1819, pd.DataFrame(newRows, columns=stats1819.columns, index=range(len(stats1819),len(stats1819)+len(newRows)))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the dataframe\n",
    "stats1819.to_csv(\"stats1819.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the file created in the cell above: we can start from here without executing the matching part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = str(Path(os.path.abspath(os.getcwd())))\n",
    "stats1819Url = path + '/stats1819.csv'\n",
    "stats1819 = pd.read_csv(stats1819Url, sep=',', index_col='indCol')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 78.1 ms\n",
      "Wall time: 93.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for ind, row in countries.iterrows():\n",
    "    country = URIRef(DCSSO[row['Alpha-2 code']])\n",
    "    g.add((country, RDF.type, DCSSO.Country))\n",
    "    g.add((country, FOAF.name, Literal(cleanString(ind), datatype=XSD.string)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Leagues\n",
    "Leagues are added manually because we need to store only five using very limited information from the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 7.8 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Graph identifier=N61b6db8a2fe442e29df2811ffbade5db (<class 'rdflib.graph.Graph'>)>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "SerieA = URIRef(DCSSO[\"IT1\"])\n",
    "g.add((SerieA, RDF.type, DCSSO.League))\n",
    "g.add((SerieA, FOAF['name'], Literal(\"Serie A\", datatype=XSD.string)))\n",
    "g.add((SerieA, DCSSO['hasCountry'], URIRef(DCSSO[\"IT\"])))\n",
    "\n",
    "Ligue1 = URIRef(DCSSO[\"FR1\"])\n",
    "g.add((Ligue1, RDF.type, DCSSO.League))\n",
    "g.add((Ligue1, FOAF['name'], Literal(\"Ligue 1\", datatype=XSD.string)))\n",
    "g.add((Ligue1, DCSSO['hasCountry'], URIRef(DCSSO[\"FR\"])))\n",
    "\n",
    "LaLiga = URIRef(DCSSO[\"ES1\"])\n",
    "g.add((LaLiga, RDF.type, DCSSO.League))\n",
    "g.add((LaLiga, FOAF['name'], Literal(\"LaLiga\", datatype=XSD.string)))\n",
    "g.add((SerieA, DCSSO['hasCountry'], URIRef(DCSSO[\"ES\"])))\n",
    "\n",
    "Premier = URIRef(DCSSO[\"GB1\"])\n",
    "g.add((Premier, RDF.type, DCSSO.League))\n",
    "g.add((Premier, FOAF['name'], Literal(\"Premier League\", datatype=XSD.string)))\n",
    "g.add((Premier, DCSSO['hasCountry'], URIRef(DCSSO[\"GB\"])))\n",
    "\n",
    "Bundesliga = URIRef(DCSSO[\"L1\"])\n",
    "g.add((Bundesliga, RDF.type, DCSSO.League))\n",
    "g.add((Bundesliga, FOAF['name'], Literal(\"Bundesliga\", datatype=XSD.string)))\n",
    "g.add((Bundesliga, DCSSO['hasCountry'], URIRef(DCSSO[\"DE\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Teams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 609 ms\n",
      "Wall time: 922 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for ind, row in teams.iterrows():\n",
    "    Team = URIRef(DCSSO[\"team\"+str(ind)])\n",
    "    g.add((Team, RDF.type, DCSSO.Team))\n",
    "    g.add((Team, FOAF.name, Literal(cleanString(row['name']), datatype=XSD.string)))\n",
    "    g.add((Team, DCSSO['participatesIn'], URIRef(DCSSO[row['domestic_competition_id']])))\n",
    "    for y in range(2015, 2020):\n",
    "        #check if there are any games for this team in season starting in year y\n",
    "        if((games[games['season'] == y]['home_club_id'] == ind).any() == True):\n",
    "            #if there are any, y represents a season in which this team has played in its domestic top league\n",
    "            Participation = URIRef(DCSSO[\"part\"+str(ind)+\"s\"+str(y)])\n",
    "            g.add((Participation, RDF.type, DCSSO.SeasonalParticipation))\n",
    "            g.add((Team, DCSSO['hasParticipation'], Participation))\n",
    "            g.add((Participation, DCSSO['season'], Literal(y, datatype=XSD.int)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1.8 s\n",
      "Wall time: 4.78 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "y = 2018\n",
    "\n",
    "players['first_name'] = players['first_name'].fillna(\"\")\n",
    "players['last_name'] = players['last_name'].fillna(\"\")\n",
    "\n",
    "# iterate over the movies dataframe\n",
    "for index, row in stats1819.iterrows():\n",
    "    tmId = str(row['trID'])\n",
    "    player = players.loc[row['trID']]\n",
    "    # the node has the namespace + the transfermarkt ID as URI\n",
    "    ref = \"player\"+tmId\n",
    "    Footballer = URIRef(DCSSO[ref])\n",
    "    g.add((Footballer, RDF.type, DCSSO.Footballer))\n",
    "    if(player['first_name'] != \"\"):\n",
    "        g.add((Footballer, FOAF['firstName'], Literal(cleanString(player['first_name']), datatype=XSD.string)))\n",
    "    if(player['last_name'] != \"\"):\n",
    "        g.add((Footballer, FOAF['familyName'], Literal(cleanString(player['last_name']), datatype=XSD.string)))\n",
    "\n",
    "    Memb = URIRef(DCSSO[\"memb\"+tmId+\"s\"+str(y)+\"t\"+'_'.join([str(tid) for tid in row['teamIDs']])])\n",
    "    g.add((Memb, RDF.type, DCSSO.SeasonalMembership))\n",
    "    g.add((Footballer, DCSSO['hasMembership'], Memb))\n",
    "    g.add((Memb, DCSSO['season'], Literal(\"2018\", datatype=XSD.int)))\n",
    "    for tid in row['teamIDs']:\n",
    "        teamId = \"team\"+str(tid)\n",
    "        g.add((Memb, DCSSO['forTeam'], URIRef(DCSSO[teamId])))\n",
    "\n",
    "    #statistics\n",
    "    g.add((Memb, DCSSO['games'], Literal(row['games'], datatype=XSD.int)))\n",
    "    g.add((Memb, DCSSO['minutes'], Literal(row['minutes_played'], datatype=XSD.int)))\n",
    "    g.add((Memb, DCSSO['goals'], Literal(row['goals'], datatype=XSD.int)))\n",
    "    g.add((Memb, DCSSO['npg'], Literal(row['npg'], datatype=XSD.int)))\n",
    "    g.add((Memb, DCSSO['assists'], Literal(row['assists'], datatype=XSD.int)))\n",
    "    g.add((Memb, DCSSO['keyPasses'], Literal(row['key_passes'], datatype=XSD.int)))\n",
    "    g.add((Memb, DCSSO['shots'], Literal(row['shots'], datatype=XSD.int)))\n",
    "    g.add((Memb, DCSSO['yellowCards'], Literal(row['yellow_cards'], datatype=XSD.int)))\n",
    "    g.add((Memb, DCSSO['redCards'], Literal(row['red_cards'], datatype=XSD.int)))\n",
    "    \n",
    "    g.add((Memb, DCSSO['xG'], Literal(row['xG'], datatype=XSD.double)))\n",
    "    g.add((Memb, DCSSO['xA'], Literal(row['xA'], datatype=XSD.double)))\n",
    "    g.add((Memb, DCSSO['npxG'], Literal(row['npxG'], datatype=XSD.double)))\n",
    "    g.add((Memb, DCSSO['xG90'], Literal(row['xG90'], datatype=XSD.double)))\n",
    "    g.add((Memb, DCSSO['xA90'], Literal(row['xA90'], datatype=XSD.double)))\n",
    "    g.add((Memb, DCSSO['npxG90'], Literal(row['npxG90'], datatype=XSD.double)))\n",
    "    g.add((Memb, DCSSO['xGBuildup'], Literal(row['xGBuildup'], datatype=XSD.double)))\n",
    "    g.add((Memb, DCSSO['xGChain'], Literal(row['xGChain'], datatype=XSD.double)))\n",
    "    \n",
    "    if(player['position'] != \"Missing\"):\n",
    "        subPosition = player['sub_position'].replace(\" \", \"\").replace(\"-\", \"\")\n",
    "        if(player['position'] == \"Goalkeeper\" or player['position'] == \"Defender\"):\n",
    "            g.add((Footballer, DCSSO['position'], DCSSO[player['position']]))\n",
    "        elif(player['position'] == \"Midfield\"):\n",
    "            g.add((Footballer, DCSSO['position'], DCSSO[\"Midfielder\"]))\n",
    "            subPosition += \"er\"\n",
    "        else:\n",
    "            g.add((Footballer, DCSSO['position'], DCSSO[\"Forward\"]))\n",
    "\n",
    "        g.add((Footballer, DCSSO['subPosition'], DCSSO[subPosition]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "CPU times: total: 2.44 s\n",
      "Wall time: 4.19 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'stats1819.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle'))\n",
    "    #.decode(\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sparqlwrapper\n",
    "# https://rdflib.github.io/sparqlwrapper/\n",
    "\n",
    "import sys\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "\n",
    "endpoint_url = \"https://query.wikidata.org/sparql\"\n",
    "\n",
    "query = \"\"\"SELECT ?oLabel\n",
    "WHERE\n",
    "{\n",
    "wd:Q192923 skos:altLabel ?o.\n",
    "FILTER(isLiteral(?o))\n",
    "SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\" }\n",
    "}\"\"\"\n",
    "\n",
    "\n",
    "def get_results(endpoint_url, query):\n",
    "    user_agent = \"WDQS-example Python/%s.%s\" % (sys.version_info[0], sys.version_info[1])\n",
    "    # TODO adjust user agent; see https://w.wiki/CX6\n",
    "    sparql = SPARQLWrapper(endpoint_url, agent=user_agent)\n",
    "    sparql.setQuery(query)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    return sparql.query().convert()\n",
    "\n",
    "\n",
    "results = get_results(endpoint_url, query)\n",
    "\n",
    "for result in results[\"results\"][\"bindings\"]:\n",
    "    print(result[\"oLabel\"][\"value\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Referential integrity\n",
    "Note that in RDF we are in an open world situation. We cannot guarantee the referential integrity between the entities. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Person\n",
    "\n",
    "Let us generate the RDF data relative to the movie workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "people = pd.read_csv(namesUrl, sep=',', index_col='imdb_name_id', keep_default_na=False, na_values=['_'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "People are modeled with the FOAF ontology. \n",
    "Refer to [FOAF Documentation](http://xmlns.com/foaf/spec/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new graph\n",
    "g = Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "#measure execution time\n",
    "\n",
    "#iterate over the person dataframe\n",
    "for index, row in people.iterrows():\n",
    "    # Create the node to add to the Graph\n",
    "    # the node has the namespace + the person id as URI\n",
    "    Person = URIRef(MO[index])\n",
    "    g.add((Person, RDF.type, FOAF.Person))\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Person, FOAF['name'], Literal(row['name'], datatype=XSD.string)))\n",
    "    if row['date_of_birth'] != '':\n",
    "        try:\n",
    "            datetime.datetime.strptime(str(row['date_of_birth']), '%Y-%m-%d')\n",
    "            g.add((Person, MO['birthday'], Literal(row['date_of_birth'], datatype=XSD.date)))\n",
    "        except ValueError:\n",
    "            # probably it's the year alone\n",
    "            # check length\n",
    "            if (len(row['date_of_birth'])==4):\n",
    "                #it is a year\n",
    "                g.add((Person, MO['birthday'], Literal(row['date_of_birth']+\"-01-01\", datatype=XSD.date)))\n",
    "    \n",
    "    if row['place_of_birth'] != '':\n",
    "        g.add((Person, MO['birthplace'], Literal(row['place_of_birth'], datatype=XSD.string)))\n",
    "    \n",
    "    # check if the death day is not empty--i.e., the person is still alive\n",
    "    if row['date_of_death'] != '':\n",
    "        try:\n",
    "            datetime.datetime.strptime(str(row['date_of_death']), '%Y-%m-%d')\n",
    "            g.add((Person, MO['deathDay'], Literal(row['date_of_death'], datatype=XSD.date)))\n",
    "        except ValueError:\n",
    "            # probably it's the year alone\n",
    "            # check length\n",
    "            if (len(row['date_of_death'])==4):\n",
    "                #it is a year\n",
    "                g.add((Person, MO['deathDay'], Literal(row['date_of_death']+\"-01-01\", datatype=XSD.date)))\n",
    "        \n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"mo\", MO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'names.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle').decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Person-Movie Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "join = pd.read_csv(joinTableUrl, sep=',', index_col='imdb_title_id', keep_default_na=False, na_values=['_'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new graph\n",
    "g = Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regular expressions\n",
    "import re\n",
    "actor = re.compile('act*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "#measure execution time\n",
    "\n",
    "#iterate over the join table dataframe\n",
    "for index, row in join.iterrows():\n",
    "    # Create the node about the movie\n",
    "    # note that we do not add this resource to the database (created before)\n",
    "    Movie = URIRef(MO[index])\n",
    "    \n",
    "    # Create the node about the person\n",
    "    # note that we do not add this resource to the database (created before)\n",
    "    Person = URIRef(MO[row['imdb_name_id']])\n",
    "    # get the role of the person\n",
    "    role = row['category']\n",
    "    \n",
    "    # we have an actor or actress\n",
    "    if actor.match(role): \n",
    "        g.add((Person, MO['acted'], Movie))\n",
    "    elif (role=='director'):\n",
    "        g.add((Person, MO['directed'], Movie))\n",
    "    else:\n",
    "        # note that, with the defined ontology, we cannot caracterize the specific role of this person in the movie. \n",
    "        # why?\n",
    "        g.add((Person, MO['worked'], Movie))\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"mo\", MO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'name_movie_join.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle').decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Awards - Oscars data\n",
    "Note that if we do not check the referential integrity then we could produce ghost triple movie-nominee-oscar where the movie is not in the RDF graph.\n",
    "\n",
    "On the other hand, we can check if an actor or a movie exists by using the DataFrame in Python. Note that this is an external check and not a constraints met by the RDF DB.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "oscars = pd.read_csv(oscarsUrl, sep=',', keep_default_na=False, na_values=['_'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from num2words import num2words\n",
    "import string\n",
    "import re\n",
    "#create a new graph\n",
    "g = Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#iterate over the join table dataframe\n",
    "for index,row in oscars.iterrows():\n",
    "    #create the oscar with a custom id \n",
    "    cat = re.sub(r'[^\\w\\s]','',row['category'])\n",
    "    Oscar = URIRef(MO['oscar_'+cat.replace(\" \", \"\").lower()+'_'+ str(num2words(row['ceremony'], to='ordinal'))])\n",
    "    \n",
    "    # check if there already is at least a triple about this oscar\n",
    "    if not (Oscar, None, None) in g:    \n",
    "        # check if the oscar is already in the graph\n",
    "        g.add((Oscar, RDF.type, MO.Oscar))\n",
    "        g.add((Oscar, MO['category'], Literal(row['category'].lower(), datatype=XSD.string)))\n",
    "        g.add((Oscar, MO['year'], Literal(row['year_ceremony'], datatype=XSD.gYear)))\n",
    "    \n",
    "    # check if there is a name matching the people, meaning that the oscar can be associated to a person\n",
    "    if (people[\"name\"] == row[\"name\"]).any() == True :\n",
    "        #there is a person with this name\n",
    "        # Create the node about the person\n",
    "        # note that we do not add this resource to the database (created before)\n",
    "        Person = URIRef(MO[people[people[\"name\"]==row[\"name\"]].index[0]])\n",
    "        if row['winner']:\n",
    "            g.add((Person, MO['winner'], Oscar))\n",
    "        else:\n",
    "            g.add((Person, MO['nominated'], Oscar))\n",
    "    \n",
    "    # an oscar for a person is also to be considered an oscar for the movie\n",
    "    # check if the movie is in our DB\n",
    "    if (movies[\"original_title\"] == row[\"film\"]).any():\n",
    "        # there is a movie with this title\n",
    "        Movie = URIRef(MO[movies[movies[\"original_title\"]==row[\"film\"]].index[0]])\n",
    "        if row['winner']:\n",
    "            g.add((Movie, MO['winner'], Oscar))\n",
    "        else:\n",
    "            g.add((Movie, MO['nominated'], Oscar))\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"mo\", MO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'oscars.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle').decode(\"utf-8\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
