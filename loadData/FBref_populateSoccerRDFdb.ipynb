{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Population of our RDF database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# parameters and URLs\n",
    "path = str(Path(os.path.abspath(os.getcwd())).parent.absolute())\n",
    "stats1819Url = path + '/inDepthSoccerStats/transfermarkt_fbref_201819.csv'\n",
    "playersUrl = path + '/inDepthSoccerStats/players.csv'\n",
    "appUrl = path + '/inDepthSoccerStats/appearances.csv'\n",
    "teamsUrl = path + '/inDepthSoccerStats/clubs.csv'\n",
    "leaguesUrl = path + '/inDepthSoccerStats/competitions.csv'\n",
    "gamesUrl = path + '/inDepthSoccerStats/games.csv'\n",
    "\n",
    "# country codes\n",
    "countriesURL = path + '/inDepthSoccerStats/wikipedia-iso-country-codes.csv'\n",
    "\n",
    "# saving folder\n",
    "savePath =  path + '/rdf/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "Index: 2100 entries, 207 to 2537\n",
      "Series name: Attendance\n",
      "Non-Null Count  Dtype \n",
      "--------------  ----- \n",
      "2100 non-null   string\n",
      "dtypes: string(1)\n",
      "memory usage: 32.8 KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2100 entries, 207 to 2537\n",
      "Columns: 399 entries, player to Season\n",
      "dtypes: float64(239), int64(151), object(8), string(1)\n",
      "memory usage: 6.4+ MB\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV files in memory\n",
    "stats1819 = pd.read_csv(stats1819Url, sep=';', index_col='Column1', dtype={\"Attendance\": \"string\"})\n",
    "players = pd.read_csv(playersUrl, sep=',', index_col='player_id')\n",
    "app = pd.read_csv(appUrl, sep=',', index_col='appearance_id')\n",
    "teams = pd.read_csv(teamsUrl, sep=',', index_col='club_id')\n",
    "leagues = pd.read_csv(leaguesUrl, sep=',', index_col='competition_id')\n",
    "games = pd.read_csv(gamesUrl, sep=',', index_col='game_id')\n",
    "\n",
    "stats1819['Attendance'] = stats1819['Attendance'].str.replace(\",\",\"\")\n",
    "stats1819.astype({'Attendance': 'int32'}).dtypes\n",
    "stats1819['Attendance'].info()\n",
    "\n",
    "# cast year to int. If type(year) = str --> Literal= year-01-01\n",
    "#stats1920.astype({'year': 'int32'}).dtypes\n",
    "#load the country codes\n",
    "# we need to convert NaN values to something else otherwise NA strings are converted to NaN -> problem with Namibia\n",
    "countries = pd.read_csv(countriesURL, sep=',', index_col='English short name lower case', keep_default_na=False, na_values=['_'])\n",
    "\n",
    "stats1819.info()\n",
    "#players.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the required libraries\n",
    "#!pip install rdflib\n",
    "from rdflib import Graph, Literal, RDF, URIRef, Namespace, term\n",
    "# rdflib knows about some namespaces, like FOAF\n",
    "from rdflib.namespace import FOAF, XSD\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Namespace and prefixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the country and the movie ontology namespaces not known by RDFlib\n",
    "#CNS = Namespace(\"http://eulersharp.sourceforge.net/2003/03swap/countries#\")\n",
    "DCSSO = Namespace(\"http://www.dei.unipd.it/db2/dcsso#\")\n",
    "\n",
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "#g.bind(\"countries\", CNS)\n",
    "g.bind(\"dcsso\", DCSSO)\n",
    "\n",
    "#term.bind(\n",
    "#    XSD.double,\n",
    "#    float,\n",
    "#   constructor=float,\n",
    "#    lexicalizer=lambda val: f\"{val:f}\",\n",
    "#    datatype_specific=True\n",
    "#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing and matching utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "#!pip install unidecode\n",
    "#!pip install googlesearch-python\n",
    "from unidecode import unidecode\n",
    "from itertools import permutations\n",
    "from difflib import SequenceMatcher\n",
    "from googlesearch import search\n",
    "from re import sub\n",
    "\n",
    "#string parsing\n",
    "\n",
    "def nameToRef(name):\n",
    "    return unidecode(name.replace(\" \",\"\"))\n",
    "\n",
    "def hyphenize(s):\n",
    "    return unidecode(s.lower().replace(\" \",\"-\").replace(\"&#039;\",\"'\"))\n",
    "\n",
    "def cleanChars(item):\n",
    "    return item.str.replace(\"ć\", \"c\").str.replace(\"ğ\",\"g\").str.replace(\"İ\",\"i\").str.replace(\"-scaron-\",\"s\")\n",
    "\n",
    "def cleanString(s):\n",
    "    return unidecode(s.replace(\"ć\", \"c\").replace(\"ğ\",\"g\").replace(\"İ\",\"i\"))\n",
    "\n",
    "#permutations strategy\n",
    "\n",
    "def genSeqByLength(perm, length):\n",
    "    newName = \"\"\n",
    "    for j in range(0, length - 1):\n",
    "        newName = newName + perm[j] + \"-\"\n",
    "    newName = newName + perm[length - 1]\n",
    "    return newName\n",
    "\n",
    "def match_seq(splitS1, list):\n",
    "    resultList = []\n",
    "    #iterate on all possible permutations\n",
    "    length = -1\n",
    "    if(len(list) == 1):\n",
    "        length = len(list[0].split(\"-\"))\n",
    "        uniqueItem = list[0]\n",
    "    for perm in permutations(splitS1):\n",
    "        if(length != -1):\n",
    "            newName = genSeqByLength(perm, length)\n",
    "            if(uniqueItem == newName):\n",
    "                return [uniqueItem]\n",
    "        #consider all lengths from 2 to n\n",
    "        else:\n",
    "            for i in range(2, len(splitS1) + 1):\n",
    "                newName = genSeqByLength(perm, i)\n",
    "                for item in list:\n",
    "                    if(item == newName):\n",
    "                        resultList.append(newName)\n",
    "    return resultList\n",
    "\n",
    "#multiple matches resolution\n",
    "\n",
    "def getAppsByID(ID, apps):\n",
    "    return np.size(apps[apps['player_id'] == ID], 0);\n",
    "\n",
    "def solve_with_apps_approx(statsGames, somePlayers, appsCol):\n",
    "    minDiff = 50\n",
    "    minInd = 0\n",
    "    for ind in somePlayers.index:\n",
    "        if(abs(statsGames - getAppsByID(ind, appsCol)) < minDiff):\n",
    "            minDiff = abs(statsGames - getAppsByID(ind, appsCol))\n",
    "            minInd = ind\n",
    "    if(minDiff <= 5):\n",
    "        player = somePlayers[somePlayers.index == minInd].iloc[0] \n",
    "        return player\n",
    "    return pd.Series([])\n",
    "\n",
    "def solve_with_height(statsHeight, somePlayers):\n",
    "    minDiff = 50\n",
    "    player = pd.Series([])\n",
    "    for ind, row in somePlayers.iterrows():\n",
    "        diff = abs(statsHeight - row['height_in_cm'])\n",
    "        if(diff <= 5 and diff < minDiff):\n",
    "            minDiff = diff\n",
    "            player = row\n",
    "    return player\n",
    "\n",
    "def solve_with_apps_birthyear(somePlayers, statsGames, statsYear, appsCol):\n",
    "    minDiff = 50\n",
    "    player = pd.Series([])\n",
    "    somePlayers = somePlayers[(statsYear - somePlayers['date_of_birth'].str[:4:].apply(int)) == 0]\n",
    "    if(np.size(somePlayers, 0) == 1):\n",
    "        return somePlayers.iloc[0]\n",
    "    elif(np.size(somePlayers, 0) == 0):\n",
    "        print(\"No match on birthdate \"+str(statsYear))\n",
    "        return player\n",
    "    for ind, row in somePlayers.iterrows():\n",
    "        diff = abs(statsGames - getAppsByID(ind, appsCol))\n",
    "        if(diff < minDiff):\n",
    "            minDiff = diff\n",
    "            player = row\n",
    "    if(minDiff > 3 and minDiff < 50):\n",
    "        print(\"minDifff: \"+str(minDiff))\n",
    "        player = pd.Series([])\n",
    "    return player\n",
    "\n",
    "def solve_with_birthyear(somePlayers, statsYear):\n",
    "    somePlayers = somePlayers[statsYear == somePlayers['date_of_birth'].str[:4:].apply(int)]\n",
    "    if(len(somePlayers) == 1):\n",
    "        return somePlayers.iloc[0], somePlayers.index[0]\n",
    "    else:\n",
    "        return pd.Series([]), 0\n",
    "\n",
    "def solve_with_maxsim_by_birthyear(players, statsName, statsYear, statsGames, appsCol):\n",
    "    maxSim = -1\n",
    "    player = pd.Series([])\n",
    "    yearPlayers = players[statsYear == players['date_of_birth'].replace(\"\",\"0000\").str[:4:].apply(float).fillna(0).apply(int)]\n",
    "    for ind, row in yearPlayers.iterrows():\n",
    "        if(abs(statsGames - getAppsByID(ind, appsCol)) < 5):\n",
    "            sm = SequenceMatcher(None, statsName, row['player_code'])\n",
    "            curr = sm.ratio()\n",
    "            if(curr > maxSim):\n",
    "                maxSim = curr\n",
    "                player = row\n",
    "    return player"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matching teams from different sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Milan --> AC Milan\n",
      "Lille --> LOSC Lille\n",
      "Roma --> AS Roma\n",
      "Valladolid --> Real Valladolid CF\n",
      "Wolves --> Wolverhampton Wanderers\n",
      "Strasbourg --> RC Strasbourg Alsace\n",
      "Reims --> Stade Reims\n",
      "Nice --> OGC Nice\n",
      "Nürnberg --> 1.FC Nuremberg\n",
      "Caen --> SM Caen\n",
      "Lyon --> Olympique Lyon\n",
      "M'Gladbach --> Borussia Mönchengladbach\n",
      "Udinese --> Udinese Calcio\n",
      "Espanyol --> RCD Espanyol Barcelona\n",
      "Bologna --> Bologna FC 1909\n",
      "Alavés --> Deportivo Alavés\n",
      "Bordeaux --> FC Girondins Bordeaux\n",
      "West Ham --> West Ham United\n",
      "Inter --> Inter Milan\n",
      "Genoa --> Genoa CFC\n",
      "Eibar --> SD Eibar\n",
      "Düsseldorf --> Fortuna Düsseldorf\n",
      "Parma --> Parma Calcio 1913\n",
      "Lazio --> SS Lazio\n",
      "Hoffenheim --> TSG 1899 Hoffenheim\n",
      "Dortmund --> Borussia Dortmund\n",
      "Tottenham --> Tottenham Hotspur\n",
      "Chievo --> Chievo Verona\n",
      "Betis --> Real Betis Balompié\n",
      "Dijon --> Dijon FCO\n",
      "Angers --> Angers SCO\n",
      "Mainz 05 --> 1.FSV Mainz 05\n",
      "Napoli --> SSC Napoli\n",
      "Leverkusen --> Bayer 04 Leverkusen\n",
      "Athletic Club --> Athletic Bilbao\n",
      "Brighton --> Brighton & Hove Albion\n",
      "Nîmes --> Nîmes Olympique\n",
      "Cagliari --> Cagliari Calcio\n",
      "Marseille --> Olympique Marseille\n",
      "Paris S-G --> Paris Saint-Germain\n",
      "Rennes --> Stade Rennais FC\n",
      "Frosinone --> Frosinone Calcio\n",
      "Good: 56 Tot: 98\n"
     ]
    }
   ],
   "source": [
    "statsTeams = list(set(stats1819['squad']))\n",
    "teamIDDict = dict()\n",
    "good = 0\n",
    "for statsTeam in statsTeams:\n",
    "    maxS = 0\n",
    "    maxId = 0\n",
    "    for tind, trow in teams.iterrows():\n",
    "        sm = SequenceMatcher(None, statsTeam, trow['name'])\n",
    "        sim = sm.ratio()\n",
    "        if(sim > maxS):\n",
    "            maxS = sim\n",
    "            maxId = tind\n",
    "\n",
    "    if(maxS < 0.8):\n",
    "        splitURL = next(search(statsTeam+\" transfermarkt startseite verein\", num_results=1)).split(\"/\")\n",
    "        #some teams contain numbers in their name, so we need to take only the suffix of the URL\n",
    "        trID = splitURL[len(splitURL) - 1]\n",
    "        if(trID != \"\"):\n",
    "            if(len(teams[teams.index == int(trID)]) == 1):\n",
    "                maxId = int(trID)\n",
    "                print(\"GOOGLE: \"+statsTeam+\" --> \"+teams.at[maxId, 'name'])\n",
    "            else:\n",
    "                print(\"Invalid ID extracted from \"+URL)\n",
    "        else:\n",
    "            print(\"No ID in URL \"+URL)\n",
    "\n",
    "    teamIDDict[statsTeam] = maxId"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matching players from different sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nürnberg --> 1.FC Nuremberg\n",
      "CPU times: total: 8.75 s\n",
      "Wall time: 37 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#select only months and years from appearance dates\n",
    "appMonth = app['date'].str.split(\"/\").str[1]\n",
    "appYear = app['date'].str.split(\"/\").str[2]\n",
    "#select appearances from 18/19 season\n",
    "is1819 = ((appYear == \"2018\") & (appMonth >= \"08\")) | ((appYear == \"2019\") & (appMonth <= \"06\"))\n",
    "app1819 = app[is1819]\n",
    "\n",
    "playerCodes = cleanChars(players['player_code'])\n",
    "\n",
    "#new column to store transfermarkt ID\n",
    "stats1819['trID'] = [0] * len(stats1819)\n",
    "stats1819['teamIDs'] = [\"\"] * len(stats1819)\n",
    "\n",
    "diff = 0\n",
    "#iterate on stats file\n",
    "exact_matches = no_matches = resolved_google = resolved_permS = resolved_permP = resolved_by = unable_to_solve = 0\n",
    "for index, row in stats1819.iterrows():\n",
    "    mode = \"NONE\"\n",
    "    player = pd.Series([])\n",
    "    statsName = hyphenize(row['player']).replace(\"'\",\"\")\n",
    "    \n",
    "    matchedPlayers = players[playerCodes == statsName]\n",
    "    \n",
    "    #multiple rows with same name in stats mapped to a single player are ok (he has changed team during the season)\n",
    "    if(np.size(matchedPlayers, 0) > 0):\n",
    "        mode = \"MATCH\"\n",
    "       \n",
    "    if(mode == \"NONE\"):\n",
    "        #split name in stats and use permutations strategy\n",
    "        splitStatsName = statsName.split(\"-\")\n",
    "        if(len(splitStatsName) >= 2):\n",
    "            matchedCodes = match_seq(splitStatsName, playerCodes)\n",
    "            if(len(matchedCodes) > 0):\n",
    "                matchedPlayers = players[playerCodes.isin(matchedCodes)]\n",
    "                mode = \"PERM1\"\n",
    "\n",
    "    if(mode == \"NONE\"):\n",
    "        maxSim = 0\n",
    "        maxC = \"\"\n",
    "        for c in playerCodes:\n",
    "            sm = SequenceMatcher(None, statsName, c)\n",
    "            #do not proceed if the upper bound is too small\n",
    "            if(sm.real_quick_ratio() >= 0.5):\n",
    "                #remember: similarity is not commutative\n",
    "                #if sim is big enough, try permutation strategy with name from players file\n",
    "                if(sm.ratio() >= 0.6):\n",
    "                    splitC = c.split(\"-\")\n",
    "                    if(len(splitC) >= len(splitStatsName) and len(splitStatsName) >= 2):\n",
    "                        matchedCodes = match_seq(splitC, [statsName])\n",
    "                        if(len(matchedCodes) > 0):\n",
    "                            newMatchedPlayers = players[players['player_code'] == c]\n",
    "                            matchedPlayers = pd.concat([matchedPlayers, newMatchedPlayers])\n",
    "\n",
    "        if(np.size(matchedPlayers, 0) > 0):\n",
    "            mode = \"PERM2\"\n",
    "\n",
    "    #managing results of any method\n",
    "    matches = np.size(matchedPlayers, 0)\n",
    "    if(matches == 1):\n",
    "        player = matchedPlayers.iloc[0]\n",
    "        trID = matchedPlayers.index[0]\n",
    "    if(matches > 1):\n",
    "        #player = solve_with_apps_approx(row['games'], matchedPlayers, app1819)\n",
    "        player, trID = solve_with_birthyear(matchedPlayers, row['birth_year'])\n",
    "        if(len(player) == 0):\n",
    "            mode = \"NONE\"\n",
    "        else:\n",
    "            mode = \"BY\"\n",
    "    if(mode == \"NONE\"):\n",
    "        splitURL = next(search(row['player']+\" \"+row['squad']+\" \"+str(row['birth_year'])+\" transfermarkt profil spieler\", num_results=1)).split(\"/\")\n",
    "        trID = splitURL[len(splitURL) - 1]\n",
    "        if(trID != \"\"):\n",
    "            urlPlayers = players[players.index == int(trID)]\n",
    "            if(len(urlPlayers) != 0):\n",
    "                player = urlPlayers.iloc[0]\n",
    "                mode = \"GOOGLE\"\n",
    "            else:\n",
    "                print(\"Invalid ID extracted from \"+URL)\n",
    "        else:\n",
    "            print(\"No ID in URL \"+URL)\n",
    "    \n",
    "    if(mode == \"NONE\"):\n",
    "        no_matches += 1\n",
    "    elif(mode == \"PERM1\"):\n",
    "        resolved_permS += 1\n",
    "    elif(mode == \"PERM2\"):\n",
    "        resolved_permP += 1\n",
    "    elif(mode == \"GOOGLE\"):\n",
    "        resolved_google += 1\n",
    "    elif(mode == \"MATCH\"):\n",
    "        exact_matches += 1\n",
    "    elif(mode == \"BY\"):\n",
    "        resolved_by += 1\n",
    "\n",
    "    if(mode == \"NONE\"):\n",
    "        print(\"NONE: \"+row['player']+\", matches: \"+str(matches))\n",
    "    elif(mode != \"MATCH\"):\n",
    "        print(mode+\": \"+row['player']+\" --> \"+player['player_code'])\n",
    "\n",
    "    if(mode != \"NONE\"):\n",
    "        stats1819.at[index, 'trID'] = int(trID)\n",
    "        stats1819.at[index, 'teamIDs'] = int(teamIDDict[row['squad']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   --- STATISTICS ---\n",
      "Total matches:                             2100 -- percentage: 100.00%\n",
      "  ---> exact matches:                      1995 -- percentage: 95.00%\n",
      "  ---> resolved permutating statsName:        2 -- percentage: 0.10%\n",
      "  ---> resolved permutating player code:      1 -- percentage: 0.05%\n",
      "  ---> resolved with google:                 17 -- percentage: 0.81%\n",
      "  ---> resolved with birthyear:              85 -- percentage: 4.05%\n",
      "No matches:                                   0 -- percentage: 0.00%\n",
      "  ---> unable to resolve between matches:     0 -- percentage: 0.00%\n",
      "  ---> zero matches found:                    0 -- percentage: 0.00%\n"
     ]
    }
   ],
   "source": [
    "#print statistics\n",
    "statsRows = np.size(stats1819, 0);\n",
    "print(\"   --- STATISTICS ---\")\n",
    "tot_matches = exact_matches + + resolved_google + resolved_permS + resolved_permP + resolved_by\n",
    "print(\"Total matches:                            \"+\"{:5d}\".format(tot_matches)+\" -- percentage: \" + \"{:.2f}%\".format(tot_matches*100/statsRows))\n",
    "print(\"  ---> exact matches:                     \"+\"{:5d}\".format(exact_matches)+\" -- percentage: \" + \"{:.2f}%\".format(exact_matches*100/statsRows)) \n",
    "print(\"  ---> resolved permutating statsName:    \"+\"{:5d}\".format(resolved_permS)+\" -- percentage: \" + \"{:.2f}%\".format(resolved_permS*100/statsRows))\n",
    "print(\"  ---> resolved permutating player code:  \"+\"{:5d}\".format(resolved_permP)+\" -- percentage: \" + \"{:.2f}%\".format(resolved_permP*100/statsRows))\n",
    "print(\"  ---> resolved with google:              \"+\"{:5d}\".format(resolved_google)+\" -- percentage: \" + \"{:.2f}%\".format(resolved_google*100/statsRows))\n",
    "print(\"  ---> resolved with birthyear:           \"+\"{:5d}\".format(resolved_by)+\" -- percentage: \" + \"{:.2f}%\".format(resolved_by*100/statsRows))\n",
    "print(\"No matches:                               \"+\"{:5d}\".format(no_matches)+\" -- percentage: \" + \"{:.2f}%\".format(no_matches*100/statsRows))\n",
    "print(\"  ---> unable to resolve between matches: \"+\"{:5d}\".format(unable_to_solve)+\" -- percentage: \" + \"{:.2f}%\".format(unable_to_solve*100/statsRows))\n",
    "print(\"  ---> zero matches found:                \"+\"{:5d}\".format(no_matches - unable_to_solve)+\" -- percentage: \" + \"{:.2f}%\".format((no_matches - unable_to_solve)*100/statsRows))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 15.6 ms\n",
      "Wall time: 39.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for ind, row in countries.iterrows():\n",
    "    country = URIRef(DCSSO[row['Alpha-2 code']])\n",
    "    g.add((country, RDF.type, DCSSO.Country))\n",
    "    g.add((country, FOAF.name, Literal(cleanString(ind), datatype=XSD.string)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Leagues\n",
    "Leagues are added manually because we need to store only five using very limited information from the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 2.09 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Graph identifier=Nbc841beccb4849d6811bf0e13c1281d9 (<class 'rdflib.graph.Graph'>)>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "SerieA = URIRef(DCSSO[\"IT1\"])\n",
    "g.add((SerieA, RDF.type, DCSSO.League))\n",
    "g.add((SerieA, FOAF['name'], Literal(\"Serie A\", datatype=XSD.string)))\n",
    "g.add((SerieA, DCSSO['hasCountry'], URIRef(DCSSO[\"IT\"])))\n",
    "\n",
    "Ligue1 = URIRef(DCSSO[\"FR1\"])\n",
    "g.add((Ligue1, RDF.type, DCSSO.League))\n",
    "g.add((Ligue1, FOAF['name'], Literal(\"Ligue 1\", datatype=XSD.string)))\n",
    "g.add((Ligue1, DCSSO['hasCountry'], URIRef(DCSSO[\"FR\"])))\n",
    "\n",
    "LaLiga = URIRef(DCSSO[\"ES1\"])\n",
    "g.add((LaLiga, RDF.type, DCSSO.League))\n",
    "g.add((LaLiga, FOAF['name'], Literal(\"LaLiga\", datatype=XSD.string)))\n",
    "g.add((SerieA, DCSSO['hasCountry'], URIRef(DCSSO[\"ES\"])))\n",
    "\n",
    "Premier = URIRef(DCSSO[\"GB1\"])\n",
    "g.add((Premier, RDF.type, DCSSO.League))\n",
    "g.add((Premier, FOAF['name'], Literal(\"Premier League\", datatype=XSD.string)))\n",
    "g.add((Premier, DCSSO['hasCountry'], URIRef(DCSSO[\"GB\"])))\n",
    "\n",
    "Bundesliga = URIRef(DCSSO[\"L1\"])\n",
    "g.add((Bundesliga, RDF.type, DCSSO.League))\n",
    "g.add((Bundesliga, FOAF['name'], Literal(\"Bundesliga\", datatype=XSD.string)))\n",
    "g.add((Bundesliga, DCSSO['hasCountry'], URIRef(DCSSO[\"DE\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Teams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 234 ms\n",
      "Wall time: 441 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for ind, row in teams.iterrows():\n",
    "    Team = URIRef(DCSSO[\"team\"+str(ind)])\n",
    "    g.add((Team, RDF.type, DCSSO.Team))\n",
    "    g.add((Team, FOAF.name, Literal(cleanString(row['name']), datatype=XSD.string)))\n",
    "    g.add((Team, DCSSO['participatesIn'], URIRef(DCSSO[row['domestic_competition_id']])))\n",
    "    for y in range(2015, 2020):\n",
    "        #check if there are any games for this team in season starting in year y\n",
    "        if((games[games['season'] == y]['home_club_id'] == ind).any() == True):\n",
    "            #if there are any, y represents a season in which this team has played in its domestic top league\n",
    "            Participation = URIRef(DCSSO[\"part\"+str(ind)+\"s\"+str(y)])\n",
    "            g.add((Participation, RDF.type, DCSSO.SeasonalParticipation))\n",
    "            g.add((Team, DCSSO['hasParticipation'], Participation))\n",
    "            g.add((Participation, DCSSO['season'], Literal(y, datatype=XSD.int)))\n",
    "\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tomovic\n",
      "Milinkovic-Savic\n",
      "CPU times: total: 1.12 s\n",
      "Wall time: 1.72 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "players['first_name'] = players['first_name'].fillna(\"\")\n",
    "players['last_name'] = players['last_name'].fillna(\"\")\n",
    "\n",
    "for index, row in stats1819.iterrows():\n",
    "    tmId = str(row['trID'])\n",
    "    player = players.loc[row['trID']]\n",
    "    # the node has the namespace + the transfermarkt ID as URI\n",
    "    ref = \"player\"+tmId\n",
    "    Footballer = URIRef(DCSSO[ref])\n",
    "    g.add((Footballer, RDF.type, DCSSO.Footballer))\n",
    "    if(player['first_name'] != \"\"):\n",
    "        g.add((Footballer, FOAF['firstName'], Literal(cleanString(player['first_name']), datatype=XSD.string)))\n",
    "    if(player['last_name'] != \"\"):\n",
    "        g.add((Footballer, FOAF['familyName'], Literal(cleanString(player['last_name']), datatype=XSD.string)))\n",
    "    #g.add((Movie, MO['releaseYear'], Literal(row['year'], datatype=XSD.gYear)))\n",
    "\n",
    "    teamId = \"team\"+str(row['teamIDs'])\n",
    "\n",
    "    Memb = URIRef(DCSSO[\"memb\"+tmId+\"s\"+str(y)+teamId])\n",
    "    g.add((Memb, RDF.type, DCSSO.SeasonalMembership))\n",
    "    g.add((Footballer, DCSSO['hasMembership'], Memb))\n",
    "    g.add((Memb, DCSSO['season'], Literal(\"2018\", datatype=XSD.int)))\n",
    "    g.add((Memb, DCSSO['forTeam'], URIRef(DCSSO[teamId])))\n",
    "    \n",
    "    \n",
    "    #statistics\n",
    "    g.add((Memb, DCSSO['games'], Literal(row['games'], datatype=XSD.int)))\n",
    "    #check: inconsistency for players switching team\n",
    "    g.add((Memb, DCSSO['minutes'], Literal(row['minutes'], datatype=XSD.int)))\n",
    "    g.add((Memb, DCSSO['goals'], Literal(row['goals'], datatype=XSD.int)))\n",
    "    #g.add((Memb, DCSSO['npg'], Literal(row['npg'], datatype=XSD.int)))\n",
    "    g.add((Memb, DCSSO['assists'], Literal(row['assists'], datatype=XSD.int)))\n",
    "    #g.add((Memb, DCSSO['keyPasses'], Literal(row['key_passes'], datatype=XSD.int)))\n",
    "    g.add((Memb, DCSSO['shots'], Literal(row['shots_total'], datatype=XSD.int)))\n",
    "    g.add((Memb, DCSSO['yellowCards'], Literal(row['cards_yellow'], datatype=XSD.int)))\n",
    "    g.add((Memb, DCSSO['redCards'], Literal(row['cards_red'], datatype=XSD.int)))\n",
    "    \n",
    "    g.add((Memb, DCSSO['xG'], Literal(row['xg'], datatype=XSD.double)))\n",
    "    g.add((Memb, DCSSO['xA'], Literal(row['xa'], datatype=XSD.double)))\n",
    "    g.add((Memb, DCSSO['npxG'], Literal(row['npxg'], datatype=XSD.double)))\n",
    "    g.add((Memb, DCSSO['xG90'], Literal(row['xg_per90'], datatype=XSD.double)))\n",
    "    g.add((Memb, DCSSO['xA90'], Literal(row['xa_per90'], datatype=XSD.double)))\n",
    "    g.add((Memb, DCSSO['npxG90'], Literal(row['npxg_per90'], datatype=XSD.double)))\n",
    "    #g.add((Memb, DCSSO['xGBuildup'], Literal(row['xGBuildup'], datatype=XSD.double)))\n",
    "    #g.add((Memb, DCSSO['xGChain'], Literal(row['xGChain'], datatype=XSD.double)))\n",
    "    \n",
    "    \n",
    "    if(player['position'] != \"Missing\"):\n",
    "        subPosition = player['sub_position'].replace(\" \", \"\").replace(\"-\", \"\")\n",
    "        if(player['position'] == \"Goalkeeper\" or player['position'] != \"Defender\"):\n",
    "            g.add((Footballer, DCSSO['position'], Literal(player['position'], datatype=XSD.string)))\n",
    "        elif(player['position'] == \"Midfield\"):\n",
    "            g.add((Footballer, DCSSO['position'], Literal(\"Midfielder\", datatype=XSD.string)))\n",
    "            subPosition += \"er\"\n",
    "        else:\n",
    "            g.add((Footballer, DCSSO['position'], Literal(\"Forward\", datatype=XSD.string)))\n",
    "\n",
    "        g.add((Footballer, DCSSO['subPosition'], Literal(subPosition, datatype=XSD.string)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "CPU times: total: 359 ms\n",
      "Wall time: 629 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'stats1819.rdf', 'w') as file:\n",
    "    file.write(g.serialize(format='xml'))\n",
    "    #.decode(\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sparqlwrapper\n",
    "# https://rdflib.github.io/sparqlwrapper/\n",
    "\n",
    "import sys\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "\n",
    "endpoint_url = \"https://query.wikidata.org/sparql\"\n",
    "\n",
    "query = \"\"\"SELECT ?oLabel\n",
    "WHERE\n",
    "{\n",
    "wd:Q192923 skos:altLabel ?o.\n",
    "FILTER(isLiteral(?o))\n",
    "SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\" }\n",
    "}\"\"\"\n",
    "\n",
    "\n",
    "def get_results(endpoint_url, query):\n",
    "    user_agent = \"WDQS-example Python/%s.%s\" % (sys.version_info[0], sys.version_info[1])\n",
    "    # TODO adjust user agent; see https://w.wiki/CX6\n",
    "    sparql = SPARQLWrapper(endpoint_url, agent=user_agent)\n",
    "    sparql.setQuery(query)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    return sparql.query().convert()\n",
    "\n",
    "\n",
    "results = get_results(endpoint_url, query)\n",
    "\n",
    "for result in results[\"results\"][\"bindings\"]:\n",
    "    print(result[\"oLabel\"][\"value\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Referential integrity\n",
    "Note that in RDF we are in an open world situation. We cannot guarantee the referential integrity between the entities. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Person\n",
    "\n",
    "Let us generate the RDF data relative to the movie workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "people = pd.read_csv(namesUrl, sep=',', index_col='imdb_name_id', keep_default_na=False, na_values=['_'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "People are modeled with the FOAF ontology. \n",
    "Refer to [FOAF Documentation](http://xmlns.com/foaf/spec/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new graph\n",
    "g = Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "#measure execution time\n",
    "\n",
    "#iterate over the person dataframe\n",
    "for index, row in people.iterrows():\n",
    "    # Create the node to add to the Graph\n",
    "    # the node has the namespace + the person id as URI\n",
    "    Person = URIRef(MO[index])\n",
    "    g.add((Person, RDF.type, FOAF.Person))\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Person, FOAF['name'], Literal(row['name'], datatype=XSD.string)))\n",
    "    if row['date_of_birth'] != '':\n",
    "        try:\n",
    "            datetime.datetime.strptime(str(row['date_of_birth']), '%Y-%m-%d')\n",
    "            g.add((Person, MO['birthday'], Literal(row['date_of_birth'], datatype=XSD.date)))\n",
    "        except ValueError:\n",
    "            # probably it's the year alone\n",
    "            # check length\n",
    "            if (len(row['date_of_birth'])==4):\n",
    "                #it is a year\n",
    "                g.add((Person, MO['birthday'], Literal(row['date_of_birth']+\"-01-01\", datatype=XSD.date)))\n",
    "    \n",
    "    if row['place_of_birth'] != '':\n",
    "        g.add((Person, MO['birthplace'], Literal(row['place_of_birth'], datatype=XSD.string)))\n",
    "    \n",
    "    # check if the death day is not empty--i.e., the person is still alive\n",
    "    if row['date_of_death'] != '':\n",
    "        try:\n",
    "            datetime.datetime.strptime(str(row['date_of_death']), '%Y-%m-%d')\n",
    "            g.add((Person, MO['deathDay'], Literal(row['date_of_death'], datatype=XSD.date)))\n",
    "        except ValueError:\n",
    "            # probably it's the year alone\n",
    "            # check length\n",
    "            if (len(row['date_of_death'])==4):\n",
    "                #it is a year\n",
    "                g.add((Person, MO['deathDay'], Literal(row['date_of_death']+\"-01-01\", datatype=XSD.date)))\n",
    "        \n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"mo\", MO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'names.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle').decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Person-Movie Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "join = pd.read_csv(joinTableUrl, sep=',', index_col='imdb_title_id', keep_default_na=False, na_values=['_'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new graph\n",
    "g = Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regular expressions\n",
    "import re\n",
    "actor = re.compile('act*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "#measure execution time\n",
    "\n",
    "#iterate over the join table dataframe\n",
    "for index, row in join.iterrows():\n",
    "    # Create the node about the movie\n",
    "    # note that we do not add this resource to the database (created before)\n",
    "    Movie = URIRef(MO[index])\n",
    "    \n",
    "    # Create the node about the person\n",
    "    # note that we do not add this resource to the database (created before)\n",
    "    Person = URIRef(MO[row['imdb_name_id']])\n",
    "    # get the role of the person\n",
    "    role = row['category']\n",
    "    \n",
    "    # we have an actor or actress\n",
    "    if actor.match(role): \n",
    "        g.add((Person, MO['acted'], Movie))\n",
    "    elif (role=='director'):\n",
    "        g.add((Person, MO['directed'], Movie))\n",
    "    else:\n",
    "        # note that, with the defined ontology, we cannot caracterize the specific role of this person in the movie. \n",
    "        # why?\n",
    "        g.add((Person, MO['worked'], Movie))\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"mo\", MO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'name_movie_join.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle').decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Awards - Oscars data\n",
    "Note that if we do not check the referential integrity then we could produce ghost triple movie-nominee-oscar where the movie is not in the RDF graph.\n",
    "\n",
    "On the other hand, we can check if an actor or a movie exists by using the DataFrame in Python. Note that this is an external check and not a constraints met by the RDF DB.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "oscars = pd.read_csv(oscarsUrl, sep=',', keep_default_na=False, na_values=['_'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from num2words import num2words\n",
    "import string\n",
    "import re\n",
    "#create a new graph\n",
    "g = Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#iterate over the join table dataframe\n",
    "for index,row in oscars.iterrows():\n",
    "    #create the oscar with a custom id \n",
    "    cat = re.sub(r'[^\\w\\s]','',row['category'])\n",
    "    Oscar = URIRef(MO['oscar_'+cat.replace(\" \", \"\").lower()+'_'+ str(num2words(row['ceremony'], to='ordinal'))])\n",
    "    \n",
    "    # check if there already is at least a triple about this oscar\n",
    "    if not (Oscar, None, None) in g:    \n",
    "        # check if the oscar is already in the graph\n",
    "        g.add((Oscar, RDF.type, MO.Oscar))\n",
    "        g.add((Oscar, MO['category'], Literal(row['category'].lower(), datatype=XSD.string)))\n",
    "        g.add((Oscar, MO['year'], Literal(row['year_ceremony'], datatype=XSD.gYear)))\n",
    "    \n",
    "    # check if there is a name matching the people, meaning that the oscar can be associated to a person\n",
    "    if (people[\"name\"] == row[\"name\"]).any() == True :\n",
    "        #there is a person with this name\n",
    "        # Create the node about the person\n",
    "        # note that we do not add this resource to the database (created before)\n",
    "        Person = URIRef(MO[people[people[\"name\"]==row[\"name\"]].index[0]])\n",
    "        if row['winner']:\n",
    "            g.add((Person, MO['winner'], Oscar))\n",
    "        else:\n",
    "            g.add((Person, MO['nominated'], Oscar))\n",
    "    \n",
    "    # an oscar for a person is also to be considered an oscar for the movie\n",
    "    # check if the movie is in our DB\n",
    "    if (movies[\"original_title\"] == row[\"film\"]).any():\n",
    "        # there is a movie with this title\n",
    "        Movie = URIRef(MO[movies[movies[\"original_title\"]==row[\"film\"]].index[0]])\n",
    "        if row['winner']:\n",
    "            g.add((Movie, MO['winner'], Oscar))\n",
    "        else:\n",
    "            g.add((Movie, MO['nominated'], Oscar))\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"mo\", MO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'oscars.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle').decode(\"utf-8\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
