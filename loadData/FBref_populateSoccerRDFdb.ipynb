{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate an RDF database\n",
    "\n",
    "This notebook reports the main steps to download CSV files, process them and create an RDF dataset from them accordingly to an ontology. \n",
    "\n",
    "To measure execution time in Jupyter notebooks: <code>pip install ipython-autotime</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# parameters and URLs\n",
    "path = str(Path(os.path.abspath(os.getcwd())).parent.absolute())\n",
    "stats1819Url = path + '/inDepthSoccerStats/transfermarkt_fbref_201819.csv'\n",
    "playersUrl = path + '/inDepthSoccerStats/players.csv'\n",
    "appUrl = path + '/inDepthSoccerStats/appearances.csv'\n",
    "\n",
    "# country codes\n",
    "# countriesURL = path + '/data/countryCodes/wikipedia-iso-country-codes.csv'\n",
    "\n",
    "# saving folder\n",
    "savePath =  path + '/rdf/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soccer Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "Index: 2100 entries, 207 to 2537\n",
      "Series name: Attendance\n",
      "Non-Null Count  Dtype \n",
      "--------------  ----- \n",
      "2100 non-null   string\n",
      "dtypes: string(1)\n",
      "memory usage: 32.8 KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2100 entries, 207 to 2537\n",
      "Columns: 399 entries, player to Season\n",
      "dtypes: float64(239), int64(151), object(8), string(1)\n",
      "memory usage: 6.4+ MB\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV files in memory\n",
    "stats1819 = pd.read_csv(stats1819Url, sep=';', index_col='Column1', dtype={\"Attendance\": \"string\"})\n",
    "players = pd.read_csv(playersUrl, sep=',', index_col='player_id')\n",
    "app = pd.read_csv(appUrl, sep=',', index_col='appearance_id')\n",
    "\n",
    "stats1819['Attendance'] = stats1819['Attendance'].str.replace(\",\",\"\")\n",
    "stats1819.astype({'Attendance': 'int32'}).dtypes\n",
    "stats1819['Attendance'].info()\n",
    "\n",
    "# cast year to int. If type(year) = str --> Literal= year-01-01\n",
    "#stats1920.astype({'year': 'int32'}).dtypes\n",
    "#load the country codes\n",
    "# we need to convert NaN values to something else otherwise NA strings are converted to NaN -> problem with Namibia\n",
    "#countries = pd.read_csv(countriesURL, sep=',', index_col='Name', keep_default_na=False, na_values=['_'])\n",
    "\n",
    "stats1819.info()\n",
    "#players.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to install <code>RDFLib</code>\n",
    "\n",
    "<code>pip3 install rdflib </code> [Documentation](https://rdflib.readthedocs.io/en/stable/gettingstarted.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the required libraries\n",
    "#!pip install rdflib\n",
    "from rdflib import Graph, Literal, RDF, URIRef, Namespace, term\n",
    "# rdflib knows about some namespaces, like FOAF\n",
    "from rdflib.namespace import FOAF, XSD\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the country and the movie ontology namespaces not known by RDFlib\n",
    "#CNS = Namespace(\"http://eulersharp.sourceforge.net/2003/03swap/countries#\")\n",
    "DCSSO = Namespace(\"http://www.dei.unipd.it/db2/dcsso#\")\n",
    "\n",
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "#g.bind(\"countries\", CNS)\n",
    "g.bind(\"dcsso\", DCSSO)\n",
    "\n",
    "#term.bind(\n",
    "#    XSD.double,\n",
    "#    float,\n",
    "#   constructor=float,\n",
    "#    lexicalizer=lambda val: f\"{val:f}\",\n",
    "#    datatype_specific=True\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "#!pip install unidecode\n",
    "#!pip install googlesearch-python\n",
    "from unidecode import unidecode\n",
    "from itertools import permutations\n",
    "from difflib import SequenceMatcher\n",
    "from googlesearch import search\n",
    "from re import sub\n",
    "\n",
    "#string parsing\n",
    "\n",
    "def nameToRef(name):\n",
    "    return unidecode(name.replace(\" \",\"\"))\n",
    "\n",
    "def hyphenize(s):\n",
    "    return unidecode(s.lower().replace(\" \",\"-\").replace(\"&#039;\",\"'\"))\n",
    "\n",
    "def cleanChars(item):\n",
    "    return item.str.replace(\"ć\", \"c\").str.replace(\"ğ\",\"g\").str.replace(\"İ\",\"i\").str.replace(\"-scaron-\",\"s\")\n",
    "\n",
    "#google lookup for wikidata item\n",
    "def checkEqualsWD(c1, c2):\n",
    "    URL1 = next(search(c1.replace(\"-\", \" \") + \" football wikidata\", num_results=1))\n",
    "    URL2 = next(search(c2.replace(\"-\", \" \") + \" football wikidata\", num_results=1))\n",
    "    if(URL1 == URL2):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "#permutations strategy\n",
    "\n",
    "def genSeqByLength(perm, length):\n",
    "    newName = \"\"\n",
    "    for j in range(0, length - 1):\n",
    "        newName = newName + perm[j] + \"-\"\n",
    "    newName = newName + perm[length - 1]\n",
    "    return newName\n",
    "\n",
    "def match_seq(splitS1, list):\n",
    "    resultList = []\n",
    "    #iterate on all possible permutations\n",
    "    length = -1\n",
    "    if(len(list) == 1):\n",
    "        length = len(list[0].split(\"-\"))\n",
    "        uniqueItem = list[0]\n",
    "    for perm in permutations(splitS1):\n",
    "        if(length != -1):\n",
    "            newName = genSeqByLength(perm, length)\n",
    "            if(uniqueItem == newName):\n",
    "                return [uniqueItem]\n",
    "        #consider all lengths from 2 to n\n",
    "        else:\n",
    "            for i in range(2, len(splitS1) + 1):\n",
    "                newName = genSeqByLength(perm, i)\n",
    "                for item in list:\n",
    "                    if(item == newName):\n",
    "                        resultList.append(newName)\n",
    "    return resultList\n",
    "\n",
    "#multiple matches resolution\n",
    "\n",
    "def getAppsByID(ID, apps):\n",
    "    return np.size(apps[apps['player_id'] == ID], 0);\n",
    "\n",
    "#I: number of games in the season, candidate players, list describing single appearances\n",
    "#O: player from players file, or empty Series\n",
    "def solve_with_apps(statsGames, somePlayers, appsCol):\n",
    "    for ind in somePlayers.index:\n",
    "        if(statsGames == getAppsByID(ind, appsCol)):\n",
    "            player = somePlayers[somePlayers.index == ind].iloc[0]\n",
    "            return player\n",
    "    return pd.Series([])\n",
    "\n",
    "def solve_with_apps_approx(statsGames, somePlayers, appsCol):\n",
    "    minDiff = 50\n",
    "    minInd = 0\n",
    "    for ind in somePlayers.index:\n",
    "        if(abs(statsGames - getAppsByID(ind, appsCol)) < minDiff):\n",
    "            minDiff = abs(statsGames - getAppsByID(ind, appsCol))\n",
    "            minInd = ind\n",
    "    if(minDiff <= 5):\n",
    "        player = somePlayers[somePlayers.index == minInd].iloc[0] \n",
    "        return player\n",
    "    return pd.Series([])\n",
    "\n",
    "def solve_with_height(statsHeight, somePlayers):\n",
    "    minDiff = 50\n",
    "    player = pd.Series([])\n",
    "    for ind, row in somePlayers.iterrows():\n",
    "        diff = abs(statsHeight - row['height_in_cm'])\n",
    "        if(diff <= 5 and diff < minDiff):\n",
    "            minDiff = diff\n",
    "            player = row\n",
    "    return player\n",
    "\n",
    "def solve_with_apps_birthyear(somePlayers, statsGames, statsYear, appsCol):\n",
    "    minDiff = 50\n",
    "    player = pd.Series([])\n",
    "    somePlayers = somePlayers[(statsYear - somePlayers['date_of_birth'].str[:4:].apply(int)) == 0]\n",
    "    if(np.size(somePlayers, 0) == 1):\n",
    "        return somePlayers.iloc[0]\n",
    "    elif(np.size(somePlayers, 0) == 0):\n",
    "        print(\"No match on birthdate \"+str(statsYear))\n",
    "        return player\n",
    "    for ind, row in somePlayers.iterrows():\n",
    "        diff = abs(statsGames - getAppsByID(ind, appsCol))\n",
    "        if(diff < minDiff):\n",
    "            minDiff = diff\n",
    "            player = row\n",
    "    if(minDiff > 3 and minDiff < 50):\n",
    "        print(\"minDifff: \"+str(minDiff))\n",
    "        player = pd.Series([])\n",
    "    return player\n",
    "\n",
    "def solve_with_birthyear(somePlayers, statsYear):\n",
    "    somePlayers = somePlayers[statsYear == somePlayers['date_of_birth'].str[:4:].apply(int)]\n",
    "    if(len(somePlayers) == 1):\n",
    "        return somePlayers.iloc[0]\n",
    "    else:\n",
    "        return pd.Series([])\n",
    "\n",
    "def solve_with_maxsim_by_birthyear(players, statsName, statsYear, statsGames, appsCol):\n",
    "    maxSim = -1\n",
    "    player = pd.Series([])\n",
    "    yearPlayers = players[statsYear == players['date_of_birth'].replace(\"\",\"0000\").str[:4:].apply(float).fillna(0).apply(int)]\n",
    "    for ind, row in yearPlayers.iterrows():\n",
    "        if(abs(statsGames - getAppsByID(ind, appsCol)) < 5):\n",
    "            sm = SequenceMatcher(None, statsName, row['player_code'])\n",
    "            curr = sm.ratio()\n",
    "            if(curr > maxSim):\n",
    "                maxSim = curr\n",
    "                player = row\n",
    "    return player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BY: Manu García --> manu-garcia\n",
      "BY: Ximo Navarro --> ximo-navarro\n",
      "BY: Fernando Pacheco --> fernando-pacheco\n",
      "PERM1: Juan Ferney Otero --> juan-otero\n",
      "BY: Aaron Ramsey --> aaron-ramsey\n",
      "BY: Raúl García --> raul-garcia\n",
      "GOOGLE: Francisco Montero --> javi-montero\n",
      "BY: Rodri --> rodri\n",
      "BY: Vitolo --> vitolo\n",
      "BY: Rafinha --> rafinha\n",
      "BY: Luis Suárez --> luis-suarez\n",
      "BY: Javi Martínez --> javi-martinez\n",
      "BY: Rafinha --> rafinha\n",
      "PERM1: Juan Manuel Valencia --> juan-valencia\n",
      "BY: Otávio --> otavio\n",
      "BY: Pablo --> pablo\n",
      "BY: Adam Smith --> adam-smith\n",
      "BY: Callum Wilson --> callum-wilson\n",
      "BY: Bernardo --> bernardo\n",
      "GOOGLE: Bruno --> bruno-saltor\n",
      "BY: João Pedro --> joao-pedro\n",
      "BY: Francesco Verde --> francesco-verde\n",
      "PERM2: Bobby Reid --> bobby-de-cordova-reid\n",
      "GOOGLE: Danny Ward --> danny-ward\n",
      "BY: Sergio Álvarez --> sergio-alvarez\n",
      "BY: Jorginho --> jorginho\n",
      "GOOGLE: Pedro --> pedro\n",
      "BY: Willian --> willian\n",
      "BY: Sergio Álvarez --> sergio-alvarez\n",
      "BY: Charles --> charles\n",
      "BY: Allan --> allan\n",
      "BY: Sergio García --> sergio-garcia\n",
      "BY: Diego López --> diego-lopez\n",
      "BY: Javi López --> javi-lopez\n",
      "BY: Naldo --> naldo\n",
      "BY: Tom Davies --> tom-davies\n",
      "BY: André Gomes --> andre-gomes\n",
      "BY: Vitor Hugo --> vitor-hugo\n",
      "BY: Ibrahima Cissé --> ibrahima-cisse\n",
      "BY: Pedro Pereira --> pedro-pereira\n",
      "BY: Rômulo --> romulo\n",
      "BY: Sandro --> sandro\n",
      "GOOGLE: Vitorino Antunes --> antunes\n",
      "GOOGLE: Aday --> aday-benitez\n",
      "GOOGLE: Raúl García --> raul-carnero\n",
      "GOOGLE: Felipe --> felipe-trevizan\n",
      "BY: Tommy Smith --> tommy-smith\n",
      "BY: Danny Williams --> danny-williams\n",
      "BY: Luisinho --> luisinho\n",
      "GOOGLE: João Mário --> joao-mario\n",
      "BY: Luis Alberto --> luis-alberto\n",
      "GOOGLE: Luiz Felipe --> luiz-felipe\n",
      "BY: Rômulo --> romulo\n",
      "BY: Wallace --> wallace\n",
      "GOOGLE: Raúl García --> raul-carnero\n",
      "BY: Diego Reyes --> diego-reyes\n",
      "BY: Aitor Fernández --> aitor-fernandez\n",
      "GOOGLE: Toño --> tono-garcia\n",
      "BY: Paulinho --> paulinho\n",
      "BY: Wendell --> wendell\n",
      "BY: Fabinho --> fabinho\n",
      "BY: Alberto Moreno --> alberto-moreno\n",
      "BY: Marcelo --> marcelo\n",
      "BY: Rafael --> rafael\n",
      "BY: Aarón Martín --> aaron-martin\n",
      "BY: Danilo --> danilo\n",
      "BY: Ederson --> ederson\n",
      "GOOGLE: Fernandinho --> fernandinho\n",
      "GOOGLE: David Silva --> david-silva\n",
      "BY: Luiz Gustavo --> luiz-gustavo\n",
      "BY: Jonas Hofmann --> jonas-hofmann\n",
      "BY: Naldo --> naldo\n",
      "BY: Pelé --> pele\n",
      "GOOGLE: Adama Traoré --> adama-traore\n",
      "BY: Pedro Mendes --> pedro-mendes\n",
      "BY: Matheus Pereira --> matheus-pereira\n",
      "GOOGLE: Gabriel Boschilia --> boschilia\n",
      "GOOGLE: Fábio --> fabio\n",
      "BY: Abdoulaye Touré --> abdoulaye-toure\n",
      "BY: Allan --> allan\n",
      "BY: Marquinhos --> marquinhos\n",
      "BY: Thiago Silva --> thiago-silva\n",
      "BY: Bruno Alves --> bruno-alves\n",
      "BY: Álvaro García --> alvaro-garcia\n",
      "BY: Javi Guerra --> javi-guerra\n",
      "BY: Marcelo --> marcelo\n",
      "BY: Diego Llorente --> diego-llorente\n",
      "BY: Abdoulaye Diallo --> abdoulaye-diallo\n",
      "BY: Jakob Johansson --> jakob-johansson\n",
      "BY: Gabriel Silva --> gabriel-silva\n",
      "BY: Marlon --> marlon\n",
      "BY: Rogério --> rogerio\n",
      "BY: Naldo --> naldo\n",
      "BY: André Silva --> andre-silva\n",
      "BY: Pablo Martinez --> pablo-martinez\n",
      "BY: Ben Davies --> ben-davies\n",
      "BY: Manu García --> manu-garcia\n",
      "BY: Sandro --> sandro\n",
      "BY: Neto --> neto\n",
      "BY: Borja Fernández --> borja-fernandez\n",
      "BY: Joaquín Fernández --> joaquin-fernandez\n",
      "BY: Álvaro González --> alvaro-gonzalez\n",
      "BY: William --> william\n",
      "BY: João Moutinho --> joao-moutinho\n",
      "BY: Adama Traoré --> adama-traore\n",
      "CPU times: total: 17.5 s\n",
      "Wall time: 46.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#select only months and years from appearance dates\n",
    "appMonth = app['date'].str.split(\"/\").str[1]\n",
    "appYear = app['date'].str.split(\"/\").str[2]\n",
    "#select appearances from 18/19 season\n",
    "is1819 = ((appYear == \"2018\") & (appMonth >= \"08\")) | ((appYear == \"2019\") & (appMonth <= \"06\"))\n",
    "app1819 = app[is1819]\n",
    "playerCodes = cleanChars(players['player_code'])\n",
    "\n",
    "diff = 0\n",
    "#iterate on stats file\n",
    "exact_matches = no_matches = resolved_google = resolved_permS = resolved_permP = resolved_by = unable_to_solve = 0\n",
    "for index, row in stats1819.iterrows():\n",
    "    mode = \"NONE\"\n",
    "    player = pd.Series([])\n",
    "    statsName = hyphenize(row['player']).replace(\"'\",\"\")\n",
    "    \n",
    "    matchedPlayers = players[playerCodes == statsName]\n",
    "    \n",
    "    #multiple rows with same name in stats mapped to a single player are ok (he has changed team during the season)\n",
    "    if(np.size(matchedPlayers, 0) > 0):\n",
    "        mode = \"MATCH\"\n",
    "       \n",
    "    if(mode == \"NONE\"):\n",
    "        #split name in stats and use permutations strategy\n",
    "        splitStatsName = statsName.split(\"-\")\n",
    "        if(len(splitStatsName) >= 2):\n",
    "            matchedCodes = match_seq(splitStatsName, playerCodes)\n",
    "            if(len(matchedCodes) > 0):\n",
    "                matchedPlayers = players[playerCodes.isin(matchedCodes)]\n",
    "                mode = \"PERM1\"\n",
    "\n",
    "    if(mode == \"NONE\"):\n",
    "        maxSim = 0\n",
    "        maxC = \"\"\n",
    "        for c in playerCodes:\n",
    "            sm = SequenceMatcher(None, statsName, c)\n",
    "            #do not proceed if the upper bound is too small\n",
    "            if(sm.real_quick_ratio() >= 0.5):\n",
    "                #remember: similarity is not commutative\n",
    "                #if sim is big enough, try permutation strategy with name from players file\n",
    "                if(sm.ratio() >= 0.6):\n",
    "                    splitC = c.split(\"-\")\n",
    "                    if(len(splitC) >= len(splitStatsName) and len(splitStatsName) >= 2):\n",
    "                        matchedCodes = match_seq(splitC, [statsName])\n",
    "                        if(len(matchedCodes) > 0):\n",
    "                            newMatchedPlayers = players[players['player_code'] == c]\n",
    "                            matchedPlayers = pd.concat([matchedPlayers, newMatchedPlayers])\n",
    "\n",
    "        if(np.size(matchedPlayers, 0) > 0):\n",
    "            mode = \"PERM2\"\n",
    "\n",
    "    #managing results of any method\n",
    "    matches = np.size(matchedPlayers, 0)\n",
    "    if(matches == 1):\n",
    "        player = matchedPlayers.iloc[0]\n",
    "    if(matches > 1):\n",
    "        #player = solve_with_apps_approx(row['games'], matchedPlayers, app1819)\n",
    "        player = solve_with_birthyear(matchedPlayers, row['birth_year'])\n",
    "        if(len(player) == 0):\n",
    "            mode = \"NONE\"\n",
    "        else:\n",
    "            mode = \"BY\"\n",
    "    if(mode == \"NONE\"):\n",
    "        URL = next(search(row['player']+\" \"+row['squad']+\" \"+str(row['birth_year'])+\" transfermarkt\", num_results=1))\n",
    "        trID = sub(\"[^0-9]\",\"\",URL)\n",
    "        if(trID != \"\"):\n",
    "            urlPlayers = players[players.index == int(trID)]\n",
    "            if(len(urlPlayers) != 0):\n",
    "                player = urlPlayers.iloc[0]\n",
    "                mode = \"GOOGLE\"\n",
    "    \n",
    "    if(mode == \"NONE\"):\n",
    "        no_matches += 1\n",
    "    elif(mode == \"PERM1\"):\n",
    "        resolved_permS += 1\n",
    "    elif(mode == \"PERM2\"):\n",
    "        resolved_permP += 1\n",
    "    elif(mode == \"GOOGLE\"):\n",
    "        resolved_google += 1\n",
    "    elif(mode == \"MATCH\"):\n",
    "        exact_matches += 1\n",
    "    elif(mode == \"BY\"):\n",
    "        resolved_by += 1\n",
    "\n",
    "    if(mode == \"NONE\"):\n",
    "        print(\"NONE: \"+row['player']+\", matches: \"+str(matches))\n",
    "    elif(mode != \"MATCH\"):\n",
    "        print(mode+\": \"+row['player']+\" --> \"+player['player_code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   --- STATISTICS ---\n",
      "Total matches:                             2100 -- percentage: 100.00%\n",
      "  ---> exact matches:                      1995 -- percentage: 95.00%\n",
      "  ---> resolved permutating statsName:        2 -- percentage: 0.10%\n",
      "  ---> resolved permutating player code:      1 -- percentage: 0.05%\n",
      "  ---> resolved with google:                 17 -- percentage: 0.81%\n",
      "  ---> resolved with birthyear:              85 -- percentage: 4.05%\n",
      "No matches:                                   0 -- percentage: 0.00%\n",
      "  ---> unable to resolve between matches:     0 -- percentage: 0.00%\n",
      "  ---> zero matches found:                    0 -- percentage: 0.00%\n"
     ]
    }
   ],
   "source": [
    "#print statistics\n",
    "statsRows = np.size(stats1819, 0);\n",
    "print(\"   --- STATISTICS ---\")\n",
    "tot_matches = exact_matches + + resolved_google + resolved_permS + resolved_permP + resolved_by\n",
    "print(\"Total matches:                            \"+\"{:5d}\".format(tot_matches)+\" -- percentage: \" + \"{:.2f}%\".format(tot_matches*100/statsRows))\n",
    "print(\"  ---> exact matches:                     \"+\"{:5d}\".format(exact_matches)+\" -- percentage: \" + \"{:.2f}%\".format(exact_matches*100/statsRows)) \n",
    "print(\"  ---> resolved permutating statsName:    \"+\"{:5d}\".format(resolved_permS)+\" -- percentage: \" + \"{:.2f}%\".format(resolved_permS*100/statsRows))\n",
    "print(\"  ---> resolved permutating player code:  \"+\"{:5d}\".format(resolved_permP)+\" -- percentage: \" + \"{:.2f}%\".format(resolved_permP*100/statsRows))\n",
    "print(\"  ---> resolved with google:              \"+\"{:5d}\".format(resolved_google)+\" -- percentage: \" + \"{:.2f}%\".format(resolved_google*100/statsRows))\n",
    "print(\"  ---> resolved with birthyear:           \"+\"{:5d}\".format(resolved_by)+\" -- percentage: \" + \"{:.2f}%\".format(resolved_by*100/statsRows))\n",
    "print(\"No matches:                               \"+\"{:5d}\".format(no_matches)+\" -- percentage: \" + \"{:.2f}%\".format(no_matches*100/statsRows))\n",
    "print(\"  ---> unable to resolve between matches: \"+\"{:5d}\".format(unable_to_solve)+\" -- percentage: \" + \"{:.2f}%\".format(unable_to_solve*100/statsRows))\n",
    "print(\"  ---> zero matches found:                \"+\"{:5d}\".format(no_matches - unable_to_solve)+\" -- percentage: \" + \"{:.2f}%\".format((no_matches - unable_to_solve)*100/statsRows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "# measure execution time\n",
    "\n",
    "# just a variable to diversify between footballers with the same name\n",
    "a = 0\n",
    "# iterate over the movies dataframe\n",
    "for index, row in stats1920.iterrows():\n",
    "    # Create the node to add to the Graph\n",
    "    # the node has the namespace + the footballer name as URI\n",
    "    name = unidecode(index.replace(\"&#039;\",\"'\"))\n",
    "    ref = name.replace(\" \",\"\")\n",
    "    # add a unique code if there are players with the same name\n",
    "    if(np.size(stats1920[stats1920.index == name], 0) > 1):\n",
    "        #print(name + \" : \" + str(np.size(stats1920[stats1920.index == name], 0)))\n",
    "        a = a + 1\n",
    "        ref = ref + str(a)\n",
    "    \n",
    "    Footballer = URIRef(DCSSO[ref])\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Footballer, RDF.type, DCSSO.Footballer))\n",
    "    g.add((Footballer, DCSSO['name'], Literal(name, datatype=XSD.string)))\n",
    "    #g.add((Movie, MO['releaseYear'], Literal(row['year'], datatype=XSD.gYear)))\n",
    "    \n",
    "    leagueName = unidecode(row['league'])\n",
    "    League = URIRef(DCSSO[leagueName.replace(\" \",\"\")])\n",
    "    g.add((League, DCSSO['name'], Literal(row['league'], datatype=XSD.string)))\n",
    "    \n",
    "    teams = unidecode(row['teams_played_for'].replace(\"&#039;\",\"'\").strip())\n",
    "    \n",
    "    membRef = ref+\"_1920_\"+teams.replace(\",\",\"_\").replace(\" \",\"\")\n",
    "    Memb = URIRef(DCSSO[membRef])\n",
    "    g.add((Footballer, DCSSO['hasMembership'], Memb))\n",
    "    \n",
    "    for teamName in teams.split(','):\n",
    "        teamRef = teamName.replace(\" \",\"\")\n",
    "        Team = URIRef(DCSSO[teamRef])\n",
    "        g.add((Team, DCSSO['name'], Literal(teamName, datatype=XSD.string)))\n",
    "        g.add((Team, DCSSO['participatesIn'], League))\n",
    "        g.add((Memb, DCSSO['forTeam'], Team))\n",
    "    \n",
    "    #statistics\n",
    "    g.add((Memb, DCSSO['games'], Literal(row['games'], datatype=XSD.int)))\n",
    "    g.add((Memb, DCSSO['minutes'], Literal(row['minutes_played'], datatype=XSD.int)))\n",
    "    g.add((Memb, DCSSO['goals'], Literal(row['goals'], datatype=XSD.int)))\n",
    "    g.add((Memb, DCSSO['npg'], Literal(row['npg'], datatype=XSD.int)))\n",
    "    g.add((Memb, DCSSO['assists'], Literal(row['assists'], datatype=XSD.int)))\n",
    "    g.add((Memb, DCSSO['keyPasses'], Literal(row['key_passes'], datatype=XSD.int)))\n",
    "    g.add((Memb, DCSSO['shots'], Literal(row['shots'], datatype=XSD.int)))\n",
    "    g.add((Memb, DCSSO['yellowCards'], Literal(row['yellow_cards'], datatype=XSD.int)))\n",
    "    g.add((Memb, DCSSO['redCards'], Literal(row['red_cards'], datatype=XSD.int)))\n",
    "    \n",
    "    g.add((Memb, DCSSO['xG'], Literal(row['xG'], datatype=XSD.double)))\n",
    "    g.add((Memb, DCSSO['xA'], Literal(row['xA'], datatype=XSD.double)))\n",
    "    g.add((Memb, DCSSO['npxG'], Literal(row['npxG'], datatype=XSD.double)))\n",
    "    g.add((Memb, DCSSO['xG90'], Literal(row['xG90'], datatype=XSD.double)))\n",
    "    g.add((Memb, DCSSO['xA90'], Literal(row['xA90'], datatype=XSD.double)))\n",
    "    g.add((Memb, DCSSO['npxG90'], Literal(row['npxG90'], datatype=XSD.double)))\n",
    "    g.add((Memb, DCSSO['xGBuildup'], Literal(row['xGBuildup'], datatype=XSD.double)))\n",
    "    g.add((Memb, DCSSO['xGChain'], Literal(row['xGChain'], datatype=XSD.double)))\n",
    "    \n",
    "        \n",
    "    #there can be more than one role per footballer\n",
    "    for rN in row['position'].split(' '):\n",
    "        g.add((Footballer, DCSSO['role'], Literal(rN.strip(), datatype=XSD.string)))    \n",
    "\n",
    "#print(\"Duplicates: \"+str(a/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'stats1920.rdf', 'w') as file:\n",
    "    file.write(g.serialize(format='xml'))\n",
    "    #.decode(\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sokratis Papastathopoulos\n",
      "Sokratis Papastathopoulos\n",
      "Papastathopoulos\n",
      "Socratis Papastathopoulos\n",
      "Socrátis Papastathópoulos\n",
      "Sokratis Papastathopoulos\n",
      "Sokratis\n",
      "Papastathopoulos\n",
      "Papastatopuls\n",
      "Sokratis Papastathopoulos\n",
      "सोक्रतीस पपस्तथोपोलोस\n",
      "Sokrates Papastathopoulos\n",
      "Sokratis Papastathopoulos\n",
      "Сокрастис Папастатопулос\n",
      "Сократис Папастатопулос\n",
      "Sokratis Papastathopulos\n",
      "柏柏斯達夫普路斯\n",
      "索克拉蒂斯·帕帕斯塔索普洛斯\n",
      "Sokratis\n"
     ]
    }
   ],
   "source": [
    "# !pip install sparqlwrapper\n",
    "# https://rdflib.github.io/sparqlwrapper/\n",
    "\n",
    "import sys\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "\n",
    "endpoint_url = \"https://query.wikidata.org/sparql\"\n",
    "\n",
    "query = \"\"\"SELECT ?oLabel\n",
    "WHERE\n",
    "{\n",
    "wd:Q192923 skos:altLabel ?o.\n",
    "FILTER(isLiteral(?o))\n",
    "SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\" }\n",
    "}\"\"\"\n",
    "\n",
    "\n",
    "def get_results(endpoint_url, query):\n",
    "    user_agent = \"WDQS-example Python/%s.%s\" % (sys.version_info[0], sys.version_info[1])\n",
    "    # TODO adjust user agent; see https://w.wiki/CX6\n",
    "    sparql = SPARQLWrapper(endpoint_url, agent=user_agent)\n",
    "    sparql.setQuery(query)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    return sparql.query().convert()\n",
    "\n",
    "\n",
    "results = get_results(endpoint_url, query)\n",
    "\n",
    "for result in results[\"results\"][\"bindings\"]:\n",
    "    print(result[\"oLabel\"][\"value\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Referential integrity\n",
    "Note that in RDF we are in an open world situation. We cannot guarantee the referential integrity between the entities. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Person\n",
    "\n",
    "Let us generate the RDF data relative to the movie workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "people = pd.read_csv(namesUrl, sep=',', index_col='imdb_name_id', keep_default_na=False, na_values=['_'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "People are modeled with the FOAF ontology. \n",
    "Refer to [FOAF Documentation](http://xmlns.com/foaf/spec/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new graph\n",
    "g = Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "#measure execution time\n",
    "\n",
    "#iterate over the person dataframe\n",
    "for index, row in people.iterrows():\n",
    "    # Create the node to add to the Graph\n",
    "    # the node has the namespace + the person id as URI\n",
    "    Person = URIRef(MO[index])\n",
    "    g.add((Person, RDF.type, FOAF.Person))\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Person, FOAF['name'], Literal(row['name'], datatype=XSD.string)))\n",
    "    if row['date_of_birth'] != '':\n",
    "        try:\n",
    "            datetime.datetime.strptime(str(row['date_of_birth']), '%Y-%m-%d')\n",
    "            g.add((Person, MO['birthday'], Literal(row['date_of_birth'], datatype=XSD.date)))\n",
    "        except ValueError:\n",
    "            # probably it's the year alone\n",
    "            # check length\n",
    "            if (len(row['date_of_birth'])==4):\n",
    "                #it is a year\n",
    "                g.add((Person, MO['birthday'], Literal(row['date_of_birth']+\"-01-01\", datatype=XSD.date)))\n",
    "    \n",
    "    if row['place_of_birth'] != '':\n",
    "        g.add((Person, MO['birthplace'], Literal(row['place_of_birth'], datatype=XSD.string)))\n",
    "    \n",
    "    # check if the death day is not empty--i.e., the person is still alive\n",
    "    if row['date_of_death'] != '':\n",
    "        try:\n",
    "            datetime.datetime.strptime(str(row['date_of_death']), '%Y-%m-%d')\n",
    "            g.add((Person, MO['deathDay'], Literal(row['date_of_death'], datatype=XSD.date)))\n",
    "        except ValueError:\n",
    "            # probably it's the year alone\n",
    "            # check length\n",
    "            if (len(row['date_of_death'])==4):\n",
    "                #it is a year\n",
    "                g.add((Person, MO['deathDay'], Literal(row['date_of_death']+\"-01-01\", datatype=XSD.date)))\n",
    "        \n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"mo\", MO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'names.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle').decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Person-Movie Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "join = pd.read_csv(joinTableUrl, sep=',', index_col='imdb_title_id', keep_default_na=False, na_values=['_'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new graph\n",
    "g = Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regular expressions\n",
    "import re\n",
    "actor = re.compile('act*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "#measure execution time\n",
    "\n",
    "#iterate over the join table dataframe\n",
    "for index, row in join.iterrows():\n",
    "    # Create the node about the movie\n",
    "    # note that we do not add this resource to the database (created before)\n",
    "    Movie = URIRef(MO[index])\n",
    "    \n",
    "    # Create the node about the person\n",
    "    # note that we do not add this resource to the database (created before)\n",
    "    Person = URIRef(MO[row['imdb_name_id']])\n",
    "    # get the role of the person\n",
    "    role = row['category']\n",
    "    \n",
    "    # we have an actor or actress\n",
    "    if actor.match(role): \n",
    "        g.add((Person, MO['acted'], Movie))\n",
    "    elif (role=='director'):\n",
    "        g.add((Person, MO['directed'], Movie))\n",
    "    else:\n",
    "        # note that, with the defined ontology, we cannot caracterize the specific role of this person in the movie. \n",
    "        # why?\n",
    "        g.add((Person, MO['worked'], Movie))\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"mo\", MO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'name_movie_join.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle').decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Awards - Oscars data\n",
    "Note that if we do not check the referential integrity then we could produce ghost triple movie-nominee-oscar where the movie is not in the RDF graph.\n",
    "\n",
    "On the other hand, we can check if an actor or a movie exists by using the DataFrame in Python. Note that this is an external check and not a constraints met by the RDF DB.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "oscars = pd.read_csv(oscarsUrl, sep=',', keep_default_na=False, na_values=['_'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from num2words import num2words\n",
    "import string\n",
    "import re\n",
    "#create a new graph\n",
    "g = Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#iterate over the join table dataframe\n",
    "for index,row in oscars.iterrows():\n",
    "    #create the oscar with a custom id \n",
    "    cat = re.sub(r'[^\\w\\s]','',row['category'])\n",
    "    Oscar = URIRef(MO['oscar_'+cat.replace(\" \", \"\").lower()+'_'+ str(num2words(row['ceremony'], to='ordinal'))])\n",
    "    \n",
    "    # check if there already is at least a triple about this oscar\n",
    "    if not (Oscar, None, None) in g:    \n",
    "        # check if the oscar is already in the graph\n",
    "        g.add((Oscar, RDF.type, MO.Oscar))\n",
    "        g.add((Oscar, MO['category'], Literal(row['category'].lower(), datatype=XSD.string)))\n",
    "        g.add((Oscar, MO['year'], Literal(row['year_ceremony'], datatype=XSD.gYear)))\n",
    "    \n",
    "    # check if there is a name matching the people, meaning that the oscar can be associated to a person\n",
    "    if (people[\"name\"] == row[\"name\"]).any() == True :\n",
    "        #there is a person with this name\n",
    "        # Create the node about the person\n",
    "        # note that we do not add this resource to the database (created before)\n",
    "        Person = URIRef(MO[people[people[\"name\"]==row[\"name\"]].index[0]])\n",
    "        if row['winner']:\n",
    "            g.add((Person, MO['winner'], Oscar))\n",
    "        else:\n",
    "            g.add((Person, MO['nominated'], Oscar))\n",
    "    \n",
    "    # an oscar for a person is also to be considered an oscar for the movie\n",
    "    # check if the movie is in our DB\n",
    "    if (movies[\"original_title\"] == row[\"film\"]).any():\n",
    "        # there is a movie with this title\n",
    "        Movie = URIRef(MO[movies[movies[\"original_title\"]==row[\"film\"]].index[0]])\n",
    "        if row['winner']:\n",
    "            g.add((Movie, MO['winner'], Oscar))\n",
    "        else:\n",
    "            g.add((Movie, MO['nominated'], Oscar))\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"mo\", MO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'oscars.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle').decode(\"utf-8\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
